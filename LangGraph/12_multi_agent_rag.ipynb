{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Agent RAG with Subagents Architecture\n",
        "\n",
        "Implements a supervisor pattern where a central main agent coordinates specialized subagents by calling them as tools. Each subagent is stateless and focused on a specific domain, with all context managed by the supervisor.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "**Subagents Pattern:**\n",
        "- Main agent (supervisor) maintains conversation state and routes tasks\n",
        "- Subagents are invoked as tools and return results to the supervisor\n",
        "- Each subagent operates in isolated context windows\n",
        "- Supervisor synthesizes results from multiple subagents\n",
        "\n",
        "**Key Benefits:**\n",
        "- Centralized control flow through supervisor\n",
        "- Context isolation prevents bloat in main conversation\n",
        "- Easy to add new specialized subagents\n",
        "- Parallel execution when tasks are independent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Knowledge Base with RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 8 Wikipedia documents\n",
            "First document preview: In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted...\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents from Wikipedia for realistic knowledge base\n",
        "# Using transformers in ML as the domain for this example\n",
        "loader = WikipediaLoader(query=\"Transformer (deep learning)\", load_max_docs=8)\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(documents)} Wikipedia documents\")\n",
        "print(f\"First document preview: {documents[0].page_content[:150]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split into 45 chunks\n"
          ]
        }
      ],
      "source": [
        "# Chunk documents for better retrieval\n",
        "# Smaller chunks improve precision, overlap maintains context continuity\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Split into {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Initialize embeddings model for semantic search\n",
        "# all-mpnet-base-v2 provides good balance of quality and speed\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created with 45 document chunks\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Create vector store with local persistence\n",
        "# Persistent storage avoids re-indexing on every run\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"transformer_knowledge\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_db_subagents\"\n",
        ")\n",
        "\n",
        "# Add documents to vector store\n",
        "vector_store.add_documents(chunks)\n",
        "print(f\"Vector store created with {len(chunks)} document chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What are the advantages of transformer architecture?\n",
            "\n",
            "Top result preview:\n",
            "The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. The predecessors of transformers were developed as an improvement over previo...\n"
          ]
        }
      ],
      "source": [
        "# Test retrieval to verify vector store is working\n",
        "test_query = \"What are the advantages of transformer architecture?\"\n",
        "test_results = vector_store.similarity_search(test_query, k=2)\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"\\nTop result preview:\")\n",
        "print(test_results[0].page_content[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Quick test\n",
        "response = llm.invoke(\"Say 'LLM initialized successfully'\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Specialized Subagents\n",
        "\n",
        "Each subagent focuses on a specific domain with its own tools and prompt. Subagents are stateless - they receive a query, execute their task, and return results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Research Subagent\n",
        "\n",
        "Handles information retrieval from the knowledge base. Translates natural language queries into effective vector searches and synthesizes findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def search_knowledge_base(query: str) -> str:\n",
        "    \"\"\"Search the transformer knowledge base for relevant information.\n",
        "    \n",
        "    Returns top 3 most relevant document chunks based on semantic similarity.\n",
        "    \"\"\"\n",
        "    results = vector_store.similarity_search(query, k=3)\n",
        "    \n",
        "    # Format results with clear separation\n",
        "    formatted_results = []\n",
        "    for i, doc in enumerate(results, 1):\n",
        "        formatted_results.append(\n",
        "            f\"Source {i}:\\n{doc.page_content}\\n\"\n",
        "        )\n",
        "    \n",
        "    return \"\\n\".join(formatted_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "RESEARCH_AGENT_PROMPT = (\n",
        "    \"You are a research specialist focused on deep learning and transformers. \"\n",
        "    \"Use the search_knowledge_base tool to find relevant information. \"\n",
        "    \"Synthesize findings from multiple sources when available. \"\n",
        "    \"Always cite which sources informed your answer. \"\n",
        "    \"If information is not found, state that clearly.\"\n",
        ")\n",
        "\n",
        "research_agent = create_agent(\n",
        "    llm,\n",
        "    tools=[search_knowledge_base],\n",
        "    system_prompt=RESEARCH_AGENT_PROMPT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calling tool: search_knowledge_base\n",
            "\n",
            "Calling tool: search_knowledge_base\n",
            "\n",
            "Calling tool: search_knowledge_base\n",
            "\n",
            "Calling tool: search_knowledge_base\n",
            "\n",
            "Calling tool: search_knowledge_base\n",
            "\n",
            "Calling tool: search_knowledge_base\n",
            "\n",
            "Research Agent Response:\n",
            "**Self‑attention in Transformers – a concise, step‑by‑step description**\n",
            "\n",
            "Below is a synthesis of the most relevant points that appear in the transformer knowledge base, together with explicit citations to the source excerpts that mention each idea.\n",
            "\n",
            "---\n",
            "\n",
            "### 1. What self‑attention does  \n",
            "\n",
            "*“Each element in the input sequence attends to all others, enabling the model to capture global dependencies.”*【source 1†L1-L4】  \n",
            "\n",
            "In other words, for every token (word, sub‑word, image patch, …) the model looks at every other token in the same sequence and decides how much information to borrow from each of them. This replaces the recurrent or convolutional mechanisms that were used in earlier sequence models.\n",
            "\n",
            "---\n",
            "\n",
            "### 2. From token embeddings to **queries, keys, and values**  \n",
            "\n",
            "1. **Embedding + positional encoding** – each token is first turned into a vector (the *input embedding*) and a positional encoding is added so the model can distinguish order.  \n",
            "2. **Linear projections** – three learned weight matrices \\(W_Q, W_K, W_V\\) map the same input vector \\(\\mathbf{x}_i\\) into three different spaces:  \n",
            "\n",
            "\\[\n",
            "\\mathbf{q}_i = \\mathbf{x}_i W_Q,\\qquad\n",
            "\\mathbf{k}_i = \\mathbf{x}_i W_K,\\qquad\n",
            "\\mathbf{v}_i = \\mathbf{x}_i W_V .\n",
            "\\]\n",
            "\n",
            "These are called **queries**, **keys**, and **values**. The knowledge base does not give the exact formulas, but the description of self‑attention as “each element attends to all others” implies the need for a pairwise comparison, which is realized through the Q/K/V projections.\n",
            "\n",
            "---\n",
            "\n",
            "### 3. Scaled dot‑product attention  \n",
            "\n",
            "The core operation is a *dot‑product* between a query and all keys, followed by a scaling factor and a softmax:\n",
            "\n",
            "\\[\n",
            "\\alpha_{ij}= \\text{softmax}_j\\!\\Bigl(\\frac{\\mathbf{q}_i\\!\\cdot\\!\\mathbf{k}_j}{\\sqrt{d_k}}\\Bigr) .\n",
            "\\]\n",
            "\n",
            "* \\(\\alpha_{ij}\\) is the attention weight that token *i* assigns to token *j*.  \n",
            "* The denominator \\(\\sqrt{d_k}\\) (where \\(d_k\\) is the dimensionality of the keys) prevents the dot‑product from growing too large, which would make the softmax overly sharp.  \n",
            "\n",
            "The knowledge base mentions “soft attention weights” that “work better than hard attention weights” and that the model builds a *weighted sum* of hidden vectors【source 2†L1-L4】 – exactly what the softmax‑scaled dot‑product produces.\n",
            "\n",
            "---\n",
            "\n",
            "### 4. Producing the output of a single attention head  \n",
            "\n",
            "Each token’s output is the weighted sum of the **values**:\n",
            "\n",
            "\\[\n",
            "\\mathbf{z}_i = \\sum_{j} \\alpha_{ij}\\,\\mathbf{v}_j .\n",
            "\\]\n",
            "\n",
            "Thus the token *i* gathers information from every other token *j*, modulated by how relevant *j* is according to the similarity of their query/key vectors.\n",
            "\n",
            "---\n",
            "\n",
            "### 5. Multi‑head attention  \n",
            "\n",
            "A single set of Q/K/V projections can only capture one type of relationship. Transformers therefore run **multiple attention heads in parallel**:\n",
            "\n",
            "* For head *h* we have separate projection matrices \\((W_Q^{(h)},W_K^{(h)},W_V^{(h)})\\).  \n",
            "* Each head computes its own \\(\\mathbf{z}_i^{(h)}\\) as above.  \n",
            "* The head outputs are concatenated and linearly projected once more to form the final per‑token representation:\n",
            "\n",
            "\\[\n",
            "\\mathbf{z}_i^{\\text{multi}} = \\text{Concat}\\bigl(\\mathbf{z}_i^{(1)},\\dots,\\mathbf{z}_i^{(H)}\\bigr)W_O .\n",
            "\\]\n",
            "\n",
            "The knowledge base emphasizes that “self‑attention … became central to the Transformer architecture”【source 1†L1-L4】, and the multi‑head design is the canonical way this central mechanism is realized in practice.\n",
            "\n",
            "---\n",
            "\n",
            "### 6. Position in the Transformer block  \n",
            "\n",
            "A full Transformer encoder (or decoder) layer typically follows this pattern:\n",
            "\n",
            "1. **Multi‑head self‑attention** (as described above).  \n",
            "2. **Add‑&‑Norm** – a residual connection adds the layer’s input to the attention output, then layer‑normalization is applied.  \n",
            "3. **Position‑wise feed‑forward network** (two linear layers with a non‑linearity in between).  \n",
            "4. **Another Add‑&‑Norm**.\n",
            "\n",
            "The “step‑by‑step operation of the attention block” is illustrated in the knowledge‑base figure referenced in the overview sections【source 1†L7-L9】, confirming that self‑attention sits at the core of each Transformer layer.\n",
            "\n",
            "---\n",
            "\n",
            "### 7. Why it works (high‑level intuition)\n",
            "\n",
            "* **Global receptive field** – because every token can directly attend to every other token, the model can capture long‑range dependencies without the vanishing‑gradient problems of recurrent nets.  \n",
            "* **Content‑based addressing** – the similarity of queries and keys lets the model decide *what* to look at, rather than *where* (as in fixed‑size convolution windows).  \n",
            "* **Parallelism** – all Q/K/V projections and dot‑products can be computed with matrix multiplications, enabling efficient GPU/TPU execution.\n",
            "\n",
            "These intuitive points are echoed in the knowledge base’s historical note that self‑attention “replaced recurrence with attention mechanisms” and became the foundation for models such as BERT, T5, and GPT【source 1†L1-L4】.\n",
            "\n",
            "---\n",
            "\n",
            "## TL;DR Summary  \n",
            "\n",
            "1. **Self‑attention** lets each token compare itself (via a *query*) to every other token (via *keys*), producing a set of attention weights.  \n",
            "2. The weights are obtained by a **scaled dot‑product** followed by a softmax, ensuring a smooth, differentiable distribution.  \n",
            "3. Each token’s new representation is a **weighted sum of the values** of all tokens.  \n",
            "4. **Multi‑head** attention runs several such comparisons in parallel, giving the model multiple “views” of the data.  \n",
            "5. The attention sub‑layer is wrapped in residual connections and layer‑norm, and sits inside each Transformer encoder/decoder block.\n",
            "\n",
            "All of the above is directly supported by the transformer knowledge base excerpts that describe self‑attention as a global, soft‑weighting mechanism central to modern models【source 1†L1-L4】【source 2†L1-L4】.\n"
          ]
        }
      ],
      "source": [
        "# Test research agent independently\n",
        "test_query = \"Explain the self-attention mechanism in transformers\"\n",
        "\n",
        "for step in research_agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": test_query}]},\n",
        "    stream_mode=\"values\"\n",
        "):\n",
        "    latest_msg = step[\"messages\"][-1]\n",
        "    if hasattr(latest_msg, 'tool_calls') and latest_msg.tool_calls:\n",
        "        print(f\"\\nCalling tool: {latest_msg.tool_calls[0]['name']}\")\n",
        "    elif latest_msg.content and latest_msg.type == \"ai\":\n",
        "        print(f\"\\nResearch Agent Response:\\n{latest_msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis Subagent\n",
        "\n",
        "Performs comparative analysis and technical evaluation. Complements research agent by providing deeper insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def compare_concepts(concept_a: str, concept_b: str) -> str:\n",
        "    \"\"\"Compare two technical concepts from the knowledge base.\n",
        "    \n",
        "    Retrieves information about both concepts and provides structured comparison.\n",
        "    \"\"\"\n",
        "    # Search for both concepts\n",
        "    results_a = vector_store.similarity_search(concept_a, k=2)\n",
        "    results_b = vector_store.similarity_search(concept_b, k=2)\n",
        "    \n",
        "    return (\n",
        "        f\"Information about {concept_a}:\\n\"\n",
        "        f\"{results_a[0].page_content[:300]}...\\n\\n\"\n",
        "        f\"Information about {concept_b}:\\n\"\n",
        "        f\"{results_b[0].page_content[:300]}...\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "ANALYSIS_AGENT_PROMPT = (\n",
        "    \"You are a technical analyst specializing in ML architectures. \"\n",
        "    \"Use compare_concepts to analyze differences between approaches. \"\n",
        "    \"Provide structured comparisons highlighting key distinctions. \"\n",
        "    \"Focus on technical accuracy and practical implications.\"\n",
        ")\n",
        "\n",
        "analysis_agent = create_agent(\n",
        "    llm,\n",
        "    tools=[compare_concepts],\n",
        "    system_prompt=ANALYSIS_AGENT_PROMPT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calling tool: compare_concepts\n",
            "\n",
            "Analysis Agent Response:\n",
            "**Transformers vs. Recurrent Neural Networks (RNNs) for Sequence Processing**  \n",
            "*(Technical analyst view – focus on architecture, performance, and practical trade‑offs)*  \n",
            "\n",
            "| Aspect | **Transformers** | **Recurrent Neural Networks (RNNs)** |\n",
            "|--------|------------------|--------------------------------------|\n",
            "| **Core Computational Paradigm** | *Self‑attention* over the whole sequence; each token attends to every other token in a single (or few) layers. | *Sequential recurrence*: hidden state \\(h_t = f(h_{t-1}, x_t)\\) is updated step‑by‑step. |\n",
            "| **Parallelism & Throughput** | Fully parallelizable across time steps during both forward and backward passes (matrix‑multiplication on \\(Q,K,V\\)). Enables massive GPU/TPU utilization; training speed scales roughly linearly with sequence length (up to memory limits). | Inherently sequential; each time step depends on the previous hidden state. Limits parallelism to batch dimension only; training speed grows linearly with sequence length and is often bottlenecked by recurrent loops. |\n",
            "| **Computational Complexity** | \\(O(N^2 \\cdot d)\\) per layer (where \\(N\\) = sequence length, \\(d\\) = hidden dimension) due to the full attention matrix. Memory also \\(O(N^2)\\). | \\(O(N \\cdot d^2)\\) per layer (matrix‑vector multiply per step). Memory is \\(O(N \\cdot d)\\) (only need to store hidden states). |\n",
            "| **Long‑Range Dependency Modeling** | Direct pairwise interactions; distance‑independent attention weights allow easy capture of global context. | Information must be propagated step‑by‑step; gradients can vanish/explode, making very long dependencies hard to learn (mitigated by LSTM/GRU gating but still limited). |\n",
            "| **Training Stability** | Stable with layer‑norm, residual connections, and dropout. Requires large batch sizes and careful learning‑rate schedules (e.g., warm‑up). | Can suffer from exploding/vanishing gradients; gating mechanisms (LSTM/GRU) improve stability but still need gradient clipping and careful initialization. |\n",
            "| **Data Efficiency** | Generally *data‑hungry*: performance scales with the amount of training data (e.g., GPT‑3‑scale models). Pre‑training on massive corpora is common. | Often more data‑efficient on modest datasets because recurrence imposes a strong inductive bias (temporal locality). Works well for low‑resource tasks when architecture is small. |\n",
            "| **Memory Footprint** | Quadratic memory in sequence length; long sequences (>1‑2k tokens) require memory‑saving tricks (sparse/linear attention, chunking, reversible layers). | Linear memory; can handle arbitrarily long sequences limited only by batch size and hidden‑state size. |\n",
            "| **Interpretability** | Attention weights provide a direct, visualizable map of token‑to‑token influence; however, they are not always causal explanations. | Hidden states are less directly interpretable; gating activations can be inspected but give a more opaque view of information flow. |\n",
            "| **Typical Use‑Cases** | • Large‑scale language models (GPT, BERT, T5)  <br>• Machine translation, summarization, code generation <br>• Vision‑language (ViT, CLIP) <br>• Any task benefitting from global context and massive pre‑training. | • Speech recognition (RNN‑Transducer, LAS) <br>• Time‑series forecasting with limited horizon <br>• Low‑resource NLP (POS tagging, NER) <br>• Situations where memory is constrained or sequence length is very large. |\n",
            "| **Pros** | • Parallel training → faster wall‑clock time on modern hardware <br>• Strong at modeling global dependencies <br>• Scales well with model size and data <br>• Attention maps aid debugging. | • Linear time & memory → can process very long sequences <br>• Strong inductive bias for sequential locality <br>• Often works well with limited data <br>• Simpler to implement on CPUs/edge devices. |\n",
            "| **Cons** | • Quadratic cost limits raw sequence length <br>• Requires large datasets & compute to reach peak performance <br>• Higher inference latency for very long inputs unless optimized. | • Sequential nature limits speed on GPUs/TPUs <br>• Struggles with very long‑range dependencies <br>• Gradient issues despite gating; harder to train deep RNNs. |\n",
            "| **Key Architectural Variants** | • **Encoder‑only** (BERT, RoBERTa) <br>• **Decoder‑only** (GPT series) <br>• **Encoder‑decoder** (T5, BART) <br>• **Sparse/Linear attention** (Longformer, Performer) for long sequences. | • **Vanilla RNN** <br>• **LSTM** (Long Short‑Term Memory) <br>• **GRU** (Gated Recurrent Unit) <br>• **Bidirectional RNNs** for context on both sides. |\n",
            "| **Practical Implications** | - **When to choose**: Large‑scale tasks with abundant data, need for global context, and access to GPU/TPU clusters. <br>- **Engineering**: Requires careful memory management, mixed‑precision training, and often distributed training pipelines. | - **When to choose**: Low‑resource settings, edge or mobile deployment, or tasks where sequence length exceeds the practical memory budget of full attention. <br>- **Engineering**: Simpler training loops, can run efficiently on CPUs, but may need gradient clipping and gating tricks. |\n",
            "\n",
            "### Bottom‑Line Takeaway\n",
            "- **Transformers** dominate modern NLP and multimodal research because their parallelizable self‑attention unlocks scaling to billions of parameters and massive corpora, delivering state‑of‑the‑art performance on tasks that demand global reasoning.  \n",
            "- **RNNs** remain valuable when computational resources are limited, data is scarce, or the problem inherently benefits from a strong sequential bias (e.g., streaming inference, very long sequences).  \n",
            "\n",
            "In practice, many production pipelines blend both: a lightweight RNN (or convolutional front‑end) for streaming or on‑device preprocessing, followed by a transformer‑based model for high‑accuracy downstream reasoning.\n"
          ]
        }
      ],
      "source": [
        "# Test analysis agent\n",
        "test_query = \"Compare transformers with RNNs for sequence processing\"\n",
        "\n",
        "for step in analysis_agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": test_query}]},\n",
        "    stream_mode=\"values\"\n",
        "):\n",
        "    latest_msg = step[\"messages\"][-1]\n",
        "    if hasattr(latest_msg, 'tool_calls') and latest_msg.tool_calls:\n",
        "        print(f\"\\nCalling tool: {latest_msg.tool_calls[0]['name']}\")\n",
        "    elif latest_msg.content and latest_msg.type == \"ai\":\n",
        "        print(f\"\\nAnalysis Agent Response:\\n{latest_msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrap Subagents as Tools\n",
        "\n",
        "This is the key architectural step. Each subagent is wrapped as a tool that the supervisor can invoke. The supervisor sees high-level capabilities, not implementation details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def research_information(query: str) -> str:\n",
        "    \"\"\"Research information from the knowledge base.\n",
        "    \n",
        "    Use this when the user needs factual information, definitions, or\n",
        "    explanations from the transformer knowledge base. Handles semantic search\n",
        "    and information synthesis.\n",
        "    \n",
        "    Input: Natural language query about transformers or deep learning\n",
        "    \"\"\"\n",
        "    result = research_agent.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
        "    })\n",
        "    \n",
        "    # Return only the final response to supervisor\n",
        "    # Supervisor doesn't need to see intermediate tool calls\n",
        "    return result[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def analyze_comparison(request: str) -> str:\n",
        "    \"\"\"Analyze and compare technical concepts.\n",
        "    \n",
        "    Use this when the user wants to compare different approaches, understand\n",
        "    tradeoffs, or analyze technical differences. Provides structured comparative\n",
        "    analysis.\n",
        "    \n",
        "    Input: Natural language request for comparison or analysis\n",
        "    \"\"\"\n",
        "    result = analysis_agent.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
        "    })\n",
        "    \n",
        "    return result[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Supervisor Agent\n",
        "\n",
        "The supervisor orchestrates subagents, making high-level routing decisions. It maintains conversation state and synthesizes results from multiple subagents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "SUPERVISOR_PROMPT = (\n",
        "    \"You are a helpful AI assistant with access to specialized subagents. \"\n",
        "    \"You can research information and perform technical analysis. \"\n",
        "    \"\\n\\nAvailable capabilities:\"\n",
        "    \"\\n- research_information: Look up facts and explanations\"\n",
        "    \"\\n- analyze_comparison: Compare technical approaches\"\n",
        "    \"\\n\\nFor complex questions, you may need to use multiple subagents in sequence. \"\n",
        "    \"Break down requests and coordinate subagent results into coherent responses.\"\n",
        ")\n",
        "\n",
        "supervisor_agent = create_agent(\n",
        "    llm,\n",
        "    tools=[research_information, analyze_comparison],\n",
        "    system_prompt=SUPERVISOR_PROMPT\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Complete Multi-Agent System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Simple Single-Domain Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Query: What is the purpose of positional encoding in transformers?\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Final Response:\n",
            "**Positional encoding (or positional embedding) is the mechanism that gives a Transformer any notion of the order of tokens in a sequence.**  \n",
            "\n",
            "### Why it’s needed\n",
            "- **Self‑attention is order‑agnostic.**  \n",
            "  In a Transformer each token attends to every other token via dot‑product attention. The attention computation itself treats the input as a set, not a sequence, so without extra information the model cannot distinguish “the first word” from “the last word” if the words themselves are identical.\n",
            "- **Sequence‑level tasks require order.**  \n",
            "  Tasks such as language modeling, translation, or any sequential prediction depend on the relative and absolute positions of tokens (e.g., “cat sat on the mat” ≠ “mat sat on the cat”). Positional encodings inject this ordering information so the model can learn patterns that depend on token positions.\n",
            "\n",
            "### How it works\n",
            "1. **Create a vector for each position** (same dimensionality as the token embeddings).  \n",
            "2. **Add (or concatenate) this vector to the token embedding** before feeding it to the first Transformer layer.  \n",
            "3. The resulting “position‑aware” embeddings are then processed by the self‑attention layers.\n",
            "\n",
            "### Common forms\n",
            "| Type | Description | Pros / Cons |\n",
            "|------|-------------|-------------|\n",
            "| **Sinusoidal (fixed)** | For position *p* and dimension *i*: <br>  PE(p,2i)   = sin(p / 10000^{2i/d}) <br>  PE(p,2i+1) = cos(p / 10000^{2i/d}) | • No learned parameters → works out‑of‑the‑box for any sequence length.<br>• Provides smooth, continuous values that let the model extrapolate to longer sequences.<br>• May be less expressive than learned embeddings for some tasks. |\n",
            "| **Learned positional embeddings** | Each position has a dedicated trainable vector (like a word embedding). | • Can adapt to the specific data distribution.<br>• Requires a fixed maximum length during training; extrapolation to longer sequences is not guaranteed.<br>• Adds extra parameters. |\n",
            "| **Relative positional encodings** (e.g., Shaw et al., 2018; Transformer‑XL, T5) | Encode the *relative* distance between query and key tokens rather than absolute positions. | • Allows the model to generalize across lengths and capture relative order more naturally.<br>• Slightly more complex implementation. |\n",
            "\n",
            "### What the encodings actually convey\n",
            "- **Absolute position** (sinusoidal or learned) tells the model “this token is the 7th word.”\n",
            "- **Relative position** tells the model “the key is 3 steps ahead of the query,” which is useful for tasks where the exact index matters less than the distance (e.g., language modeling, music generation).\n",
            "\n",
            "### Effect on attention\n",
            "When the query, key, and value vectors are computed, the positional component influences the dot‑product scores:\n",
            "\n",
            "\\[\n",
            "\\text{score}(q_i, k_j) = (x_i + p_i)^\\top (x_j + p_j)\n",
            "\\]\n",
            "\n",
            "where \\(x_i\\) is the token embedding and \\(p_i\\) the positional encoding. The added \\(p_i\\) and \\(p_j\\) shift the similarity scores so that tokens at certain relative distances receive higher or lower attention weights, enabling the model to learn patterns like “the next word is often related to the previous word” or “subject‑verb agreement across a fixed distance.”\n",
            "\n",
            "### Summary\n",
            "- **Purpose:** Provide the Transformer with information about token order, which self‑attention alone cannot infer.\n",
            "- **Implementation:** Added to token embeddings as fixed sinusoidal vectors, learned vectors, or relative distance terms.\n",
            "- **Impact:** Enables the model to capture sequential relationships, making it effective for language, time‑series, and any data where order matters.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the purpose of positional encoding in transformers?\"\n",
        "\n",
        "print(f\"User Query: {query}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "for step in supervisor_agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
        "    stream_mode=\"values\"\n",
        "):\n",
        "    latest_msg = step[\"messages\"][-1]\n",
        "    \n",
        "    if hasattr(latest_msg, 'tool_calls') and latest_msg.tool_calls:\n",
        "        for tool_call in latest_msg.tool_calls:\n",
        "            print(f\"\\nSupervisor calling: {tool_call['name']}\")\n",
        "    \n",
        "    elif latest_msg.type == \"tool\":\n",
        "        print(f\"\\nSubagent completed: {latest_msg.name}\")\n",
        "    \n",
        "    elif latest_msg.content and latest_msg.type == \"ai\":\n",
        "        print(f\"\\nFinal Response:\\n{latest_msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Complex Multi-Domain Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Query: First, explain what attention mechanism is. Then compare how attention works in transformers versus traditional RNNs.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Supervisor calling: research_information\n",
            "\n",
            "Subagent completed: research_information\n",
            "\n",
            "Supervisor calling: analyze_comparison\n",
            "\n",
            "Subagent completed: analyze_comparison\n",
            "\n",
            "Final Response:\n",
            "**What is an attention mechanism?**  \n",
            "\n",
            "In deep‑learning models an **attention mechanism** is a differentiable module that lets the network decide, for each output element, how much “focus” to give to each part of its input.  \n",
            "It works by computing a set of *attention scores* that measure the relevance of every input token (or hidden state) to the current processing step, turning those scores into a probability distribution with a soft‑max, and then taking a weighted sum of the input representations (the *values*). The result – the *context vector* – is fed to the next layer (decoder, classifier, etc.).  \n",
            "\n",
            "The most common formulation is the **query‑key‑value** paradigm:\n",
            "\n",
            "1. **Query (Q)** – derived from the element that is producing an output (e.g., the current decoder hidden state).  \n",
            "2. **Key (K)** – derived from each input element (or from the same sequence in self‑attention).  \n",
            "3. **Value (V)** – the actual representations that will be combined.\n",
            "\n",
            "\\[\n",
            "\\text{Attention}(Q,K,V)=\\text{softmax}\\!\\Big(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Big)\\,V\n",
            "\\]\n",
            "\n",
            "where \\(d_k\\) is the dimensionality of the keys (the scaling factor in *scaled dot‑product attention*).  \n",
            "\n",
            "Key properties:\n",
            "\n",
            "| Property | Meaning |\n",
            "|----------|---------|\n",
            "| **Dynamic weighting** | The model can emphasize the most relevant inputs for each output. |\n",
            "| **Differentiable** | End‑to‑end training via back‑propagation. |\n",
            "| **Global context (self‑attention)** | Every token can attend to every other token, capturing long‑range dependencies without recurrence. |\n",
            "| **Interpretability** | Attention weights can be visualized to see what the model “looked at”. |\n",
            "\n",
            "---\n",
            "\n",
            "## Comparing Attention in Transformers vs. Traditional RNNs  \n",
            "\n",
            "Below is a side‑by‑side comparison that highlights the main architectural and practical differences.\n",
            "\n",
            "| Aspect | **Transformer‑style (self‑)attention** | **RNN‑based encoder‑decoder with attention** |\n",
            "|--------|----------------------------------------|---------------------------------------------|\n",
            "| **Core role of attention** | The *only* mixing operation. Each layer consists of multi‑head self‑attention + feed‑forward network; positional encodings are added to retain order. | The encoder is a (often bidirectional) RNN (LSTM/GRU). The decoder is another RNN that, at each time step, computes a context vector by attending over **all** encoder hidden states (Bahdanau or Luong attention). |\n",
            "| **Computation** | For every token pair (i, j) compute a similarity (dot‑product of Q and K), scale, soft‑max, and weight V. Done **in parallel** for all positions and heads. | At decoder step *t*, the current decoder hidden state (query) is compared to each encoder hidden state (keys) via an additive or dot‑product score. The resulting distribution yields a weighted sum of encoder values → context vector. This is **sequential**: a new context is computed for each output token. |\n",
            "| **Complexity** | **O(N²·d)** per layer (N = sequence length, d = model dimension). Quadratic in length but highly parallelizable on GPUs/TPUs. | Encoder: **O(N·d²)** (linear in length). Decoder attention: **O(T·N·d)** (T = output length). Decoding is sequential, so effective runtime is linear in N but *non‑parallel* across time steps. |\n",
            "| **Long‑range dependencies** | Direct pairwise interactions let any token attend to any other token in a single layer → efficient modeling of very distant relationships. | RNN hidden states propagate information step‑by‑step; even with attention, the decoder still depends on the encoder’s final hidden representation. Long‑range dependencies can be weakened by vanishing gradients, though attention provides a shortcut from any encoder state. |\n",
            "| **Parallelism** | Full parallelism across all positions during both training and inference (except causal masking in the decoder). Enables massive speed‑ups. | Encoder can be parallelized (bidirectional), but the decoder **must** run sequentially because each output token depends on the previous hidden state and its own attention context. This limits inference speed. |\n",
            "| **Training dynamics** | • Stable gradients thanks to layer‑norm and residual connections.<br>• No recurrence → back‑propagation depth equals number of layers (typically < 12).<br>• Requires large batches and learning‑rate schedules (warm‑up, decay). | • BPTT through time for both encoder and decoder → longer gradient paths (O(N+T)).<br>• Susceptible to vanishing/exploding gradients; gating (LSTM/GRU) and gradient clipping are essential.<br>• Attention is learned jointly but overall optimization is more sensitive to sequence length. |\n",
            "| **Positional information** | Explicit sinusoidal or learned positional encodings are added because self‑attention is order‑agnostic. | Order is implicit in the recurrent hidden‑state updates; no extra positional encoding needed. |\n",
            "| **Memory footprint** | Stores Q, K, V for each head → O(N·d) per layer plus the O(N²) attention matrix (often the dominant term). | Stores encoder hidden states (O(N·d)) and decoder hidden state (O(T·d)). No quadratic matrix, so memory scales linearly with sequence length—advantageous for very long inputs when memory is limited. |\n",
            "| **Typical use‑cases** | Large‑scale language models, machine translation, vision‑language tasks, any setting where global context and high throughput are critical. | Low‑resource or latency‑sensitive settings where model size must be tiny, or tasks that benefit from a strong sequential inductive bias (e.g., speech synthesis, certain time‑series). |\n",
            "\n",
            "### TL;DR Summary  \n",
            "\n",
            "* **Transformers** replace recurrence with *global self‑attention*. This yields **quadratic‑time but fully parallel** computation, excellent ability to capture long‑range dependencies, and stable training, at the cost of higher memory for long sequences.  \n",
            "\n",
            "* **RNN encoder‑decoder with attention** retains sequential recurrence. Decoding is **linear‑time but inherently sequential**, memory scales linearly, and the model has a stronger bias toward local, order‑preserving patterns. It can still benefit from attention (providing a shortcut to any encoder state) but struggles more with very long‑range dependencies and slower inference.  \n",
            "\n",
            "Both paradigms use attention to focus on relevant parts of the input, yet the **where** and **how** of that focus differ dramatically, shaping their computational trade‑offs and suitability for different practical scenarios.\n"
          ]
        }
      ],
      "source": [
        "complex_query = (\n",
        "    \"First, explain what attention mechanism is. \"\n",
        "    \"Then compare how attention works in transformers versus traditional RNNs.\"\n",
        ")\n",
        "\n",
        "print(f\"User Query: {complex_query}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "for step in supervisor_agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": complex_query}]},\n",
        "    stream_mode=\"values\"\n",
        "):\n",
        "    latest_msg = step[\"messages\"][-1]\n",
        "    \n",
        "    if hasattr(latest_msg, 'tool_calls') and latest_msg.tool_calls:\n",
        "        for tool_call in latest_msg.tool_calls:\n",
        "            print(f\"\\nSupervisor calling: {tool_call['name']}\")\n",
        "    \n",
        "    elif latest_msg.type == \"tool\":\n",
        "        print(f\"\\nSubagent completed: {latest_msg.name}\")\n",
        "    \n",
        "    elif latest_msg.content and latest_msg.type == \"ai\":\n",
        "        print(f\"\\nFinal Response:\\n{latest_msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Multi-Turn Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Turn 1: What are transformer models?\n",
            "================================================================================\n",
            "\n",
            "Assistant: **Transformer models** are a family of deep‑learning architectures that process sequences (or sets) by repeatedly applying **self‑attention**, feed‑forward networks, and residual‑norm layers—allowing every element of the input to directly “attend” to all other elements without using recurrence or convolution.\n",
            "\n",
            "---\n",
            "\n",
            "### Core Ideas\n",
            "\n",
            "| Concept | What it means | Why it matters |\n",
            "|---------|---------------|----------------|\n",
            "| **Self‑attention** | Each token computes a weighted sum of *all* other tokens, where the weights (attention scores) are learned. | Captures **global dependencies** in a single layer, unlike RNNs that see only nearby context step‑by‑step. |\n",
            "| **Multi‑head** | The attention operation is performed in parallel across several “heads”, each learning different relational patterns. | Increases expressive power; the model can attend to many aspects of the data simultaneously. |\n",
            "| **Positional encoding** | Since attention is order‑agnostic, a deterministic (sinusoidal) or learned vector is added to token embeddings to inject sequence order. | Gives the model a sense of position without recurrence. |\n",
            "| **Feed‑forward network (FFN)** | A small two‑layer MLP applied independently to each token after attention. | Adds non‑linearity and depth, enabling richer transformations. |\n",
            "| **Residual connections + LayerNorm** | Each sub‑layer’s output is added to its input and normalized. | Stabilizes training, eases gradient flow, and allows very deep stacks. |\n",
            "| **Encoder‑decoder structure** (original design) | • **Encoder**: stacks of self‑attention + FFN → contextualized representations.<br>• **Decoder**: masked self‑attention + cross‑attention to encoder outputs + FFN → autoregressive generation. | Supports sequence‑to‑sequence tasks (e.g., translation). Variants keep only the encoder (BERT‑style) or only the decoder (GPT‑style). |\n",
            "\n",
            "---\n",
            "\n",
            "### Typical Architectures\n",
            "\n",
            "| Variant | Main Use‑Case | Sketch |\n",
            "|---------|---------------|--------|\n",
            "| **Encoder‑only** (e.g., BERT, RoBERTa) | Representation learning, classification, token‑level tasks | `Input → Embedding+PosEnc → [Self‑Attn → Add&Norm → FFN → Add&Norm] × L → Task head` |\n",
            "| **Decoder‑only** (e.g., GPT‑3/4, PaLM) | Autoregressive generation (text, code, etc.) | `Input → Embedding+PosEnc → [Masked Self‑Attn → Add&Norm → FFN → Add&Norm] × L → LM head` |\n",
            "| **Encoder‑decoder** (e.g., T5, BART, original Transformer) | Seq‑2‑seq (translation, summarisation, QA) | `Encoder stack → latent representation → Decoder stack (cross‑attention) → Output head` |\n",
            "\n",
            "---\n",
            "\n",
            "### Why Transformers Became Dominant\n",
            "\n",
            "| Advantage | Explanation |\n",
            "|-----------|-------------|\n",
            "| **Parallelism** | No recurrence → all tokens processed simultaneously → massive speed‑ups on GPUs/TPUs. |\n",
            "| **Scalability** | Depth, width, and data can be increased almost arbitrarily; large models follow predictable “scaling laws”. |\n",
            "| **Flexibility** | Same building blocks work for text, images (Vision Transformers), audio, multimodal data, reinforcement learning, etc. |\n",
            "| **Pre‑training + Fine‑tuning** | Self‑supervised objectives (masked LM, next‑token prediction) produce generic representations that can be adapted cheaply to downstream tasks. |\n",
            "\n",
            "---\n",
            "\n",
            "### Limitations & Ongoing Research\n",
            "\n",
            "| Issue | Current research directions |\n",
            "|-------|------------------------------|\n",
            "| **Quadratic cost** of self‑attention (O(N²) memory/computation) | Efficient attention variants: **Linformer**, **Performer**, **FlashAttention**, **Sparse/Longformer**, **Reformer**, etc. |\n",
            "| **Data hunger** (very large corpora needed) | Data‑efficient pre‑training, curriculum learning, synthetic data generation. |\n",
            "| **Interpretability** | Probing studies, attention‑flow analysis, mechanistic interpretability, attribution methods. |\n",
            "| **Robustness & Hallucination** | Alignment techniques, RLHF, retrieval‑augmented generation, factuality filters. |\n",
            "\n",
            "---\n",
            "\n",
            "### One‑Sentence Definition\n",
            "\n",
            "> *A transformer is a deep‑learning model that replaces recurrence with multi‑head self‑attention, enabling every token in a sequence to directly attend to all others, and stacks this mechanism with feed‑forward layers, residual connections, and positional encodings to learn rich, scalable representations.*\n",
            "\n",
            "---\n",
            "\n",
            "### Quick Visual of an Encoder‑Only Layer\n",
            "\n",
            "```\n",
            "Input tokens\n",
            "   │\n",
            "   ├─► Token Embedding + Positional Encoding\n",
            "   │\n",
            "   └─► [Layer 1] ──► [Layer 2] ──► … ──► [Layer L]\n",
            "          │                │                │\n",
            "          └─ Multi‑head Self‑Attention\n",
            "          └─ Add & LayerNorm\n",
            "          └─ Feed‑Forward (FFN)\n",
            "          └─ Add & LayerNorm\n",
            "   │\n",
            "   └─► Final hidden states → task‑specific head (e.g., classification)\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### Bottom Line\n",
            "\n",
            "Transformers introduced **self‑attention** as a universal, parallelizable way to model long‑range relationships. Since the 2017 *“Attention Is All You Need”* paper, they have become the backbone of modern AI—powering large language models, vision transformers, multimodal systems, and countless specialized applications. Their ability to scale, be pre‑trained on massive data, and be fine‑tuned for diverse tasks makes them the de‑facto standard for today’s foundation models.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Turn 2: How do they differ from LSTM networks?\n",
            "================================================================================\n",
            "\n",
            "Assistant: **Transformer vs. LSTM – Quick‑Reference Cheat Sheet**\n",
            "\n",
            "| Aspect | **Transformer** (self‑attention) | **LSTM** (recurrent) |\n",
            "|--------|----------------------------------|----------------------|\n",
            "| **Core building block** | Multi‑head *self‑attention* + feed‑forward sub‑layer (no recurrence) | Gated recurrent cell (input/forget/output gates) that passes a hidden state forward step‑by‑step |\n",
            "| **How order is handled** | Positional encodings are added to token embeddings (order is injected explicitly) | Order is implicit – the hidden state carries sequential information automatically |\n",
            "| **Parallelism** | **Full‑sequence parallelism** (all tokens processed simultaneously; only causal masking limits decoder) | **Sequential** – each time step must wait for the previous one; only the internal matrix ops can be parallelized |\n",
            "| **Long‑range dependencies** | Direct connections between any two positions → path length = 1, so global context is easy to learn | Information must travel through the chain → path length grows with distance; gates help but long‑range links are still hard |\n",
            "| **Training efficiency** | Very fast on GPUs/TPUs because of parallelism; gradient flow is uniform across layers | Slower because of the recurrence; gradients can vanish/explode over many steps, needing tricks (gradient clipping, careful init) |\n",
            "| **Memory & compute cost** | **Quadratic** in sequence length (O(N²·d) for attention matrix). Memory‑heavy for long sequences. | **Linear** in sequence length (O(N·d²)). Only need to store the previous hidden state (plus gradients). |\n",
            "| **Scalability** | Scales smoothly to billions of parameters and massive data (scaling laws). Main bottleneck is attention cost → many “efficient‑attention” variants (Linformer, Performer, FlashAttention). | Scaling depth/width quickly hits diminishing returns; recurrent bottleneck limits how large a model can be trained efficiently. |\n",
            "| **Typical use‑cases** | • Large‑scale language models (GPT‑4, PaLM, LLaMA) <br>• Machine translation, summarisation, QA <br>• Vision Transformers, multimodal models (CLIP, Flamingo) <br>• Any task where global context matters and compute/data are plentiful | • Speech recognition, low‑resource NLP <br>• Time‑series forecasting, sensor data <br>• Sequence labeling (POS, NER) when data is limited <br>• Edge/embedded devices with tight memory/compute budgets |\n",
            "| **Key advantages** | • Global attention → excellent at capturing long‑range patterns <br>• Highly parallel → fast training on modern hardware <br>• Uniform architecture can be reused across modalities | • Low memory footprint <br>• Naturally suited for streaming/online inference (process one step at a time) <br>• Works well with modest datasets |\n",
            "| **Key limitations** | • O(N²) attention cost limits very long sequences unless you use sparse/linear‑attention tricks <br>• Requires huge data & compute to reach peak performance | • Sequential bottleneck makes large‑scale pre‑training impractical <br>• Struggles with dependencies beyond a few hundred steps despite gating |\n",
            "\n",
            "---\n",
            "\n",
            "### TL;DR Narrative\n",
            "\n",
            "- **Transformers** replace the recurrent loop of an LSTM with a *self‑attention* mechanism that lets every token look at every other token in one shot. This gives them **global context** and **massive parallelism**, which is why they dominate today’s large‑scale language, vision, and multimodal models. The trade‑off is a **quadratic memory/computation cost** that grows with sequence length, so researchers add sparse or linear‑attention tricks to handle very long inputs.\n",
            "\n",
            "- **LSTMs** keep a hidden state that is passed forward step‑by‑step. This makes them **sequential**, limiting parallel speed, but also **memory‑light** and naturally suited for streaming or low‑resource environments. Their gating helps with medium‑range dependencies, yet they still have trouble learning relationships that span many hundreds of steps.\n",
            "\n",
            "---\n",
            "\n",
            "### When to Choose Which\n",
            "\n",
            "| Situation | Prefer **Transformer** | Prefer **LSTM** |\n",
            "|-----------|------------------------|----------------|\n",
            "| You have **lots of data & GPU/TPU compute** and need the best possible performance on tasks like translation, summarisation, or large‑scale language modeling | ✅ | ❌ |\n",
            "| Your inputs are **very long** (e.g., whole documents, DNA sequences) and you can afford specialized efficient‑attention kernels | ✅ (with Linformer/Performer/etc.) | ❌ |\n",
            "| You need **real‑time streaming inference** on a device with **tight memory/compute** (e.g., on‑device speech recogniser) | ❌ (causal masking helps but still heavy) | ✅ |\n",
            "| You are in a **low‑resource setting** (small dataset, limited GPU memory) and the sequence length is modest (< 200 tokens) | ❌ (overkill) | ✅ |\n",
            "| You want a **single, reusable backbone** for many modalities (text, image, audio) | ✅ | ❌ |\n",
            "\n",
            "---\n",
            "\n",
            "### Bottom Line\n",
            "\n",
            "- **Transformers** = *parallel, global‑attention, scalable, data‑hungry*  \n",
            "- **LSTMs** = *sequential, memory‑efficient, good for modest data/compute*  \n",
            "\n",
            "Both are still useful; the choice hinges on the **resource budget**, **sequence length**, and **performance requirements** of your specific problem.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Turn 3: Which one should I use for a sequence-to-sequence task?\n",
            "================================================================================\n",
            "\n",
            "Assistant: ## TL;DR  \n",
            "**For almost every modern sequence‑to‑sequence (seq2seq) problem—machine translation, summarisation, data‑to‑text generation, code generation, speech‑to‑text, etc.—an **encoder‑decoder **Transformer** is the default choice.**  \n",
            "Only in a few niche situations (tiny training corpora, very strict memory/latency limits, or when you need a streaming‑only model) does an **LSTM‑based encoder‑decoder** remain competitive.\n",
            "\n",
            "Below is a decision‑making guide that explains **why** the Transformer usually wins, **when** an LSTM might still be sensible, and **how** to pick a concrete model family for your task.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. What “seq2seq” really needs\n",
            "\n",
            "| Requirement | Why it matters for the architecture |\n",
            "|-------------|--------------------------------------|\n",
            "| **Global context** (the output token may depend on any part of the input) | Transformers give every output token direct access to the whole encoded sequence via cross‑attention. |\n",
            "| **Variable‑length inputs & outputs** | Both architectures support this, but Transformers do it without a recurrent bottleneck. |\n",
            "| **Training speed & scalability** | Parallel processing of the whole source sequence (and, in the decoder, of the whole target prefix) makes Transformers far faster on GPUs/TPUs. |\n",
            "| **Data efficiency** | LSTMs can sometimes learn from a few thousand examples, but Transformers usually need *more* data; however, pre‑training (e.g., T5, BART) can bridge that gap. |\n",
            "| **Inference latency** | LSTMs can emit one token at a time with a tiny memory footprint; Transformers need to keep the whole attention matrix for the current prefix (but recent “decoder‑only” models with caching make this cheap). |\n",
            "| **Resource constraints** (GPU memory, edge devices) | LSTMs are lighter; Transformers can be pruned, quantised, or replaced with efficient variants (e.g., Longformer, Reformer) if needed. |\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Transformer Encoder‑Decoder (the “go‑to” today)\n",
            "\n",
            "| Strength | Typical models | When it shines |\n",
            "|----------|----------------|----------------|\n",
            "| **Full‑sequence cross‑attention** – decoder can look at any encoder hidden state at each step | **T5**, **BART**, **mBART**, **MarianMT**, **Pegasus**, **mT5**, **M2M‑100**, **Seq2Seq‑LM** (HuggingFace) | Machine translation, abstractive summarisation, data‑to‑text, code generation, multimodal captioning |\n",
            "| **Pre‑training + fine‑tuning** – massive self‑supervised corpora give strong generic representations | **T5‑base/large**, **BART‑large**, **mBART‑50**, **mT5‑xxl** | Low‑resource languages (transfer learning), domain‑adaptation, few‑shot tasks |\n",
            "| **Scalable to billions of parameters** – performance improves predictably with size | **T5‑XXL (11 B)**, **BART‑large (400 M)**, **mT5‑xxl (13 B)** | High‑quality translation, long‑form generation, scientific text generation |\n",
            "| **Parallel training** – all source tokens processed together → fast epochs | All modern transformer libraries (PyTorch, TensorFlow, JAX) | When you have GPUs/TPUs and want to iterate quickly |\n",
            "| **Rich tooling** – HuggingFace 🤗 Transformers, Fairseq, OpenNMT‑py, Tensor2Tensor, etc. | Same as above | Easy to prototype, benchmark, and deploy |\n",
            "\n",
            "**Practical tip:**  \n",
            "If you have *any* pre‑trained encoder‑decoder checkpoint that matches your language/domain, start by fine‑tuning it. Even a 220 M‑parameter model (e.g., **t5‑small**) often outperforms a freshly trained LSTM encoder‑decoder on the same data.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. LSTM Encoder‑Decoder (when it still makes sense)\n",
            "\n",
            "| Strength | Typical models / libraries | When it shines |\n",
            "|----------|---------------------------|----------------|\n",
            "| **Very low memory footprint** – only hidden state per time step (≈ O(d)) | **OpenNMT‑lua**, **OpenNMT‑py**, **TensorFlow seq2seq**, **Keras LSTM‑based models** | Edge devices, micro‑controllers, or inference on CPUs with < 1 GB RAM |\n",
            "| **Streaming / online generation** – can produce output token as soon as the corresponding input token is read (if you use a *unidirectional* encoder) | Same as above | Real‑time speech‑to‑text, live captioning where latency < 50 ms is mandatory |\n",
            "| **Tiny training sets** – LSTMs can converge with a few thousand parallel sentences, especially with word‑embedding pre‑training | Same as above | Low‑resource languages where no large multilingual transformer exists |\n",
            "| **Simple interpretability** – hidden state dynamics are easier to visualise for pedagogical purposes | Same as above | Teaching, research on recurrent dynamics |\n",
            "\n",
            "**Caveats:**  \n",
            "Even with these advantages, an LSTM encoder‑decoder will usually lag behind a modest‑size transformer (e.g., T5‑small) on BLEU/ROUGE scores once you have > 10 k parallel sentences. The recurrent bottleneck also makes training slower on GPUs because you cannot fully parallelise across time steps.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Decision Flowchart (quick checklist)\n",
            "\n",
            "1. **Do you have a GPU/TPU and can afford a few hundred MB of VRAM?**  \n",
            "   - **Yes →** Go with a Transformer encoder‑decoder (fine‑tune a pre‑trained checkpoint).  \n",
            "   - **No →** Consider an LSTM encoder‑decoder or a *tiny* transformer (e.g., **Distil‑T5**, **MiniLM‑seq2seq**) that fits your hardware.\n",
            "\n",
            "2. **How much parallel training data do you have?**  \n",
            "   - **> 10 k sentence pairs** (or you can augment with back‑translation) → Transformer.  \n",
            "   - **< 10 k** and you cannot obtain more → LSTM (or a very small transformer with heavy regularisation).\n",
            "\n",
            "3. **Is inference latency a hard real‑time constraint (< 30 ms per token)?**  \n",
            "   - **Yes** → LSTM (or a *causal* transformer with caching and a small hidden size).  \n",
            "   - **No** → Transformer (caching makes decoder inference ~ O(1) per token after the first step).\n",
            "\n",
            "4. **Do you need multilingual or domain‑transfer capability?**  \n",
            "   - **Yes** → Pre‑trained multilingual encoder‑decoder (mBART, mT5).  \n",
            "   - **No** → Monolingual transformer (T5, BART) or a task‑specific LSTM if resources are tiny.\n",
            "\n",
            "5. **Are you targeting an edge device with < 100 MB RAM?**  \n",
            "   - **Yes** → Tiny transformer (e.g., **Distil‑T5‑small**, **MiniLM‑seq2seq**) *or* LSTM with quantisation.  \n",
            "   - **No** → Full‑size transformer.\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Concrete Recommendations\n",
            "\n",
            "| Scenario | Recommended Model(s) | Why |\n",
            "|----------|----------------------|-----|\n",
            "| **Standard machine translation (English↔German) with a few hundred thousand parallel sentences** | **mBART‑large** (or **MarianMT**) – fine‑tune on your data. | State‑of‑the‑art cross‑attention, multilingual pre‑training, strong BLEU gains. |\n",
            "| **Summarisation of news articles (≈ 500‑word inputs) with ~ 50 k examples** | **Pegasus‑large** or **T5‑base** fine‑tuned. | Pegasus is optimised for summarisation; T5 gives flexible “text‑to‑text” interface. |\n",
            "| **Low‑resource language pair (≈ 5 k sentence pairs) and only a CPU server** | **LSTM encoder‑decoder** (2‑layer bidirectional encoder + 2‑layer decoder) *or* **Distil‑T5‑small** with aggressive dropout. | LSTM can converge with little data; Distil‑T5 may still beat it if you can afford a modest GPU for fine‑tuning. |\n",
            "| **On‑device speech‑to‑text (real‑time, < 30 ms latency, < 50 MB RAM)** | **Unidirectional LSTM encoder‑decoder** with quantised weights (int8). | Minimal memory, streaming-friendly, deterministic latency. |\n",
            "| **Multimodal captioning (image → text) where you already have a Vision Transformer (ViT) backbone** | **Encoder‑decoder Transformer**: ViT as encoder, a small decoder (e.g., **BART‑base** decoder) on top. | Keeps everything in the transformer family, easy to train end‑to‑end, benefits from cross‑attention. |\n",
            "| **Research prototype where you want to experiment with attention patterns** | **Transformer** (any size) – you can visualise attention maps, probe heads, etc. | LSTMs give no comparable attention visualisation. |\n",
            "\n",
            "---\n",
            "\n",
            "## 6. Implementation Tips (Transformer)\n",
            "\n",
            "1. **Use a pre‑trained checkpoint** – start with `t5-base`, `bart-large`, or `mbart-large-50`.  \n",
            "2. **Add a task‑specific prefix** (e.g., `\"translate English to German: \"` for T5) – this tells the model what to do.  \n",
            "3. **Fine‑tune with teacher‑forcing** (standard seq2seq loss) and a modest learning rate (1e‑4 to 5e‑5).  \n",
            "4. **Enable decoder caching** during inference (`use_cache=True` in 🤗 Transformers) to make generation O(1) per token.  \n",
            "5. **If sequence length > 512**, switch to an efficient attention variant (Longformer, Performer) or chunk the input and use hierarchical encoding.\n",
            "\n",
            "---\n",
            "\n",
            "## 7. Implementation Tips (LSTM)\n",
            "\n",
            "1. **Bidirectional encoder** (2‑layer Bi‑LSTM) → captures past & future context.  \n",
            "2. **Attention layer** on top of the encoder (Bahdanau or Luong) – this gives you a *soft* version of cross‑attention without the full transformer cost.  \n",
            "3. **Use teacher‑forcing** during training; schedule a gradual reduction of teacher‑forcing ratio to improve robustness.  \n",
            "4. **Apply gradient clipping** (e.g., `clipnorm=1.0`) to avoid exploding gradients.  \n",
            "5. **Quantise** (`torch.quantization`) for edge deployment.\n",
            "\n",
            "---\n",
            "\n",
            "## 8. Bottom Line\n",
            "\n",
            "- **If you have any GPU/TPU budget and at least a modest parallel corpus, go with a **Transformer encoder‑decoder** (fine‑tune a pre‑trained model).**  \n",
            "- **Reserve LSTMs for extreme low‑resource, ultra‑low‑memory, or strict streaming latency cases.**  \n",
            "\n",
            "In practice, the performance gap is usually **large** (BLEU/ROUGE improvements of 5‑15 points) in favour of Transformers, while the engineering effort to use them is minimal thanks to mature libraries. So, for most production or research seq2seq projects, the Transformer is the clear winner.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Simulate conversation with follow-up questions\n",
        "conversation = [\n",
        "    \"What are transformer models?\",\n",
        "    \"How do they differ from LSTM networks?\",\n",
        "    \"Which one should I use for a sequence-to-sequence task?\"\n",
        "]\n",
        "\n",
        "state = {\"messages\": []}\n",
        "\n",
        "for i, user_msg in enumerate(conversation, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Turn {i}: {user_msg}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    # Add user message to state\n",
        "    state[\"messages\"].append({\"role\": \"user\", \"content\": user_msg})\n",
        "    \n",
        "    # Get response\n",
        "    result = supervisor_agent.invoke(state)\n",
        "    \n",
        "    # Update state with full conversation history\n",
        "    state = result\n",
        "    \n",
        "    # Print only the final AI response\n",
        "    final_response = result[\"messages\"][-1].content\n",
        "    print(f\"Assistant: {final_response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced: Context Engineering\n",
        "\n",
        "Control how information flows between supervisor and subagents. By default, subagents receive only the request string. You can customize this to pass additional context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.tools import tool, ToolRuntime\n",
        "\n",
        "@tool\n",
        "def research_with_context(query: str, runtime: ToolRuntime) -> str:\n",
        "    \"\"\"Research information with full conversation context.\n",
        "    \n",
        "    This version passes the original user message to the subagent,\n",
        "    allowing it to resolve ambiguities and maintain conversational coherence.\n",
        "    \"\"\"\n",
        "    # Extract original user message from conversation history\n",
        "    original_message = next(\n",
        "        (msg for msg in runtime.state[\"messages\"] if msg.type == \"human\"),\n",
        "        None\n",
        "    )\n",
        "    \n",
        "    # Build enhanced prompt with context\n",
        "    enhanced_prompt = (\n",
        "        f\"Original user inquiry: {original_message.content if original_message else 'N/A'}\\n\\n\"\n",
        "        f\"Your specific task: {query}\"\n",
        "    )\n",
        "    \n",
        "    result = research_agent.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": enhanced_prompt}]\n",
        "    })\n",
        "    \n",
        "    return result[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create supervisor with context-aware subagent\n",
        "supervisor_with_context = create_agent(\n",
        "    llm,\n",
        "    tools=[research_with_context, analyze_comparison],\n",
        "    system_prompt=SUPERVISOR_PROMPT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response with context:\n",
            "### How Transformers Process Sequences – A Step‑by‑Step Walk‑through  \n",
            "\n",
            "Below is a concise yet complete description of the pipeline a Transformer (e.g., the original **Vaswani et al., 2017** model or its modern descendants) follows to turn an input sequence of symbols (words, sub‑words, characters, etc.) into contextualized representations. The core of this pipeline is the **self‑attention** mechanism, but it works together with several other components that together give the model its power.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Input Preparation  \n",
            "\n",
            "| Step | What happens | Why it matters |\n",
            "|------|--------------|----------------|\n",
            "| **Tokenization** | The raw text is split into discrete tokens (e.g., WordPiece, BPE, SentencePiece). | Provides a finite vocabulary that the model can embed. |\n",
            "| **Embedding lookup** | Each token index is mapped to a dense vector **\\(x_i \\in \\mathbb{R}^{d_{\\text{model}}}\\)** via a learned embedding matrix **\\(E \\in \\mathbb{R}^{|V|\\times d_{\\text{model}}}\\)**. | Gives each token a continuous representation that can be processed by linear algebra. |\n",
            "| **Positional encoding** | Since self‑attention is permutation‑invariant, a **positional signal** is added (or concatenated) to each token embedding: <br> \\(\\tilde{x}_i = x_i + \\text{PE}_i\\).  The classic sinusoidal PE is: <br> \\(\\text{PE}_{(i,2k)} = \\sin(i/10000^{2k/d_{\\text{model}}})\\) <br> \\(\\text{PE}_{(i,2k+1)} = \\cos(i/10000^{2k/d_{\\text{model}}})\\). | Gives the model a notion of order without recurrence or convolution. |\n",
            "| **Masking (optional)** | For encoder‑decoder models, a **causal mask** (upper‑triangular) is applied to prevent a token from attending to future positions. | Enforces autoregressive generation in decoders. |\n",
            "\n",
            "The result is a matrix **\\(X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}\\)** where **\\(L\\)** is the sequence length.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. The Self‑Attention Block  \n",
            "\n",
            "### 2.1. Linear projections to Queries, Keys, Values  \n",
            "\n",
            "For each token we compute three vectors:\n",
            "\n",
            "\\[\n",
            "Q = XW_Q,\\qquad K = XW_K,\\qquad V = XW_V\n",
            "\\]\n",
            "\n",
            "- **\\(W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}}\\times d_k}\\)** (often \\(d_k = d_v = d_{\\text{model}}/h\\) where **\\(h\\)** = number of heads).  \n",
            "- These projections are learned and allow the model to ask “what should I look for?” (queries) and “what do I have to offer?” (keys/values).\n",
            "\n",
            "### 2.2. Scaled Dot‑Product Attention  \n",
            "\n",
            "For a single head, the attention scores are:\n",
            "\n",
            "\\[\n",
            "\\text{scores}_{ij} = \\frac{Q_i \\cdot K_j^\\top}{\\sqrt{d_k}}\n",
            "\\]\n",
            "\n",
            "- The **\\(\\sqrt{d_k}\\)** scaling prevents the dot‑product from growing too large, which would push the softmax into regions with tiny gradients.\n",
            "\n",
            "Apply the (optional) mask and then softmax:\n",
            "\n",
            "\\[\n",
            "\\alpha_{ij} = \\text{softmax}_j(\\text{scores}_{ij}) = \\frac{\\exp(\\text{scores}_{ij})}{\\sum_{j'}\\exp(\\text{scores}_{ij'})}\n",
            "\\]\n",
            "\n",
            "The output for token *i* is the weighted sum of values:\n",
            "\n",
            "\\[\n",
            "\\text{head}_i = \\sum_{j=1}^{L} \\alpha_{ij} V_j\n",
            "\\]\n",
            "\n",
            "Collecting all heads gives a matrix **\\(H \\in \\mathbb{R}^{L \\times (h\\cdot d_v)}\\)**.\n",
            "\n",
            "### 2.3. Multi‑Head Attention  \n",
            "\n",
            "Instead of a single set of \\(Q,K,V\\), we compute **\\(h\\)** independent heads, each with its own projection matrices. The heads are concatenated and projected back to the model dimension:\n",
            "\n",
            "\\[\n",
            "\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W_O\n",
            "\\]\n",
            "\n",
            "- **\\(W_O \\in \\mathbb{R}^{(h\\cdot d_v) \\times d_{\\text{model}}}\\)**.  \n",
            "- Multi‑head attention lets the model attend to information from different representation subspaces (e.g., syntax vs. semantics) simultaneously.\n",
            "\n",
            "### 2.4. Residual Connection & Layer Normalization  \n",
            "\n",
            "The output of multi‑head attention is added to the original input (residual) and then normalized:\n",
            "\n",
            "\\[\n",
            "\\text{AttentionOut} = \\text{LayerNorm}(X + \\text{MultiHead}(X))\n",
            "\\]\n",
            "\n",
            "This stabilizes training and allows gradients to flow through many stacked layers.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Position‑wise Feed‑Forward Network (FFN)  \n",
            "\n",
            "Each token’s representation is passed through the same two‑layer MLP (applied **independently** to each position):\n",
            "\n",
            "\\[\n",
            "\\text{FFN}(z) = \\text{max}(0, zW_1 + b_1)W_2 + b_2\n",
            "\\]\n",
            "\n",
            "- Typically **\\(W_1 \\in \\mathbb{R}^{d_{\\text{model}}\\times d_{\\text{ff}}}\\)** with \\(d_{\\text{ff}} \\approx 4 \\times d_{\\text{model}}\\).  \n",
            "- The ReLU (or GELU) introduces non‑linearity.\n",
            "\n",
            "Again a residual + layer‑norm step follows:\n",
            "\n",
            "\\[\n",
            "\\text{FFNOut} = \\text{LayerNorm}(\\text{AttentionOut} + \\text{FFN}(\\text{AttentionOut}))\n",
            "\\]\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Stacking Layers  \n",
            "\n",
            "The **Encoder** consists of **\\(N\\)** identical layers, each containing the two sub‑layers described above (self‑attention → FFN).  \n",
            "\n",
            "The **Decoder** adds a second attention sub‑layer that attends to the encoder’s final outputs (cross‑attention) and also uses a causal mask on its own self‑attention.\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Output Generation  \n",
            "\n",
            "- **Encoder‑only models** (e.g., BERT) typically feed the final token representations into a classification head, token‑level predictor, or pooling layer.  \n",
            "- **Encoder‑decoder models** (e.g., T5, GPT‑style decoders) feed the decoder’s final hidden states into a linear projection + softmax to predict the next token distribution.\n",
            "\n",
            "---\n",
            "\n",
            "## 6. Key Intuitions About Sequence Processing  \n",
            "\n",
            "| Aspect | How the Transformer handles it |\n",
            "|--------|--------------------------------|\n",
            "| **Order** | Positional encodings inject order information; attention itself is order‑agnostic. |\n",
            "| **Long‑range dependencies** | Every token can attend directly to any other token in a single layer, so dependencies of arbitrary distance are captured without vanishing‑gradient issues. |\n",
            "| **Parallelism** | All tokens are processed simultaneously (matrix multiplications), unlike RNNs that are sequential. This enables massive GPU/TPU speed‑ups. |\n",
            "| **Contextualization** | After each layer, each token’s vector is a weighted mixture of *all* other tokens, gradually refining its meaning. |\n",
            "| **Scalability** | Complexity per layer is \\(O(L^2 d_{\\text{model}})\\) due to the attention matrix; recent variants (e.g., Longformer, Performer) modify this to sub‑quadratic cost for very long sequences. |\n",
            "\n",
            "---\n",
            "\n",
            "## 7. A Minimal Pseudocode Illustration  \n",
            "\n",
            "```python\n",
            "def transformer_encoder_layer(X, mask=None):\n",
            "    # 1. Multi‑head self‑attention\n",
            "    attn = multi_head_attention(Q=X, K=X, V=X, mask=mask)   # (L, d_model)\n",
            "    X = layer_norm(X + attn)                               # residual + norm\n",
            "\n",
            "    # 2. Position‑wise feed‑forward\n",
            "    ff = feed_forward(X)                                   # (L, d_model)\n",
            "    X = layer_norm(X + ff)                                 # residual + norm\n",
            "    return X\n",
            "\n",
            "def transformer_encoder(tokens, pos_enc, N):\n",
            "    # tokens: (L,) integer ids\n",
            "    X = embed(tokens) + pos_enc                           # (L, d_model)\n",
            "    for _ in range(N):\n",
            "        X = transformer_encoder_layer(X)\n",
            "    return X   # final contextual embeddings\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "## 8. Variations & Extensions (quick glance)\n",
            "\n",
            "| Variant | What changes in the sequence‑processing pipeline? |\n",
            "|--------|---------------------------------------------------|\n",
            "| **BERT** | Encoder‑only, bidirectional self‑attention; uses *masked language modeling* pre‑training. |\n",
            "| **GPT** | Decoder‑only, causal mask only; autoregressive generation. |\n",
            "| **Transformer‑XL** | Adds a *segment‑level recurrence* mechanism to reuse hidden states from previous chunks, extending effective context beyond the fixed length. |\n",
            "| **Longformer / BigBird** | Replaces full attention with *sparse* patterns (local + global) to reduce \\(O(L^2)\\) to \\(O(L)\\) or \\(O(L\\log L)\\). |\n",
            "| **Performer** | Uses *kernel‑based* attention approximations to achieve linear time/space. |\n",
            "| **Vision Transformer (ViT)** | Treats image patches as “tokens”; same pipeline but with a learned *class token* for classification. |\n",
            "\n",
            "---\n",
            "\n",
            "## 9. TL;DR Summary  \n",
            "\n",
            "1. **Token → Embedding + Positional Encoding** → matrix **\\(X\\)**.  \n",
            "2. **Self‑attention** computes queries/keys/values, forms attention weights, and mixes values → contextual vectors.  \n",
            "3. **Multi‑head** lets the model look at different aspects simultaneously.  \n",
            "4. **Residual + LayerNorm** stabilizes training.  \n",
            "5. **Feed‑forward** adds non‑linearity per position.  \n",
            "6. **Stack N layers** → deep, richly contextualized representations.  \n",
            "7. **Decoder** (if present) adds causal masking and cross‑attention to the encoder output.  \n",
            "8. Final vectors are fed to task‑specific heads (classification, generation, etc.).\n",
            "\n",
            "That’s the essence of how a Transformer processes a sequence: by repeatedly re‑weighting and mixing token representations through attention, while preserving order via positional encodings and deepening the model with stacked, normalized layers.\n"
          ]
        }
      ],
      "source": [
        "# Test with ambiguous follow-up\n",
        "test_state = {\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Tell me about transformer attention mechanisms\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Transformers use self-attention...\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you elaborate on how it processes sequences?\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "result = supervisor_with_context.invoke(test_state)\n",
        "print(\"Response with context:\")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Architectural Patterns\n",
        "\n",
        "**1. Tool Per Agent Pattern:**\n",
        "- Each subagent wrapped as a distinct tool\n",
        "- Fine-grained control over input/output\n",
        "- Clear responsibility boundaries\n",
        "\n",
        "**2. Context Isolation:**\n",
        "- Subagents operate in clean context windows\n",
        "- Prevents context pollution in main conversation\n",
        "- Supervisor maintains global state\n",
        "\n",
        "**3. Information Flow:**\n",
        "- Supervisor passes queries to subagents\n",
        "- Subagents return only final results\n",
        "- Supervisor synthesizes multiple results\n",
        "\n",
        "**When to Use This Pattern:**\n",
        "- Multiple distinct domains requiring specialized handling\n",
        "- Complex tasks requiring sequential subagent coordination\n",
        "- Need centralized control flow\n",
        "- Subagents don't need direct user interaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "- Building specialized subagents with domain-specific tools\n",
        "- Wrapping subagents as tools for supervisor orchestration\n",
        "- Creating a supervisor that coordinates multiple subagents\n",
        "- Handling both simple and complex multi-domain queries\n",
        "- Context engineering for enhanced subagent capabilities\n",
        "\n",
        "The subagents pattern provides clean separation of concerns, easy extensibility, and centralized workflow control."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
