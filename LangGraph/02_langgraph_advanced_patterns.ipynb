{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "### LangGraph - Persistence, State Management & Real-World Applications\n",
        "\n",
        "**Building on LangGraph Building Blocks**\n",
        "\n",
        "This notebook continues from `langGraph_buildingblocks.ipynb` and covers:\n",
        "- Persistence with checkpointers (save and resume execution)\n",
        "- Thread-based state management (conversation memory)\n",
        "- Cross-thread memory with stores\n",
        "- Human-in-the-loop workflows with interrupts\n",
        "- State inspection and time travel\n",
        "- Production-ready patterns\n",
        "\n",
        "**Prerequisites**: Complete the building blocks notebook first to understand State, Nodes, Edges, and Command.\n",
        "\n",
        "**Key Concepts**:\n",
        "- **Checkpointers**: Save graph state at each step for resumption\n",
        "- **Threads**: Unique conversation IDs that track state history\n",
        "- **Stores**: Cross-thread memory for user preferences/data\n",
        "- **Interrupts**: Pause execution for human input/approval\n",
        "\n",
        "We'll use Groq for LLMs and build practical chatbot examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports successful\n"
          ]
        }
      ],
      "source": [
        "# Core imports for advanced LangGraph patterns\n",
        "\n",
        "from typing import TypedDict, Annotated, Literal\n",
        "from typing_extensions import TypedDict as ExtTypedDict\n",
        "from operator import add\n",
        "import uuid\n",
        "\n",
        "# LangGraph core\n",
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "# Checkpointing and storage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "\n",
        "# LangChain\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.store.base import BaseStore\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "print(\"Imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "config",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŒŸâœ¨ *A ripple of code whispers through the digital ether...* âœ¨ðŸŒŸ  \n",
            "\n",
            "**â€œHey there, LangChain! ðŸš€â€**  \n",
            "\n",
            "â€” a friendly ping from the world of prompts, ready to link ideas together!\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Load .env from project root even if the kernel starts in a subfolder\n",
        "env_candidates = [Path.cwd(), Path.cwd().parent]\n",
        "env_path = next((p / \".env\" for p in env_candidates if (p / \".env\").exists()), None)\n",
        "if not env_path:\n",
        "    raise FileNotFoundError(\"Could not find .env next to the project root\")\n",
        "load_dotenv(env_path, override=True)\n",
        "\n",
        "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
        "if not groq_key:\n",
        "    raise RuntimeError(\"GROQ_API_KEY missing; check your .env\")\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_key.strip()\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"Say 'Hello, LangChain!' in a creative way.\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "persistence-intro",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Persistence - Why Checkpointers Matter\n",
        "\n",
        "**The Problem Without Persistence**:\n",
        "- Graphs forget everything after execution\n",
        "- Can't pause and resume\n",
        "- No conversation history\n",
        "- Can't handle failures gracefully\n",
        "\n",
        "**What Checkpointers Provide**:\n",
        "1. **State snapshots**: Save state at each graph step\n",
        "2. **Resumption**: Pick up where you left off\n",
        "3. **Memory**: Maintain conversation context\n",
        "4. **Human-in-the-loop**: Pause for approval/input\n",
        "5. **Time travel**: Inspect and replay any step\n",
        "\n",
        "**Key Concept - Threads**:\n",
        "- Each conversation gets a unique `thread_id`\n",
        "- All checkpoints for that conversation are stored in that thread\n",
        "- Different threads = different conversations with separate histories"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "simple-persistence",
      "metadata": {},
      "source": [
        "## Part 2: Simple Persistence Example\n",
        "\n",
        "Let's build a basic chatbot that remembers conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "simple-chatbot-state",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State defined\n"
          ]
        }
      ],
      "source": [
        "# Define state for our chatbot\n",
        "# Using MessagesState - a built-in that includes messages with add_messages reducer\n",
        "\n",
        "class ChatbotState(MessagesState):\n",
        "    # MessagesState already includes: messages: Annotated[list, add_messages]\n",
        "    # We can add additional fields\n",
        "    user_name: str  # Track user's name\n",
        "\n",
        "print(\"State defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "simple-chatbot-node",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot node defined\n"
          ]
        }
      ],
      "source": [
        "# Define chatbot node\n",
        "# This node has access to full conversation history through state\n",
        "\n",
        "def chatbot_node(state: ChatbotState) -> dict:\n",
        "    \"\"\"\n",
        "    Simple chatbot that responds to user messages.\n",
        "    Has access to full message history via state['messages'].\n",
        "    \"\"\"\n",
        "    # Get conversation history\n",
        "    messages = state['messages']\n",
        "    \n",
        "    # Add system message if user name is known\n",
        "    if state.get('user_name'):\n",
        "        system_msg = SystemMessage(content=f\"You are a helpful assistant. The user's name is {state['user_name']}.\")\n",
        "        messages = [system_msg] + messages\n",
        "    \n",
        "    # Generate response using full conversation context\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    # Return the response (add_messages will append it)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "print(\"Chatbot node defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "simple-chatbot-graph",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph compiled with checkpointer\n"
          ]
        }
      ],
      "source": [
        "# Build graph WITH checkpointer\n",
        "# This is the key difference - we add a checkpointer\n",
        "\n",
        "# Create checkpointer - saves state in memory\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Build graph\n",
        "workflow = StateGraph(ChatbotState)\n",
        "workflow.add_node(\"chatbot\", chatbot_node)\n",
        "workflow.add_edge(START, \"chatbot\")\n",
        "workflow.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Compile WITH checkpointer - this enables persistence\n",
        "simple_chatbot = workflow.compile(checkpointer=checkpointer)\n",
        "\n",
        "print(\"Graph compiled with checkpointer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test-simple-chatbot",
      "metadata": {},
      "source": [
        "## Testing the Persistent Chatbot\n",
        "\n",
        "Notice how we pass `thread_id` in the config - this is REQUIRED when using checkpointers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "chatbot-conversation-1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== First Exchange ===\n",
            "HumanMessage: Hi, my name is Alice\n",
            "AIMessage: Hello Alice! Nice to meet you. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# Start a conversation\n",
        "# The thread_id identifies this specific conversation\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "\n",
        "# First message\n",
        "result1 = simple_chatbot.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Hi, my name is Alice\")],\n",
        "        \"user_name\": \"\"\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"=== First Exchange ===\")\n",
        "for msg in result1[\"messages\"]:\n",
        "    print(f\"{msg.__class__.__name__}: {msg.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "chatbot-conversation-2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Follow-up (Should remember name) ===\n",
            "AI: Your name is Alice.\n"
          ]
        }
      ],
      "source": [
        "# Continue the SAME conversation\n",
        "# Notice: We only pass the new message, not the full history\n",
        "# The checkpointer automatically loads previous messages\n",
        "\n",
        "result2 = simple_chatbot.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"What's my name?\")]\n",
        "    },\n",
        "    config=config  # Same thread_id\n",
        ")\n",
        "\n",
        "print(\"\\n=== Follow-up (Should remember name) ===\")\n",
        "print(f\"AI: {result2['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "chatbot-new-thread",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== New Thread (Should NOT know name) ===\n",
            "AI: I donâ€™t have any information about your name. Could you let me know what youâ€™d like to be called?\n"
          ]
        }
      ],
      "source": [
        "# Start a DIFFERENT conversation (different thread)\n",
        "# This is a completely separate conversation with no shared history\n",
        "\n",
        "new_config = {\"configurable\": {\"thread_id\": \"conversation_2\"}}\n",
        "\n",
        "result3 = simple_chatbot.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"What's my name?\")]\n",
        "    },\n",
        "    config=new_config  # Different thread_id\n",
        ")\n",
        "\n",
        "print(\"\\n=== New Thread (Should NOT know name) ===\")\n",
        "print(f\"AI: {result3['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "state-inspection",
      "metadata": {},
      "source": [
        "## Part 3: State Inspection - Looking Inside the Checkpoint\n",
        "\n",
        "Checkpointers save the complete state at each step.\n",
        "We can inspect this state at any time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "get-state",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Current State Inspection ===\n",
            "\n",
            "State values: {'messages': [HumanMessage(content='Hi, my name is Alice', additional_kwargs={}, response_metadata={}, id='3273c308-a742-4a8c-a6d5-6271651a1a81'), AIMessage(content='Hello Alice! Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hi, my name is Alice\". We should greet them, maybe ask how we can help. No policy issues.'}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 77, 'total_tokens': 134, 'completion_time': 0.13295972, 'completion_tokens_details': {'reasoning_tokens': 33}, 'prompt_time': 0.003087279, 'prompt_tokens_details': None, 'queue_time': 0.054536771, 'total_time': 0.136046999}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_70d048ba3c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--0e5054c2-3aa2-41b1-a2bd-bedf11aed54a-0', usage_metadata={'input_tokens': 77, 'output_tokens': 57, 'total_tokens': 134, 'output_token_details': {'reasoning': 33}}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='e4cd0d1a-016f-485e-9b59-c8b46d06c648'), AIMessage(content='Your name is Alice.', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" The assistant should respond with \"Your name is Alice.\" This is straightforward.'}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 106, 'total_tokens': 144, 'completion_time': 0.084195447, 'completion_tokens_details': {'reasoning_tokens': 24}, 'prompt_time': 0.004308027, 'prompt_tokens_details': None, 'queue_time': 0.049272443, 'total_time': 0.088503474}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a28df4bce5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--c6cfe915-46dd-452f-bef4-70a1a0d0c82a-0', usage_metadata={'input_tokens': 106, 'output_tokens': 38, 'total_tokens': 144, 'output_token_details': {'reasoning': 24}})], 'user_name': ''}\n",
            "\n",
            "Next nodes to execute: ()\n",
            "\n",
            "Checkpoint ID: 1f0d6b43-f484-6cc2-8004-45945df01590\n",
            "\n",
            "Number of messages: 4\n"
          ]
        }
      ],
      "source": [
        "# Get the current state of a thread\n",
        "# This returns a StateSnapshot object\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "current_state = simple_chatbot.get_state(config)\n",
        "\n",
        "print(\"=== Current State Inspection ===\")\n",
        "print(f\"\\nState values: {current_state.values}\")\n",
        "print(f\"\\nNext nodes to execute: {current_state.next}\")\n",
        "print(f\"\\nCheckpoint ID: {current_state.config['configurable']['checkpoint_id']}\")\n",
        "print(f\"\\nNumber of messages: {len(current_state.values['messages'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "get-state-history",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== State History (Total checkpoints: 6) ===\n",
            "\n",
            "Checkpoint 1:\n",
            "  Messages: 4\n",
            "  Next: ()\n",
            "  Created: 2025-12-11T17:10:09.264529+00:00\n",
            "\n",
            "Checkpoint 2:\n",
            "  Messages: 3\n",
            "  Next: ('chatbot',)\n",
            "  Created: 2025-12-11T17:10:09.103672+00:00\n",
            "\n",
            "Checkpoint 3:\n",
            "  Messages: 2\n",
            "  Next: ('__start__',)\n",
            "  Created: 2025-12-11T17:10:09.102832+00:00\n"
          ]
        }
      ],
      "source": [
        "# Get full state history for a thread\n",
        "# Returns all checkpoints in reverse chronological order\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "history = list(simple_chatbot.get_state_history(config))\n",
        "\n",
        "print(f\"=== State History (Total checkpoints: {len(history)}) ===\")\n",
        "\n",
        "for i, checkpoint in enumerate(history[:3]):  # Show first 3\n",
        "    print(f\"\\nCheckpoint {i+1}:\")\n",
        "    print(f\"  Messages: {len(checkpoint.values.get('messages', []))}\")\n",
        "    print(f\"  Next: {checkpoint.next}\")\n",
        "    print(f\"  Created: {checkpoint.created_at}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "human-in-loop-intro",
      "metadata": {},
      "source": [
        "## Part 4: Human-in-the-Loop with Interrupts\n",
        "\n",
        "**Use Cases**:\n",
        "- Get approval before taking actions\n",
        "- Collect additional information\n",
        "- Review AI decisions\n",
        "- Multi-step workflows with human oversight\n",
        "\n",
        "**How it Works**:\n",
        "1. Node calls `interrupt()` to pause execution\n",
        "2. Graph saves current state and stops\n",
        "3. Human reviews state and provides input\n",
        "4. Graph resumes from exact same point with human input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "approval-bot-state",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approval bot state defined\n"
          ]
        }
      ],
      "source": [
        "# State for approval bot\n",
        "# We'll build a bot that needs approval before sending emails\n",
        "\n",
        "class ApprovalBotState(MessagesState):\n",
        "    email_draft: str  # The drafted email\n",
        "    approved: bool    # Whether human approved it\n",
        "    recipient: str    # Who to send to\n",
        "\n",
        "print(\"Approval bot state defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "approval-bot-nodes",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approval workflow nodes defined\n"
          ]
        }
      ],
      "source": [
        "# Nodes for approval workflow\n",
        "\n",
        "def draft_email_node(state: ApprovalBotState) -> dict:\n",
        "    \"\"\"Generate email draft based on user request\"\"\"\n",
        "    messages = state['messages']\n",
        "    recipient = state.get('recipient', 'recipient@example.com')\n",
        "    \n",
        "    # Ask LLM to draft email\n",
        "    prompt = f\"Draft a professional email to {recipient} based on this request: {messages[-1].content}\"\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    \n",
        "    return {\n",
        "        \"email_draft\": response.content,\n",
        "        \"messages\": [AIMessage(content=f\"I've drafted an email. Please review it.\")]\n",
        "    }\n",
        "\n",
        "def approval_node(state: ApprovalBotState) -> Command[Literal[\"send_email\", END]]:\n",
        "    \"\"\"\n",
        "    Pause execution and wait for human approval.\n",
        "    This is the key node that uses interrupt().\n",
        "    \"\"\"\n",
        "    # Show the draft to human and wait for approval\n",
        "    # interrupt() pauses execution here\n",
        "    human_response = interrupt(\n",
        "        {\n",
        "            \"draft\": state['email_draft'],\n",
        "            \"recipient\": state['recipient'],\n",
        "            \"question\": \"Do you approve this email? (yes/no)\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # When resumed, human_response contains the human's input\n",
        "    approved = human_response.get('approved', False)\n",
        "    \n",
        "    # Route based on approval\n",
        "    if approved:\n",
        "        return Command(\n",
        "            update={\"approved\": True},\n",
        "            goto=\"send_email\"\n",
        "        )\n",
        "    else:\n",
        "        return Command(\n",
        "            update={\n",
        "                \"approved\": False,\n",
        "                \"messages\": [AIMessage(content=\"Email cancelled.\")]\n",
        "            },\n",
        "            goto=END\n",
        "        )\n",
        "\n",
        "def send_email_node(state: ApprovalBotState) -> dict:\n",
        "    \"\"\"Simulate sending email\"\"\"\n",
        "    # In production, this would actually send the email\n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=f\"Email sent to {state['recipient']}!\")]\n",
        "    }\n",
        "\n",
        "print(\"Approval workflow nodes defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "approval-bot-graph",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approval bot compiled\n"
          ]
        }
      ],
      "source": [
        "# Build approval bot graph\n",
        "\n",
        "# Must use checkpointer for interrupts to work\n",
        "approval_checkpointer = MemorySaver()\n",
        "\n",
        "workflow = StateGraph(ApprovalBotState)\n",
        "workflow.add_node(\"draft_email\", draft_email_node)\n",
        "workflow.add_node(\"approval\", approval_node)\n",
        "workflow.add_node(\"send_email\", send_email_node)\n",
        "\n",
        "workflow.add_edge(START, \"draft_email\")\n",
        "workflow.add_edge(\"draft_email\", \"approval\")\n",
        "workflow.add_edge(\"send_email\", END)\n",
        "\n",
        "approval_bot = workflow.compile(checkpointer=approval_checkpointer)\n",
        "\n",
        "print(\"Approval bot compiled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "test-approval-bot",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Workflow Paused ===\n",
            "Graph stopped at interrupt. Check state:\n",
            "\n",
            "Interrupt data: (Interrupt(value={'draft': '**Subject:** Thank You for Todayâ€™s Meeting  \\n\\n**To:** boss@company.com  \\n\\n---\\n\\nDear [Bossâ€™s Name],\\n\\nI wanted to thank you for taking the time to meet with me today. I appreciate the opportunity to discuss [briefly mention the main topic or project, e.g., â€œthe upcoming product launch strategyâ€] and to receive your valuable insights.\\n\\nYour guidance on [specific point discussed, e.g., â€œprioritizing the market research phase and aligning our timeline with the sales teamâ€™s objectivesâ€] was especially helpful. I am confident that, with your direction, we can move forward effectively and achieve the desired outcomes.\\n\\nAs a next step, I will [outline any agreedâ€‘upon actions, e.g., â€œprepare a detailed project plan and share it with the team by Fridayâ€]. Please let me know if there are any additional items youâ€™d like me to address.\\n\\nThank you again for your support and leadership. I look forward to keeping you updated on our progress.\\n\\nBest regards,\\n\\n[Your Full Name]  \\n[Your Position]  \\n[Your Department]  \\n[Phone Number]  \\n[Company Name]  ', 'recipient': 'boss@company.com', 'question': 'Do you approve this email? (yes/no)'}, id='3efd0c8005b0c3cb0bf0bacd13f1e97d'),)\n"
          ]
        }
      ],
      "source": [
        "# Test the approval workflow\n",
        "# Step 1: Start the workflow - it will pause at approval_node\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"email_1\"}}\n",
        "\n",
        "initial_result = approval_bot.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Send a thank you email for the meeting\")],\n",
        "        \"recipient\": \"boss@company.com\",\n",
        "        \"email_draft\": \"\",\n",
        "        \"approved\": False\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"=== Workflow Paused ===\")\n",
        "print(\"Graph stopped at interrupt. Check state:\")\n",
        "\n",
        "# Check what the interrupt is asking for\n",
        "state = approval_bot.get_state(config)\n",
        "print(f\"\\nInterrupt data: {state.tasks[0].interrupts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "resume-approval-bot",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Workflow Resumed ===\n",
            "Final message: Email sent to boss@company.com!\n",
            "Approved: True\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Human reviews and approves\n",
        "# Resume execution with Command.resume\n",
        "\n",
        "# Simulate human approval\n",
        "human_decision = Command(\n",
        "    resume={\"approved\": True}  # Human approved the email\n",
        ")\n",
        "\n",
        "# Resume the graph\n",
        "final_result = approval_bot.invoke(\n",
        "    human_decision,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"=== Workflow Resumed ===\")\n",
        "print(f\"Final message: {final_result['messages'][-1].content}\")\n",
        "print(f\"Approved: {final_result['approved']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cross-thread-memory",
      "metadata": {},
      "source": [
        "## Part 5: Cross-Thread Memory with Stores\n",
        "\n",
        "**The Problem**:\n",
        "- Checkpointers save state per thread (per conversation)\n",
        "- What if we want to remember things ACROSS all conversations?\n",
        "- Example: User preferences, profile info, learned facts\n",
        "\n",
        "**The Solution - Stores**:\n",
        "- Store data that persists across ALL threads\n",
        "- Namespaced by user_id or other identifier\n",
        "- Perfect for user profiles, preferences, long-term memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "store-setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory store created\n"
          ]
        }
      ],
      "source": [
        "# Create a store for cross-thread memory\n",
        "# This will remember information across different conversations\n",
        "\n",
        "memory_store = InMemoryStore()\n",
        "\n",
        "print(\"Memory store created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "store-state",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory bot state defined\n"
          ]
        }
      ],
      "source": [
        "# State for chatbot with cross-thread memory\n",
        "\n",
        "class MemoryBotState(MessagesState):\n",
        "    user_id: str  # Identifies the user across threads\n",
        "\n",
        "print(\"Memory bot state defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "store-nodes",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory-enabled node defined\n"
          ]
        }
      ],
      "source": [
        "# Nodes that use the store\n",
        "\n",
        "def chatbot_with_memory(state: MemoryBotState, config: RunnableConfig, *, store: BaseStore) -> dict:\n",
        "    \"\"\"\n",
        "    Chatbot that can access and update cross-thread memory.\n",
        "    Notice the 'store' parameter - this gives us access to the memory store.\n",
        "    \"\"\"\n",
        "    user_id = state['user_id']\n",
        "    messages = state['messages']\n",
        "    \n",
        "    # Define namespace for this user's memories\n",
        "    namespace = (user_id, \"preferences\")\n",
        "    \n",
        "    # Search for relevant memories\n",
        "    # This searches across ALL conversations with this user\n",
        "    memories = store.search(namespace)\n",
        "    \n",
        "    # Build context from memories\n",
        "    memory_context = \"\"\n",
        "    if memories:\n",
        "        memory_context = \"\\n\".join([\n",
        "            f\"- {mem.value.get('fact', '')}\" \n",
        "            for mem in memories\n",
        "        ])\n",
        "    \n",
        "    # Add system message with memory context\n",
        "    system_content = \"You are a helpful assistant.\"\n",
        "    if memory_context:\n",
        "        system_content += f\"\\n\\nWhat you know about this user:\\n{memory_context}\"\n",
        "    \n",
        "    full_messages = [SystemMessage(content=system_content)] + messages\n",
        "    \n",
        "    # Generate response\n",
        "    response = llm.invoke(full_messages)\n",
        "    \n",
        "    # Check if user shared new information to remember\n",
        "    last_msg = messages[-1].content.lower()\n",
        "    \n",
        "    # Simple logic to detect preferences (in production, use an LLM)\n",
        "    if \"my favorite\" in last_msg or \"i like\" in last_msg or \"i prefer\" in last_msg:\n",
        "        # Store this as a new memory\n",
        "        memory_id = str(uuid.uuid4())\n",
        "        store.put(\n",
        "            namespace,\n",
        "            memory_id,\n",
        "            {\"fact\": messages[-1].content}\n",
        "        )\n",
        "    \n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "print(\"Memory-enabled node defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "store-graph",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory bot compiled with checkpointer and store\n"
          ]
        }
      ],
      "source": [
        "# Build graph with BOTH checkpointer AND store\n",
        "\n",
        "memory_checkpointer = MemorySaver()\n",
        "\n",
        "workflow = StateGraph(MemoryBotState)\n",
        "workflow.add_node(\"chatbot\", chatbot_with_memory)\n",
        "workflow.add_edge(START, \"chatbot\")\n",
        "workflow.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Compile with BOTH checkpointer (for thread state) and store (for cross-thread memory)\n",
        "memory_bot = workflow.compile(\n",
        "    checkpointer=memory_checkpointer,\n",
        "    store=memory_store  # This is the key addition\n",
        ")\n",
        "\n",
        "print(\"Memory bot compiled with checkpointer and store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "test-memory-bot-1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== First Conversation ===\n",
            "User: My favorite color is blue\n",
            "AI: Thatâ€™s a great choice! Blue is often associated with calmness, creativity, and depth. Do you have a particular shade of blue you love most?\n"
          ]
        }
      ],
      "source": [
        "# Test 1: First conversation - share a preference\n",
        "\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        \"thread_id\": \"user_bob_conversation_1\",\n",
        "        \"user_id\": \"user_bob\"  # This identifies the user\n",
        "    }\n",
        "}\n",
        "\n",
        "result1 = memory_bot.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"My favorite color is blue\")],\n",
        "        \"user_id\": \"user_bob\"\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"=== First Conversation ===\")\n",
        "print(f\"User: My favorite color is blue\")\n",
        "print(f\"AI: {result1['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "test-memory-bot-2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Different Conversation (Should Remember) ===\n",
            "User: What's my favorite color?\n",
            "AI: Your favorite color is blue.\n"
          ]
        }
      ],
      "source": [
        "# Test 2: DIFFERENT conversation (different thread) - should still remember\n",
        "\n",
        "new_config = {\n",
        "    \"configurable\": {\n",
        "        \"thread_id\": \"user_bob_conversation_2\",  # Different thread!\n",
        "        \"user_id\": \"user_bob\"  # Same user\n",
        "    }\n",
        "}\n",
        "\n",
        "result2 = memory_bot.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"What's my favorite color?\")],\n",
        "        \"user_id\": \"user_bob\"\n",
        "    },\n",
        "    config=new_config\n",
        ")\n",
        "\n",
        "print(\"\\n=== Different Conversation (Should Remember) ===\")\n",
        "print(f\"User: What's my favorite color?\")\n",
        "print(f\"AI: {result2['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "inspect-store",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Stored Memories ===\n",
            "\n",
            "Memory ID: b5cd655b-46f2-4c1d-8dfa-431718109cf4\n",
            "Fact: My favorite color is blue\n",
            "Created: 2025-12-11 17:10:10.895478+00:00\n",
            "\n",
            "Memory ID: 0072b56b-1b28-44ba-94e7-519f7ebbe170\n",
            "Fact: What's my favorite color?\n",
            "Created: 2025-12-11 17:10:11.270532+00:00\n"
          ]
        }
      ],
      "source": [
        "# Inspect what's in the store\n",
        "\n",
        "namespace = (\"user_bob\", \"preferences\")\n",
        "all_memories = memory_store.search(namespace)\n",
        "\n",
        "print(\"=== Stored Memories ===\")\n",
        "for mem in all_memories:\n",
        "    print(f\"\\nMemory ID: {mem.key}\")\n",
        "    print(f\"Fact: {mem.value['fact']}\")\n",
        "    print(f\"Created: {mem.created_at}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "production-patterns",
      "metadata": {},
      "source": [
        "## Part 6: Production-Ready Patterns\n",
        "\n",
        "**Key Production Considerations**:\n",
        "1. Use persistent storage (Postgres, SQLite) instead of in-memory\n",
        "2. Implement proper error handling and retry logic\n",
        "3. Add monitoring and logging\n",
        "4. Handle concurrent access to threads\n",
        "5. Implement cleanup for old threads\n",
        "\n",
        "Let's build a production-ready chatbot pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "production-state",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Production state defined\n"
          ]
        }
      ],
      "source": [
        "# Production state with metadata\n",
        "\n",
        "class ProductionBotState(MessagesState):\n",
        "    user_id: str\n",
        "    session_id: str\n",
        "    error_count: int  # Track errors for retry logic\n",
        "    last_action: str  # Track what we last did\n",
        "\n",
        "print(\"Production state defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "production-nodes",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Production node with error handling defined\n"
          ]
        }
      ],
      "source": [
        "# Production nodes with error handling\n",
        "\n",
        "def production_chatbot_node(state: ProductionBotState, config: RunnableConfig) -> dict:\n",
        "    \"\"\"\n",
        "    Production-ready chatbot with error handling.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        messages = state['messages']\n",
        "        \n",
        "        # Add metadata to context\n",
        "        metadata = config.get('metadata', {})\n",
        "        session_info = f\"Session: {state.get('session_id', 'unknown')}\"\n",
        "        \n",
        "        # Generate response\n",
        "        response = llm.invoke(messages)\n",
        "        \n",
        "        return {\n",
        "            \"messages\": [response],\n",
        "            \"last_action\": \"chat_response\",\n",
        "            \"error_count\": 0  # Reset on success\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Log error in production\n",
        "        print(f\"Error in chatbot: {e}\")\n",
        "        \n",
        "        error_count = state.get('error_count', 0) + 1\n",
        "        \n",
        "        # If too many errors, give up\n",
        "        if error_count >= 3:\n",
        "            return {\n",
        "                \"messages\": [AIMessage(content=\"I'm having technical difficulties. Please try again later.\")],\n",
        "                \"last_action\": \"error_max_retries\",\n",
        "                \"error_count\": error_count\n",
        "            }\n",
        "        \n",
        "        # Otherwise, increment error count and continue\n",
        "        return {\n",
        "            \"messages\": [AIMessage(content=\"I encountered an error. Let me try again.\")],\n",
        "            \"last_action\": \"error_retry\",\n",
        "            \"error_count\": error_count\n",
        "        }\n",
        "\n",
        "print(\"Production node with error handling defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "production-graph",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Production bot compiled\n"
          ]
        }
      ],
      "source": [
        "# Build production graph\n",
        "\n",
        "prod_checkpointer = MemorySaver()\n",
        "\n",
        "workflow = StateGraph(ProductionBotState)\n",
        "workflow.add_node(\"chatbot\", production_chatbot_node)\n",
        "workflow.add_edge(START, \"chatbot\")\n",
        "workflow.add_edge(\"chatbot\", END)\n",
        "\n",
        "production_bot = workflow.compile(checkpointer=prod_checkpointer)\n",
        "\n",
        "print(\"Production bot compiled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "test-production",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Production Bot Test ===\n",
            "Response: Hello! How can I assist you today?\n",
            "Last action: chat_response\n",
            "Error count: 0\n"
          ]
        }
      ],
      "source": [
        "# Test production bot\n",
        "\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        \"thread_id\": \"prod_session_1\",\n",
        "        \"user_id\": \"user_123\"\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"environment\": \"production\",\n",
        "        \"version\": \"1.0\"\n",
        "    }\n",
        "}\n",
        "\n",
        "result = production_bot.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Hello\")],\n",
        "        \"user_id\": \"user_123\",\n",
        "        \"session_id\": \"prod_session_1\",\n",
        "        \"error_count\": 0,\n",
        "        \"last_action\": \"init\"\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"=== Production Bot Test ===\")\n",
        "print(f\"Response: {result['messages'][-1].content}\")\n",
        "print(f\"Last action: {result['last_action']}\")\n",
        "print(f\"Error count: {result['error_count']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "state-update",
      "metadata": {},
      "source": [
        "## Part 7: Advanced State Management - Update State\n",
        "\n",
        "Sometimes you need to manually modify the graph state.\n",
        "This is useful for:\n",
        "- Correcting mistakes\n",
        "- Adding context mid-conversation\n",
        "- Implementing human edits\n",
        "- Branching conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "update-state-example",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Current State ===\n",
            "Messages: 4\n",
            "\n",
            "After update - Messages: 5\n",
            "Latest message: Remember to be extra friendly\n"
          ]
        }
      ],
      "source": [
        "# Get current state\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "current_state = simple_chatbot.get_state(config)\n",
        "\n",
        "print(\"=== Current State ===\")\n",
        "print(f\"Messages: {len(current_state.values['messages'])}\")\n",
        "\n",
        "# Update state - add a system message\n",
        "simple_chatbot.update_state(\n",
        "    config,\n",
        "    {\n",
        "        \"messages\": [SystemMessage(content=\"Remember to be extra friendly\")]\n",
        "    }\n",
        ")\n",
        "\n",
        "# Check updated state\n",
        "updated_state = simple_chatbot.get_state(config)\n",
        "print(f\"\\nAfter update - Messages: {len(updated_state.values['messages'])}\")\n",
        "print(f\"Latest message: {updated_state.values['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "streaming",
      "metadata": {},
      "source": [
        "## Part 8: Streaming with Persistence\n",
        "\n",
        "Streaming works seamlessly with checkpointers.\n",
        "Each streamed chunk is still checkpointed properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "streaming-example",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Streaming with Persistence ===\n",
            "\n",
            "[Node: chatbot]\n",
            "Output: ## Transformers in Deep Learning â€“ A Highâ€‘Level Overview  \n",
            "\n",
            "Transformers have become the dominant ar...\n",
            "\n",
            "State saved: 2 messages\n"
          ]
        }
      ],
      "source": [
        "# Stream responses with persistence\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"stream_conversation\"}}\n",
        "\n",
        "print(\"=== Streaming with Persistence ===\")\n",
        "\n",
        "for chunk in simple_chatbot.stream(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Tell me about transformers in deep learning\")],\n",
        "        \"user_name\": \"Student\"\n",
        "    },\n",
        "    config=config,\n",
        "    stream_mode=\"updates\"  # Stream each node's output\n",
        "):\n",
        "    for node_name, node_output in chunk.items():\n",
        "        print(f\"\\n[Node: {node_name}]\")\n",
        "        if 'messages' in node_output:\n",
        "            print(f\"Output: {node_output['messages'][-1].content[:100]}...\")\n",
        "\n",
        "# Verify state was saved\n",
        "state = simple_chatbot.get_state(config)\n",
        "print(f\"\\nState saved: {len(state.values['messages'])} messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "complete-example",
      "metadata": {},
      "source": [
        "## Complete Example\n",
        "\n",
        "Putting it all together: A production chatbot with:\n",
        "- Persistence (checkpointer)\n",
        "- Cross-thread memory (store)\n",
        "- Error handling\n",
        "- Metadata tracking\n",
        "- Proper state management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "final-example",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete  bot ready\n"
          ]
        }
      ],
      "source": [
        "# Complete production-ready chatbot\n",
        "\n",
        "class CompleteBotState(MessagesState):\n",
        "    user_id: str\n",
        "    session_id: str\n",
        "    error_count: int\n",
        "\n",
        "def complete_chatbot(state: CompleteBotState, config: RunnableConfig, *, store: BaseStore) -> dict:\n",
        "    \"\"\"Complete chatbot with all features\"\"\"\n",
        "    try:\n",
        "        user_id = state['user_id']\n",
        "        messages = state['messages']\n",
        "        \n",
        "        # Load user preferences from store\n",
        "        namespace = (user_id, \"preferences\")\n",
        "        memories = store.search(namespace)\n",
        "        \n",
        "        # Build context\n",
        "        memory_text = \"\\n\".join([m.value.get('fact', '') for m in memories])\n",
        "        system_msg = f\"You are a helpful assistant.\\n\\nUser info:\\n{memory_text}\" if memory_text else \"You are a helpful assistant.\"\n",
        "        \n",
        "        full_messages = [SystemMessage(content=system_msg)] + messages\n",
        "        \n",
        "        # Generate response\n",
        "        response = llm.invoke(full_messages)\n",
        "        \n",
        "        # Extract and store new information if mentioned\n",
        "        last_msg = messages[-1].content.lower()\n",
        "        if any(keyword in last_msg for keyword in [\"my favorite\", \"i like\", \"i prefer\"]):\n",
        "            store.put(namespace, str(uuid.uuid4()), {\"fact\": messages[-1].content})\n",
        "        \n",
        "        return {\n",
        "            \"messages\": [response],\n",
        "            \"error_count\": 0\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_count = state.get('error_count', 0) + 1\n",
        "        return {\n",
        "            \"messages\": [AIMessage(content=\"I encountered an error. Please try again.\")],\n",
        "            \"error_count\": error_count\n",
        "        }\n",
        "\n",
        "# Build complete bot\n",
        "complete_checkpointer = MemorySaver()\n",
        "complete_store = InMemoryStore()\n",
        "\n",
        "workflow = StateGraph(CompleteBotState)\n",
        "workflow.add_node(\"chatbot\", complete_chatbot)\n",
        "workflow.add_edge(START, \"chatbot\")\n",
        "workflow.add_edge(\"chatbot\", END)\n",
        "\n",
        "complete_bot = workflow.compile(\n",
        "    checkpointer=complete_checkpointer,\n",
        "    store=complete_store\n",
        ")\n",
        "\n",
        "print(\"Complete  bot ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "test-complete",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== First Interaction ===\n",
            "AI: That's awesome! Python is a fantastic languageâ€”readable, versatile, and backed by a huge community. What do you enjoy most about it? Do you have a favorite library or project youâ€™ve been working on?\n",
            "\n",
            "=== New Thread (Should Remember) ===\n",
            "AI: You mentioned that your favorite programming language is **Python**.\n"
          ]
        }
      ],
      "source": [
        "# Test complete bot\n",
        "\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        \"thread_id\": \"complete_test_1\",\n",
        "        \"user_id\": \"alice\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# First interaction - share preference\n",
        "result1 = complete_bot.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"My favorite programming language is Python\")],\n",
        "        \"user_id\": \"alice\",\n",
        "        \"session_id\": \"test_1\",\n",
        "        \"error_count\": 0\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"=== First Interaction ===\")\n",
        "print(f\"AI: {result1['messages'][-1].content}\")\n",
        "\n",
        "# New thread, same user - should remember\n",
        "new_config = {\n",
        "    \"configurable\": {\n",
        "        \"thread_id\": \"complete_test_2\",\n",
        "        \"user_id\": \"alice\"  # Same user\n",
        "    }\n",
        "}\n",
        "\n",
        "result2 = complete_bot.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"What programming language do I prefer?\")],\n",
        "        \"user_id\": \"alice\",\n",
        "        \"session_id\": \"test_2\",\n",
        "        \"error_count\": 0\n",
        "    },\n",
        "    config=new_config\n",
        ")\n",
        "\n",
        "print(\"\\n=== New Thread (Should Remember) ===\")\n",
        "print(f\"AI: {result2['messages'][-1].content}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
