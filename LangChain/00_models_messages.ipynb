{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50c9efc",
   "metadata": {},
   "source": [
    "## Models and Messages \n",
    "\n",
    "This notebook shows **how to use LangChain models directly** (no agents) and how **message objects** work.\n",
    "\n",
    "We’ll use:\n",
    "\n",
    "- **Groq** chat models via `ChatGroq` (fast, OpenAI-compatible API).\n",
    "- **HuggingFace** embeddings via `HuggingFaceEmbeddings`.\n",
    "\n",
    "Topics covered:\n",
    "\n",
    "- **Models**: Chat models, LLMs, and embedding models\n",
    "- **Messages**: The core I/O format for conversational AI\n",
    "- **Runnables**: LangChain's composable abstraction for building pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f094cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello, LangChain!\" echoed through the digital realm, as a chorus of code and conversation came together in perfect harmony, marking the beginning of a beautiful friendship between humans and AI.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Say 'Hello, LangChain!' in a creative way.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d2986",
   "metadata": {},
   "source": [
    "### LangChain model interfaces: quick overview\n",
    "\n",
    "- **Chat models**: take a sequence of messages and return a message.   \n",
    "- **Embeddings models**: turn text into vectors for similarity / search.   \n",
    "\n",
    "Key methods:\n",
    "\n",
    "- `.invoke(input)` – single call\n",
    "- `.batch(list_of_inputs)` – run many calls in parallel\n",
    "- `.stream(input)` – (used later) stream tokens/chunks as they are generated\n",
    "\n",
    "### Why LangChain Wraps Models?\n",
    "- **Portability**: Switch between OpenAI, Groq, Anthropic with minimal code changes\n",
    "- **Unified API**: Same interface across all providers\n",
    "- **Enhanced Features**: Built-in retry logic, caching, tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324074d",
   "metadata": {},
   "source": [
    "### Messages: The Core I/O Format\n",
    "\n",
    "LangChain uses **message objects** instead of raw strings to represent conversational interactions.\n",
    "\n",
    "### Why Messages?\n",
    "- **Structure**: Clear separation of roles (system, user, assistant)\n",
    "- **Metadata**: Attach additional info (timestamps, sources, tool calls)\n",
    "- **Type Safety**: Easier to validate and debug\n",
    "\n",
    "### Common Message Types\n",
    "\n",
    "| Type | Role | Purpose |\n",
    "|------|------|---------|\n",
    "| `SystemMessage` | system | Instructions/context for the model |\n",
    "| `HumanMessage` | user | User input/queries |\n",
    "| `AIMessage` | assistant | Model-generated responses |\n",
    "| `ToolMessage` | tool | Results from tool/function calls (covered later) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f8de161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Response content: **List Comprehension Definition**\n",
      "\n",
      "A list comprehension is a compact way to create lists in Python. It consists of brackets containing the expression, which is executed for each element, along with the `for` loop to loop over the elements.\n",
      "\n",
      "**Basic Syntax**\n",
      "\n",
      "The basic syntax of a list comprehension is as follows:\n",
      "```python\n",
      "new_list = [expression for element in iterable]\n",
      "```\n",
      "Here:\n",
      "- `expression` is the operation you want to perform on each element.\n",
      "- `element` is the temporary variable used to represent each element in the `iterable`.\n",
      "- `iterable` is the list, tuple, or other iterable you want to process.\n",
      "\n",
      "**Example**\n",
      "\n",
      "Here's an example of using a list comprehension to square each number in a list:\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "squared_numbers = [x**2 for x in numbers]\n",
      "print(squared_numbers)  # Output: [1, 4, 9, 16, 25]\n",
      "```\n",
      "**Conditional List Comprehension**\n",
      "\n",
      "You can also add a condition to filter the elements:\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "even_numbers = [x for x in numbers if x % 2 == 0]\n",
      "print(even_numbers)  # Output: [2, 4]\n",
      "```\n",
      "**Nested List Comprehension**\n",
      "\n",
      "List comprehensions can be nested to process multi-dimensional data:\n",
      "```python\n",
      "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
      "squared_matrix = [[x**2 for x in row] for row in matrix]\n",
      "print(squared_matrix)  \n",
      "# Output: [[1, 4, 9], [16, 25, 36], [49, 64, 81]]\n",
      "```\n",
      "List comprehensions provide a concise and efficient way to perform common data transformations and filtering operations in Python.\n",
      "\n",
      "Full response object:\n",
      "content=\"**List Comprehension Definition**\\n\\nA list comprehension is a compact way to create lists in Python. It consists of brackets containing the expression, which is executed for each element, along with the `for` loop to loop over the elements.\\n\\n**Basic Syntax**\\n\\nThe basic syntax of a list comprehension is as follows:\\n```python\\nnew_list = [expression for element in iterable]\\n```\\nHere:\\n- `expression` is the operation you want to perform on each element.\\n- `element` is the temporary variable used to represent each element in the `iterable`.\\n- `iterable` is the list, tuple, or other iterable you want to process.\\n\\n**Example**\\n\\nHere's an example of using a list comprehension to square each number in a list:\\n```python\\nnumbers = [1, 2, 3, 4, 5]\\nsquared_numbers = [x**2 for x in numbers]\\nprint(squared_numbers)  # Output: [1, 4, 9, 16, 25]\\n```\\n**Conditional List Comprehension**\\n\\nYou can also add a condition to filter the elements:\\n```python\\nnumbers = [1, 2, 3, 4, 5]\\neven_numbers = [x for x in numbers if x % 2 == 0]\\nprint(even_numbers)  # Output: [2, 4]\\n```\\n**Nested List Comprehension**\\n\\nList comprehensions can be nested to process multi-dimensional data:\\n```python\\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\nsquared_matrix = [[x**2 for x in row] for row in matrix]\\nprint(squared_matrix)  \\n# Output: [[1, 4, 9], [16, 25, 36], [49, 64, 81]]\\n```\\nList comprehensions provide a concise and efficient way to perform common data transformations and filtering operations in Python.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 413, 'prompt_tokens': 52, 'total_tokens': 465, 'completion_time': 0.883175434, 'completion_tokens_details': None, 'prompt_time': 0.002439501, 'prompt_tokens_details': None, 'queue_time': 0.053108039, 'total_time': 0.885614935}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_68f543a7cc', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--fb33d83e-747d-4d8b-809d-9658ed2c5379-0' usage_metadata={'input_tokens': 52, 'output_tokens': 413, 'total_tokens': 465}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant specializing in Python programming.\"),\n",
    "    HumanMessage(content=\"What is a list comprehension?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"Response type:\", type(response))\n",
    "print(\"Response content:\", response.content)\n",
    "print(\"\\nFull response object:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8737e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1 (AI): Transformers are a type of neural network architecture that use self-attention mechanisms to weigh and combine input elements, allowing for highly effective processing of sequential data such as text and speech.\n",
      "\n",
      "Turn 2 (AI): The attention mechanism is a technique that enables neural networks to focus on specific parts of the input data by assigning weighted importance to each element, allowing the model to selectively concentrate on relevant information.\n",
      "\n",
      "==================================================\n",
      "Full Conversation History:\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a concise technical explainer.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain transformers in one sentence.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Transformers are a type of neural network architecture that use self-attention mechanisms to weigh and combine input elements, allowing for highly effective processing of sequential data such as text and speech.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Now explain attention mechanism in the same way.\n"
     ]
    }
   ],
   "source": [
    "# Simulate a multi-turn conversation\n",
    "conversation_history = [\n",
    "    SystemMessage(content=\"You are a concise technical explainer.\"),\n",
    "    HumanMessage(content=\"Explain transformers in one sentence.\"),\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response1 = llm.invoke(conversation_history)\n",
    "print(\"Turn 1 (AI):\", response1.content)\n",
    "\n",
    "conversation_history.append(AIMessage(content=response1.content))\n",
    "\n",
    "# Second turn - ask follow-up\n",
    "conversation_history.append(\n",
    "    HumanMessage(content=\"Now explain attention mechanism in the same way.\")\n",
    ")\n",
    "\n",
    "response2 = llm.invoke(conversation_history)\n",
    "print(\"\\nTurn 2 (AI):\", response2.content)\n",
    "\n",
    "# View full conversation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Full Conversation History:\")\n",
    "for msg in conversation_history:\n",
    "   msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a37989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Bonjour, monde !\n",
      "\n",
      "Response Metadata:\n",
      "  Model: llama-3.3-70b-versatile\n",
      "  Tokens used: {'completion_tokens': 5, 'prompt_tokens': 44, 'total_tokens': 49, 'completion_time': 0.008872069, 'completion_tokens_details': None, 'prompt_time': 0.001960832, 'prompt_tokens_details': None, 'queue_time': 0.053055007, 'total_time': 0.010832901}\n",
      "  Finish reason: stop\n"
     ]
    }
   ],
   "source": [
    "# Messages can carry metadata\n",
    "message_with_metadata = HumanMessage(\n",
    "    content=\"Translate this to French: Hello, world!\",\n",
    "    additional_kwargs={\"user_id\": \"12345\", \"session\": \"abc\"}\n",
    ")\n",
    "\n",
    "response = llm.invoke([message_with_metadata])\n",
    "\n",
    "# Inspect response metadata\n",
    "print(\"Content:\", response.content)\n",
    "print(\"\\nResponse Metadata:\")\n",
    "print(f\"  Model: {response.response_metadata.get('model_name')}\")\n",
    "print(f\"  Tokens used: {response.response_metadata.get('token_usage')}\")\n",
    "print(f\"  Finish reason: {response.response_metadata.get('finish_reason')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c10e1a",
   "metadata": {},
   "source": [
    "## Working with Chat Models\n",
    "\n",
    "Chat models are the primary interface for modern LLMs in LangChain.\n",
    "\n",
    "### Initialization Parameters\n",
    "```python\n",
    "ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",  # Model identifier\n",
    "    temperature=0.7,                   # Randomness (0=deterministic, 1=creative)\n",
    "    max_tokens=500,                    # Max response length\n",
    "    timeout=30,                        # Request timeout\n",
    "    max_retries=2,                     # Retry failed requests\n",
    ")\n",
    "```\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "**Single-turn Q&A:**\n",
    "```python\n",
    "llm.invoke(\"Your question here\")\n",
    "```\n",
    "\n",
    "**Few-shot learning:**\n",
    "```python\n",
    "llm.invoke([\n",
    "    SystemMessage(content=\"Classify sentiment as positive/negative/neutral.\"),\n",
    "    HumanMessage(content=\"I love this product! → Positive\"),\n",
    "    HumanMessage(content=\"It's okay. → Neutral\"),\n",
    "    HumanMessage(content=\"This is terrible. → ?\")\n",
    "])\n",
    "```\n",
    "\n",
    "**Role-based system prompts:**\n",
    "```python\n",
    "llm.invoke([\n",
    "    SystemMessage(content=\"You are a Shakespearean poet.\"),\n",
    "    HumanMessage(content=\"Describe programming in verse.\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb93515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Machine Learning Overview**\n",
      "==========================\n",
      "\n",
      "Machine learning (ML) is a subset of artificial intelligence (AI) that involves the use of algorithms and statistical models to enable machines to perform tasks without being explicitly programmed. It allows systems to learn from data, identify patterns, and make predictions or decisions with minimal human intervention.\n",
      "\n",
      "**Key Characteristics:**\n",
      "\n",
      "1. **Data-driven**: ML relies on large datasets to train and improve models.\n",
      "2. **Automated learning**: ML algorithms can learn from data without being explicitly programmed.\n",
      "3. **Pattern recognition**: ML models can identify complex patterns in data.\n",
      "4. **Prediction and decision-making**: ML models can make predictions or decisions based on learned patterns.\n",
      "\n",
      "**Types of Machine Learning:**\n",
      "\n",
      "1. **Supervised Learning**: The model is trained on labeled data to learn the relationship between input and output.\n",
      "2. **Unsupervised Learning**: The model is trained on unlabeled data to identify patterns or structure.\n",
      "3. **Reinforcement Learning**: The model learns through trial and error by interacting with an environment.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "1. **Image and speech recognition**\n",
      "2. **Natural language processing**\n",
      "3. **Predictive maintenance**\n",
      "4. **Recommendation systems**\n",
      "5. **Automated decision-making**\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "1. **Improved accuracy**: ML models can learn from large datasets and improve their performance over time.\n",
      "2. **Increased efficiency**: ML can automate tasks and reduce the need for human intervention.\n",
      "3. **Enhanced decision-making**: ML can provide insights and predictions to support informed decision-making.\n",
      "\n",
      "**Common Machine Learning Algorithms:**\n",
      "\n",
      "1. **Linear Regression**\n",
      "2. **Decision Trees**\n",
      "3. **Random Forest**\n",
      "4. **Support Vector Machines (SVMs)**\n",
      "5. **Neural Networks**\n",
      "\n",
      "**Real-World Examples:**\n",
      "\n",
      "1. **Virtual assistants** (e.g., Siri, Alexa)\n",
      "2. **Image recognition** (e.g., Google Photos)\n",
      "3. **Recommendation systems** (e.g., Netflix, Amazon)\n",
      "4. **Self-driving cars** (e.g., Waymo)\n",
      "5. **Medical diagnosis** (e.g., disease detection using ML algorithms)\n",
      "1. None\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Deep Learning Overview**\n",
      "==========================\n",
      "\n",
      "Deep learning is a subset of machine learning that involves the use of artificial neural networks to analyze and interpret data. These neural networks are designed to mimic the structure and function of the human brain, with multiple layers of interconnected nodes (neurons) that process and transform inputs into meaningful outputs.\n",
      "\n",
      "**Key Characteristics**\n",
      "------------------------\n",
      "\n",
      "* **Multiple Layers**: Deep learning models typically consist of multiple layers, including input layers, hidden layers, and output layers. Each layer processes and transforms the input data, allowing the model to learn complex patterns and relationships.\n",
      "* **Neural Networks**: Deep learning models are based on artificial neural networks, which are composed of interconnected nodes (neurons) that process and transmit information.\n",
      "* **Non-Linear Transformations**: Deep learning models use non-linear transformations to introduce complexity and non-linearity into the model, allowing it to learn and represent complex data distributions.\n",
      "* **Large Amounts of Data**: Deep learning models require large amounts of data to train and validate, as they need to learn from examples and generalize to new, unseen data.\n",
      "\n",
      "**Types of Deep Learning Models**\n",
      "-------------------------------\n",
      "\n",
      "* **Convolutional Neural Networks (CNNs)**: Used for image and video processing, CNNs are designed to extract features from data using convolutional and pooling layers.\n",
      "* **Recurrent Neural Networks (RNNs)**: Used for sequential data, such as text, speech, or time series data, RNNs are designed to capture temporal relationships and patterns in data.\n",
      "* **Autoencoders**: Used for dimensionality reduction and generative modeling, autoencoders are designed to learn compact representations of data and generate new samples.\n",
      "* **Generative Adversarial Networks (GANs)**: Used for generative modeling and data augmentation, GANs are designed to generate new samples that are indistinguishable from real data.\n",
      "\n",
      "**Applications of Deep Learning**\n",
      "--------------------------------\n",
      "\n",
      "* **Computer Vision**: Image classification, object detection, segmentation, and generation.\n",
      "* **Natural Language Processing**: Text classification, sentiment analysis, language modeling, and machine translation.\n",
      "* **Speech Recognition**: Speech-to-text, voice recognition, and speech synthesis.\n",
      "* **Robotics and Control**: Control systems, robotics, and autonomous vehicles.\n",
      "\n",
      "**Advantages and Challenges**\n",
      "---------------------------\n",
      "\n",
      "* **Advantages**: Deep learning models can learn complex patterns and relationships in data, achieve state-of-the-art performance in many applications, and can be used for a wide range of tasks.\n",
      "* **Challenges**: Deep learning models require large amounts of data, can be computationally expensive to train, and can be sensitive to hyperparameters and model architecture.\n",
      "2. None\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Reinforcement Learning (RL) Overview**\n",
      "=====================================\n",
      "\n",
      "Reinforcement learning is a subfield of machine learning that involves training an agent to make decisions in an environment to maximize a reward or achieve a goal. The agent learns through trial and error by interacting with the environment, receiving feedback in the form of rewards or penalties, and adjusting its behavior accordingly.\n",
      "\n",
      "**Key Components of RL**\n",
      "-----------------------\n",
      "\n",
      "1. **Agent**: The decision-making entity that interacts with the environment.\n",
      "2. **Environment**: The external world that the agent interacts with, which can be fully or partially observable.\n",
      "3. **Actions**: The decisions made by the agent in the environment.\n",
      "4. **Rewards**: The feedback received by the agent for its actions, which can be positive (reward) or negative (penalty).\n",
      "5. **Policy**: The strategy used by the agent to select actions in the environment.\n",
      "6. **Value Function**: A function that estimates the expected return or value of taking an action in a given state.\n",
      "\n",
      "**RL Process**\n",
      "-------------\n",
      "\n",
      "1. The agent observes the current state of the environment.\n",
      "2. The agent selects an action based on its policy.\n",
      "3. The agent takes the action and receives a reward or penalty.\n",
      "4. The agent updates its policy and value function based on the reward and the new state of the environment.\n",
      "5. Steps 1-4 are repeated until the agent achieves its goal or a termination condition is met.\n",
      "\n",
      "**Types of RL**\n",
      "----------------\n",
      "\n",
      "1. **Episodic**: The agent learns from a sequence of episodes, where each episode consists of a finite sequence of actions and rewards.\n",
      "2. **Continuing**: The agent learns from a continuous stream of experiences, with no clear episode boundaries.\n",
      "3. **Model-based**: The agent has a model of the environment and uses it to make decisions.\n",
      "4. **Model-free**: The agent learns without a model of the environment, relying on trial and error.\n",
      "\n",
      "**RL Applications**\n",
      "------------------\n",
      "\n",
      "1. **Game playing**: RL has been used to achieve superhuman performance in games like Go, Poker, and Video Games.\n",
      "2. **Robotics**: RL is used to control robots and learn complex tasks like grasping and manipulation.\n",
      "3. **Autonomous vehicles**: RL is used to learn control policies for self-driving cars.\n",
      "4. **Recommendation systems**: RL is used to personalize recommendations and optimize user engagement.\n",
      "\n",
      "**Challenges in RL**\n",
      "-------------------\n",
      "\n",
      "1. **Exploration-Exploitation Trade-off**: Balancing the need to explore new actions and states with the need to exploit known good actions.\n",
      "2. **Curse of Dimensionality**: Dealing with high-dimensional state and action spaces.\n",
      "3. **Off-policy learning**: Learning from experiences gathered without following the same policy as the one being learned.\n",
      "\n",
      "Overall, reinforcement learning is a powerful framework for learning complex decision-making tasks, with a wide range of applications in robotics, game playing, and other areas.\n",
      "3. None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process multiple queries in one call (more efficient than loop)\n",
    "queries = [\n",
    "    [HumanMessage(content=\"What is machine learning?\")],\n",
    "    [HumanMessage(content=\"What is deep learning?\")],\n",
    "    [HumanMessage(content=\"What is reinforcement learning?\")]\n",
    "]\n",
    "\n",
    "# Batch invoke\n",
    "responses = llm.batch(queries)\n",
    "\n",
    "for i, response in enumerate(responses, 1):\n",
    "    print(f\"{i}. {response.pretty_print()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00488a18",
   "metadata": {},
   "source": [
    "### Embedding Models\n",
    "Embeddings convert text into dense numerical vectors that capture semantic meaning.\n",
    "\n",
    "### Why Embeddings?\n",
    "- **Semantic Search**: Find similar documents based on meaning, not keywords\n",
    "- **RAG**: Retrieve relevant context for language models\n",
    "- **Classification**: Use vectors as features for ML models\n",
    "- **Clustering**: Group similar texts together\n",
    "\n",
    "### How They Work\n",
    "```\n",
    "\"Paris is the capital of France\" \n",
    "    ↓ embedding model ↓\n",
    "[0.234, -0.123, 0.456, ..., 0.789]  # 768-dimensional vector\n",
    "```\n",
    "\n",
    "Similar texts produce similar vectors (high cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings model loaded\n",
      "Model: sentence-transformers/all-mpnet-base-v2\n",
      "Text: Transformers revolutionized natural language processing.\n",
      "Embedding dimension: 768\n",
      "First 10 values: [0.06474940478801727, 0.04542214050889015, -0.023475218564271927, 0.02088870294392109, -0.020095515996217728, -0.005383903626352549, 0.026434894651174545, 0.007855139672756195, -0.07080433517694473, -0.08333080261945724]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},  # Use \"cuda\" if you have GPU\n",
    "    encode_kwargs={\"normalize_embeddings\": True}  # Normalize to unit length\n",
    ")\n",
    "\n",
    "print(\"Embeddings model loaded\")\n",
    "print(f\"Model: {embeddings_model.model_name}\")\n",
    "\n",
    "# Embed single text\n",
    "text = \"Transformers revolutionized natural language processing.\"\n",
    "embedding = embeddings_model.embed_query(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278874c",
   "metadata": {},
   "source": [
    "### Runnable Pipelines (LangChain Core Pattern)\n",
    "\n",
    "### What is a Runnable?\n",
    "A `Runnable` is LangChain's universal abstraction for composable components.\n",
    "\n",
    "**Key idea:** Everything implements the same interface:\n",
    "- `.invoke(input)` - Synchronous execution\n",
    "- `.batch(inputs)` - Process multiple inputs\n",
    "- `.stream(input)` - Streaming output\n",
    "- `.ainvoke()` / `.abatch()` / `.astream()` - Async versions\n",
    "\n",
    "### Why Runnables Matter?\n",
    "- **Composability**: Chain components using `|` operator - Lanchain expression language\n",
    "- **Consistency**: Same API for models, prompts, retrievers, agents\n",
    "- **Debugging**: Built-in tracing and logging\n",
    "- **Production**: Easy to swap implementations\n",
    "\n",
    "### Basic Pattern\n",
    "```python\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke({\"topic\": \"AI\"})\n",
    "```\n",
    "\n",
    "### Components that are Runnables:\n",
    "- ✅ Models (chat, LLM, embeddings)\n",
    "- ✅ Prompts (prompt templates)\n",
    "- ✅ Output parsers (extract structured data)\n",
    "- ✅ Retrievers (vector search)\n",
    "- ✅ Tools (functions)\n",
    "- ✅ Agents (orchestrators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b839e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent is an optimization algorithm used in machine learning to minimize the loss function of a model by iteratively adjusting its parameters in the direction of the negative gradient, which is the direction of the steepest descent. By repeatedly calculating the gradient of the loss function and updating the parameters, the model converges to a local or global minimum, resulting in the best possible fit to the training data.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that explains {topic} concepts concisely.\"),\n",
    "    (\"human\", \"Explain {concept} in 2 sentences.\")\n",
    "])\n",
    "\n",
    "# Chain: prompt → model\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke chain with inputs\n",
    "# Now result is just a string (not AIMessage object) due to the output parser\n",
    "result = chain.invoke({\n",
    "    \"topic\": \"machine learning\",\n",
    "    \"concept\": \"gradient descent\"\n",
    "})\n",
    "\n",
    "# print(result.content) # If output parser is not used the result is an AIMessage object\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2065ddeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Indexing in databases is a technique that improves query performance by creating a data structure that facilitates quick lookup and retrieval of specific data, similar to an index in a book. By indexing specific columns, the database can rapidly locate and access the required data, reducing the time it takes to execute queries and enhancing overall database efficiency.\n",
      "\n",
      "2. The TCP/IP (Transmission Control Protocol/Internet Protocol) is a set of communication protocols that enables devices to communicate over the internet, with TCP ensuring reliable data transfer and IP providing device addressing and routing. This protocol suite allows devices to establish connections, exchange data, and manage communication flows, making it the foundation of the modern internet and enabling global networking and communication.\n",
      "\n",
      "3. Binary search is an efficient algorithm that finds an item in a sorted list by repeatedly dividing the list in half and searching for the item in one of the two halves. This process continues until the item is found or the list is exhausted, resulting in a time complexity of O(log n), making it much faster than linear search for large datasets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process multiple inputs through the chain\n",
    "inputs = [\n",
    "    {\"topic\": \"databases\", \"concept\": \"indexing\"},\n",
    "    {\"topic\": \"networking\", \"concept\": \"TCP/IP\"},\n",
    "    {\"topic\": \"algorithms\", \"concept\": \"binary search\"}\n",
    "]\n",
    "\n",
    "results = chain.batch(inputs)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef299333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECURSION IS A PROGRAMMING TECHNIQUE WHERE A FUNCTION CALLS ITSELF REPEATEDLY UNTIL IT REACHES A BASE CASE THAT STOPS THE RECURSION, ALLOWING THE FUNCTION TO SOLVE PROBLEMS BY BREAKING THEM DOWN INTO SMALLER INSTANCES OF THE SAME PROBLEM. THIS PROCESS UNWINDS AS EACH RECURSIVE CALL RETURNS, COMBINING THE RESULTS TO PRODUCE THE FINAL SOLUTION, MAKING RECURSION A POWERFUL TOOL FOR SOLVING COMPLEX PROBLEMS WITH A SIMPLE AND ELEGANT CODE STRUCTURE.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "# Decorate any function to make it a Runnable\n",
    "# The @chain decorator converts a normal Python function into a Runnable\n",
    "@chain\n",
    "def uppercase_output(ai_message):\n",
    "    \"\"\"Custom processing: convert response to uppercase.\"\"\"\n",
    "    return ai_message.content.upper()\n",
    "\n",
    "# Chain: prompt → model → custom function\n",
    "chain = prompt | llm | uppercase_output\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"topic\": \"programming\",\n",
    "    \"concept\": \"recursion\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c85e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the chain components\n",
    "simple_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Chain structure:\")\n",
    "print(simple_chain)\n",
    "\n",
    "# Get input/output schema (useful for debugging)\n",
    "print(\"\\nInput schema:\")\n",
    "print(simple_chain.input_schema.schema())\n",
    "\n",
    "print(\"\\nOutput schema:\")\n",
    "print(simple_chain.output_schema.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0c0e2",
   "metadata": {},
   "source": [
    "### 9. Extra: Parallel pipelines with RunnableMap / RunnableParallel\n",
    "\n",
    "LangChain’s `RunnableParallel` (alias `RunnableMap`) lets you run multiple independent runnables on the same input — returning a dictionary of results. :contentReference[oaicite:5]{index=5}  \n",
    "Below are examples:\n",
    "\n",
    "- Wrapping a Python function with `RunnableLambda`.  \n",
    "- Running embedding generation + simple text processing in parallel.\n",
    "\n",
    "This pattern is powerful when you want to derive **multiple outputs** from the same input (e.g. summary + metadata + embeddings), or do **parallel tasks** for efficiency.  \n",
    "\n",
    "You can integrate this with prompt → model → post-processing chains, agents, or RAG pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a66a03c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Why did the bear go to the doctor?\n",
      "\n",
      "Because it had a grizzly cough!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "In the forest, a bear does roam, \n",
      "Its gentle strength, a wondrous home.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "\n",
    "\n",
    "joke_chain = (\n",
    "    ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | llm\n",
    ")\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n",
    "    | llm\n",
    ")\n",
    "\n",
    "runnable = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "runnable.invoke({\"topic\": \"bear\"})\n",
    "for msg in runnable.invoke({\"topic\": \"bear\"}).values():\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26a679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
