{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "### Hybrid Search with Reranking\n",
    "\n",
    "**Core Concept**: Combine the speed of keyword matching (BM25) with the semantic understanding of dense embeddings, then use reranking to surface the most relevant results.\n",
    "\n",
    "**The Pipeline**:\n",
    "1. Stage 1 - Hybrid Retrieval: Cast a wide net using both BM25 and vector similarity\n",
    "2. Stage 2 - Reranking: Apply a cross-encoder or LLM to deeply score query-document pairs\n",
    "\n",
    "**Why This Matters**:\n",
    "- BM25 catches exact term matches (good for technical queries)\n",
    "- Dense retrieval catches semantic similarity (good for conceptual queries)\n",
    "- Reranking ensures the *best* results bubble to the top, not just *good* results\n",
    "\n",
    "**What You'll Learn**:\n",
    "- Building hybrid retrievers with ensemble methods\n",
    "- Cross-encoder reranking for quality improvements\n",
    "- LLM-based reranking for complex reasoning\n",
    "- Production patterns and cost considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_prep",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Load and chunk documents. Chunking strategy matters for retrieval quality:\n",
    "- Smaller chunks (200-300 tokens): Better precision, more granular matching\n",
    "- Larger chunks (500-1000 tokens): More context, but can dilute relevance signals\n",
    "\n",
    "For hybrid search, moderate chunk sizes with overlap work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 articles\n",
      "First doc preview:\n",
      " In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is convert ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashsarode/Downloads/Personal Projects/Python/LangGraph-personal/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/yashsarode/Downloads/Personal Projects/Python/LangGraph-personal/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "loader = WikipediaLoader(query=\"Transformer (deep learning)\", load_max_docs=8)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} articles\")\n",
    "print(\"First doc preview:\\n\", docs[0].page_content[:220], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retriever_setup",
   "metadata": {},
   "source": [
    "### Building the Hybrid Retriever\n",
    "\n",
    "**BM25 Retriever**: Statistical, keyword-based. Fast but misses synonyms.\n",
    "- Scores based on term frequency and inverse document frequency\n",
    "- Good for exact matches like \"LangChain agents\" or \"FAISS indexing\"\n",
    "\n",
    "**Dense Retriever**: Embedding-based, semantic. Slower but understands meaning.\n",
    "- Scores based on cosine similarity in vector space\n",
    "- Good for \"how to build stateful assistants\" → matches \"memory in LangChain\"\n",
    "\n",
    "**EnsembleRetriever**: Combines both using weighted scoring (Reciprocal Rank Fusion by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "build_retrievers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid retriever created\n",
      "Strategy: Reciprocal Rank Fusion (RRF)\n"
     ]
    }
   ],
   "source": [
    "# BM25 Retriever - keyword-based\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 10  # Retrieve top 10 from BM25\n",
    "\n",
    "# Dense Retriever - embedding-based\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Ensemble combines both\n",
    "# Weights: [BM25, Dense] - adjust based on your use case\n",
    "# 0.5/0.5 = balanced, 0.7/0.3 = favor keywords, 0.3/0.7 = favor semantics\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, dense_retriever],\n",
    "    weights=[0.5, 0.5]  # Equal weight for demonstration\n",
    ")\n",
    "\n",
    "print(\"Hybrid retriever created\")\n",
    "print(\"Strategy: Reciprocal Rank Fusion (RRF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rrf_explanation",
   "metadata": {},
   "source": [
    "### Understanding Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "RRF is the default scoring method in EnsembleRetriever. It works by:\n",
    "\n",
    "```\n",
    "For each document:\n",
    "    RRF_score = Σ (weight_i / (k + rank_i))\n",
    "    where k=60 (default constant)\n",
    "```\n",
    "\n",
    "**Example**:\n",
    "- Doc A: BM25 rank=1, Dense rank=5\n",
    "  - RRF = 0.5/(60+1) + 0.5/(60+5) = 0.0082 + 0.0077 = 0.0159\n",
    "- Doc B: BM25 rank=10, Dense rank=2\n",
    "  - RRF = 0.5/(60+10) + 0.5/(60+2) = 0.0071 + 0.0081 = 0.0152\n",
    "\n",
    "Doc A wins despite worse dense ranking because it's much better in BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "test_hybrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is transformer and why is better than LSTM??\n",
      "\n",
      "Retrieved 7 documents\n",
      "\n",
      "Result 1:\n",
      "In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted\n",
      "\n",
      "Result 2:\n",
      "A vision transformer (ViT) is a transformer designed for computer vision. A ViT decomposes an input image into a series of patches (rather than text i\n",
      "\n",
      "Result 3:\n",
      "In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that seq\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test hybrid retrieval\n",
    "query = \"What is transformer and why is better than LSTM??\"\n",
    "\n",
    "hybrid_results = hybrid_retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nRetrieved {len(hybrid_results)} documents\\n\")\n",
    "\n",
    "for i, doc in enumerate(hybrid_results[:3], 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(doc.page_content[:150])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reranking_intro",
   "metadata": {},
   "source": [
    "### Stage 2: Cross-Encoder Reranking\n",
    "\n",
    "**The Problem**: Hybrid retrieval gives us candidates, but they're not perfectly ordered. A document ranked #8 might actually be more relevant than #1.\n",
    "\n",
    "**The Solution**: Cross-encoders process [query, document] pairs jointly and output a relevance score. Unlike bi-encoders (used in dense retrieval), they see both inputs together, enabling deeper understanding.\n",
    "\n",
    "**Trade-off**: \n",
    "- Much slower than bi-encoders (can't pre-compute)\n",
    "- Much more accurate (full attention between query and doc)\n",
    "\n",
    "**Production Pattern**: Retrieve 50-100 candidates with hybrid search, rerank with cross-encoder to get top 5-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "setup_crossencoder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking pipeline ready\n",
      "Flow: Hybrid retrieval → Cross-encoder scoring → Top 5 results\n"
     ]
    }
   ],
   "source": [
    "# Cross-encoder model - trained specifically for reranking\n",
    "# ms-marco-MiniLM is fast and effective for most use cases\n",
    "cross_encoder = HuggingFaceCrossEncoder(\n",
    "    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ")\n",
    "\n",
    "# Reranker wrapper\n",
    "reranker = CrossEncoderReranker(\n",
    "    model=cross_encoder,\n",
    "    top_n=5  # Return only top 5 after reranking all candidates\n",
    ")\n",
    "\n",
    "# Compression retriever orchestrates: retrieve → rerank → return\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker,\n",
    "    base_retriever=hybrid_retriever\n",
    ")\n",
    "\n",
    "print(\"Reranking pipeline ready\")\n",
    "print(f\"Flow: Hybrid retrieval → Cross-encoder scoring → Top {reranker.top_n} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "test_reranking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BEFORE RERANKING (Hybrid Retrieval Only)\n",
      "======================================================================\n",
      "\n",
      "[1] In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechani...\n",
      "\n",
      "[2] A vision transformer (ViT) is a transformer designed for computer vision. A ViT decomposes an input image into a series ...\n",
      "\n",
      "[3] In machine learning, attention is a method that determines the importance of each component in a sequence relative to th...\n",
      "\n",
      "[4] Noam Shazeer (born 1975 or 1976) is an American computer scientist and entrepreneur known for his contributions to the f...\n",
      "\n",
      "[5] A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely used in generative AI ...\n",
      "\n",
      "======================================================================\n",
      "AFTER RERANKING (Cross-Encoder Refined)\n",
      "======================================================================\n",
      "\n",
      "[1] In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechani...\n",
      "\n",
      "[2] A vision transformer (ViT) is a transformer designed for computer vision. A ViT decomposes an input image into a series ...\n",
      "\n",
      "[3] A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely used in generative AI ...\n",
      "\n",
      "[4] Mamba is a deep learning architecture focused on sequence modeling. It was developed by researchers from Carnegie Mellon...\n",
      "\n",
      "[5] Noam Shazeer (born 1975 or 1976) is an American computer scientist and entrepreneur known for his contributions to the f...\n",
      "\n",
      "======================================================================\n",
      "Notice how reranking reordered results based on deeper relevance\n"
     ]
    }
   ],
   "source": [
    "# Compare: Before and After Reranking\n",
    "query = \"What is transformer and why is better than LSTM??\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BEFORE RERANKING (Hybrid Retrieval Only)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "hybrid_results = hybrid_retriever.invoke(query)\n",
    "for i, doc in enumerate(hybrid_results[:5], 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:120]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AFTER RERANKING (Cross-Encoder Refined)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "reranked_results = compression_retriever.invoke(query)\n",
    "for i, doc in enumerate(reranked_results, 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:120]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Notice how reranking reordered results based on deeper relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm_reranking_intro",
   "metadata": {},
   "source": [
    "### LLM-Based Reranking\n",
    "\n",
    "When cross-encoders aren't enough, use an LLM to rerank. This is powerful when you need:\n",
    "- Complex reasoning (\"Find docs that *contradict* the query\")\n",
    "- Domain-specific criteria (\"Prioritize recent medical studies over older ones\")\n",
    "- Explanations (\"Why is this doc ranked first?\")\n",
    "\n",
    "**Cost Warning**: LLMs are 10-100x more expensive than cross-encoders for reranking. Use sparingly.\n",
    "\n",
    "**Pattern**: Cross-encoder for initial filtering (100→20), then LLM for final refinement (20→5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "setup_llm_reranking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM reranking chain created\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM for reranking\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0  # Deterministic for consistent ranking\n",
    ")\n",
    "\n",
    "# Reranking prompt with clear instructions\n",
    "rerank_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a document relevance expert. Your task is to rank documents by relevance to a query.\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze each document's relevance to the query\n",
    "2. Consider semantic meaning, not just keyword matching\n",
    "3. Prioritize documents that directly answer the query\n",
    "4. Return a comma-separated list of document numbers in ranked order (most relevant first)\n",
    "\n",
    "Output format: 3,1,5,2,4 (just the numbers, no explanations)\n",
    "\"\"\")\n",
    "\n",
    "rerank_chain = rerank_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"LLM reranking chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "llm_rerank_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM reranking function ready\n"
     ]
    }
   ],
   "source": [
    "def llm_rerank(query: str, documents: list, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Use LLM to rerank documents.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        documents: List of Document objects\n",
    "        top_n: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of reranked documents\n",
    "    \"\"\"\n",
    "    # Format documents for the prompt\n",
    "    doc_texts = [\n",
    "        f\"{i+1}. {doc.page_content}\"\n",
    "        for i, doc in enumerate(documents)\n",
    "    ]\n",
    "    formatted_docs = \"\\n\\n\".join(doc_texts)\n",
    "    \n",
    "    # Get rankings from LLM\n",
    "    response = rerank_chain.invoke({\n",
    "        \"query\": query,\n",
    "        \"documents\": formatted_docs\n",
    "    })\n",
    "    \n",
    "    # Parse rankings (handle potential formatting issues)\n",
    "    try:\n",
    "        # Extract only digit sequences\n",
    "        indices = [\n",
    "            int(x.strip()) - 1  # Convert to 0-indexed\n",
    "            for x in response.split(\",\")\n",
    "            if x.strip().isdigit()\n",
    "        ]\n",
    "        \n",
    "        # Reorder documents based on rankings\n",
    "        reranked = [\n",
    "            documents[i]\n",
    "            for i in indices\n",
    "            if 0 <= i < len(documents)\n",
    "        ][:top_n]\n",
    "        \n",
    "        return reranked\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing rankings: {e}\")\n",
    "        print(f\"LLM response: {response}\")\n",
    "        return documents[:top_n]  # Fallback to original order\n",
    "\n",
    "print(\"LLM reranking function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "test_llm_reranking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LLM RERANKING\n",
      "======================================================================\n",
      "\n",
      "[1] In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted...\n",
      "\n",
      "[2] In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that seq...\n",
      "\n",
      "[3] A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely used in generative AI chatbots. GPTs are based on a ...\n",
      "\n",
      "[4] A vision transformer (ViT) is a transformer designed for computer vision. A ViT decomposes an input image into a series of patches (rather than text i...\n",
      "\n",
      "[5] Ashish Vaswani (born 1986) is an Indian computer scientist. He worked as a research scientist at Google Brain and Information Sciences Institute. \n",
      "Vas...\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test LLM reranking\n",
    "query = \"What is transformer and why is better than LSTM??\"\n",
    "\n",
    "# Get initial results from hybrid retrieval\n",
    "candidates = hybrid_retriever.invoke(query)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LLM RERANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "llm_reranked = llm_rerank(query, candidates, top_n=5)\n",
    "\n",
    "for i, doc in enumerate(llm_reranked, 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "### Three-Way Comparison\n",
    "\n",
    "Let's compare all three approaches side by side:\n",
    "1. Hybrid retrieval only (fast, decent quality)\n",
    "2. Hybrid + Cross-encoder (balanced speed/quality)\n",
    "3. Hybrid + LLM reranking (best quality, slowest/most expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "full_comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: Three Reranking Approaches\n",
      "================================================================================\n",
      "\n",
      "[1] HYBRID RETRIEVAL ONLY\n",
      "Time: 0.433s\n",
      "  1. In deep learning, the transformer is an artificial neural network architecture based on the multi-he...\n",
      "  2. A vision transformer (ViT) is a transformer designed for computer vision. A ViT decomposes an input ...\n",
      "  3. In machine learning, attention is a method that determines the importance of each component in a seq...\n",
      "  4. Noam Shazeer (born 1975 or 1976) is an American computer scientist and entrepreneur known for his co...\n",
      "  5. A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely us...\n",
      "\n",
      "[2] HYBRID + CROSS-ENCODER\n",
      "Time: 0.226s (0.5x slower)\n",
      "  1. In deep learning, the transformer is an artificial neural network architecture based on the multi-he...\n",
      "  2. A vision transformer (ViT) is a transformer designed for computer vision. A ViT decomposes an input ...\n",
      "  3. A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely us...\n",
      "  4. Mamba is a deep learning architecture focused on sequence modeling. It was developed by researchers ...\n",
      "  5. Noam Shazeer (born 1975 or 1976) is an American computer scientist and entrepreneur known for his co...\n",
      "\n",
      "[3] HYBRID + LLM RERANKING\n",
      "Time: 0.972s (2.2x slower)\n",
      "  1. In deep learning, the transformer is an artificial neural network architecture based on the multi-he...\n",
      "  2. In machine learning, attention is a method that determines the importance of each component in a seq...\n",
      "  3. A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely us...\n",
      "  4. A vision transformer (ViT) is a transformer designed for computer vision. A ViT decomposes an input ...\n",
      "  5. Mamba is a deep learning architecture focused on sequence modeling. It was developed by researchers ...\n",
      "\n",
      "================================================================================\n",
      "Key Takeaways:\n",
      "- Cross-encoder adds minimal latency with significant quality gain\n",
      "- LLM reranking is much slower but can handle complex reasoning\n",
      "- Choose based on your quality/speed/cost requirements\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query = \"What is transformer and why is better than LSTM??\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: Three Reranking Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Approach 1: Hybrid only\n",
    "start = time.time()\n",
    "hybrid_only = hybrid_retriever.invoke(query)[:5]\n",
    "time_hybrid = time.time() - start\n",
    "\n",
    "print(\"\\n[1] HYBRID RETRIEVAL ONLY\")\n",
    "print(f\"Time: {time_hybrid:.3f}s\")\n",
    "for i, doc in enumerate(hybrid_only, 1):\n",
    "    print(f\"  {i}. {doc.page_content[:100]}...\")\n",
    "\n",
    "# Approach 2: Hybrid + Cross-encoder\n",
    "start = time.time()\n",
    "cross_encoder_reranked = compression_retriever.invoke(query)\n",
    "time_cross = time.time() - start\n",
    "\n",
    "print(f\"\\n[2] HYBRID + CROSS-ENCODER\")\n",
    "print(f\"Time: {time_cross:.3f}s ({time_cross/time_hybrid:.1f}x slower)\")\n",
    "for i, doc in enumerate(cross_encoder_reranked, 1):\n",
    "    print(f\"  {i}. {doc.page_content[:100]}...\")\n",
    "\n",
    "# Approach 3: Hybrid + LLM\n",
    "start = time.time()\n",
    "candidates = hybrid_retriever.invoke(query)\n",
    "llm_reranked_results = llm_rerank(query, candidates, top_n=5)\n",
    "time_llm = time.time() - start\n",
    "\n",
    "print(f\"\\n[3] HYBRID + LLM RERANKING\")\n",
    "print(f\"Time: {time_llm:.3f}s ({time_llm/time_hybrid:.1f}x slower)\")\n",
    "for i, doc in enumerate(llm_reranked_results, 1):\n",
    "    print(f\"  {i}. {doc.page_content[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"- Cross-encoder adds minimal latency with significant quality gain\")\n",
    "print(\"- LLM reranking is much slower but can handle complex reasoning\")\n",
    "print(\"- Choose based on your quality/speed/cost requirements\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production_pattern",
   "metadata": {},
   "source": [
    "### Production Reranking Pattern\n",
    "\n",
    "For most production systems, use a staged approach:\n",
    "\n",
    "```\n",
    "Stage 1: Hybrid Retrieval (BM25 + Dense)\n",
    "  ↓ 200 candidates (fast, broad coverage)\n",
    "\n",
    "Stage 2: Cross-Encoder Reranking\n",
    "  ↓ 20 results (accurate, still efficient)\n",
    "\n",
    "Stage 3: LLM Reranking (optional)\n",
    "  ↓ 5 final results (deep reasoning, expensive)\n",
    "```\n",
    "\n",
    "**Why This Works**:\n",
    "- Stage 1 ensures recall (don't miss relevant docs)\n",
    "- Stage 2 improves precision efficiently\n",
    "- Stage 3 handles edge cases requiring complex logic\n",
    "\n",
    "**Cost Analysis**:\n",
    "- Hybrid retrieval: ~1ms, negligible cost\n",
    "- Cross-encoder (200 docs): ~50-100ms, negligible cost\n",
    "- LLM reranking (20 docs): ~1-3s, $0.001-0.01 per query\n",
    "\n",
    "Skip Stage 3 for most queries; use it only when:\n",
    "- User explicitly needs best possible results\n",
    "- Query is ambiguous or requires reasoning\n",
    "- You're building for low-volume, high-value use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "production_example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRODUCTION CONFIGURATION 1: Cross-encoder only\n",
      "Stages used: 2\n",
      "Timing: {'stage1_hybrid': '0.173s', 'stage2_cross_encoder': '0.337s', 'total': '0.337s'}\n",
      "\n",
      "PRODUCTION CONFIGURATION 2: Full pipeline with LLM\n",
      "Stages used: 3\n",
      "Timing: {'stage1_hybrid': '0.007s', 'stage2_cross_encoder': '0.094s', 'total': '0.633s'}\n",
      "\n",
      "Top result (LLM-reranked):\n",
      "In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less\n"
     ]
    }
   ],
   "source": [
    "def production_retrieve_and_rerank(\n",
    "    query: str,\n",
    "    use_llm_rerank: bool = False,\n",
    "    final_top_k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Production-grade retrieval with optional LLM reranking.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        use_llm_rerank: Whether to apply expensive LLM reranking\n",
    "        final_top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of top documents with metadata\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Stage 1: Hybrid retrieval\n",
    "    # Retrieve more candidates for better recall\n",
    "    hybrid_retriever_wide = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, dense_retriever],\n",
    "        weights=[0.5, 0.5]\n",
    "    )\n",
    "    \n",
    "    candidates = hybrid_retriever_wide.invoke(query)\n",
    "    stage1_time = time.time() - start_time\n",
    "    \n",
    "    # Stage 2: Cross-encoder reranking\n",
    "    intermediate_k = 10 if use_llm_rerank else final_top_k\n",
    "    \n",
    "    reranker_stage2 = CrossEncoderReranker(\n",
    "        model=cross_encoder,\n",
    "        top_n=intermediate_k\n",
    "    )\n",
    "    \n",
    "    compression_retriever_stage2 = ContextualCompressionRetriever(\n",
    "        base_compressor=reranker_stage2,\n",
    "        base_retriever=hybrid_retriever_wide\n",
    "    )\n",
    "    \n",
    "    stage2_results = compression_retriever_stage2.invoke(query)\n",
    "    stage2_time = time.time() - start_time\n",
    "    \n",
    "    # Stage 3: Optional LLM reranking\n",
    "    if use_llm_rerank:\n",
    "        final_results = llm_rerank(query, stage2_results, top_n=final_top_k)\n",
    "        stage3_time = time.time() - start_time\n",
    "    else:\n",
    "        final_results = stage2_results[:final_top_k]\n",
    "        stage3_time = stage2_time\n",
    "    \n",
    "    # Package results with metadata\n",
    "    return {\n",
    "        \"documents\": final_results,\n",
    "        \"timing\": {\n",
    "            \"stage1_hybrid\": f\"{stage1_time:.3f}s\",\n",
    "            \"stage2_cross_encoder\": f\"{stage2_time:.3f}s\",\n",
    "            \"total\": f\"{stage3_time:.3f}s\"\n",
    "        },\n",
    "        \"stages_used\": 3 if use_llm_rerank else 2\n",
    "    }\n",
    "\n",
    "# Test both configurations\n",
    "query = \"What is transformer and why is better than LSTM??\"\n",
    "\n",
    "print(\"\\nPRODUCTION CONFIGURATION 1: Cross-encoder only\")\n",
    "result1 = production_retrieve_and_rerank(query, use_llm_rerank=False)\n",
    "print(f\"Stages used: {result1['stages_used']}\")\n",
    "print(f\"Timing: {result1['timing']}\")\n",
    "\n",
    "print(\"\\nPRODUCTION CONFIGURATION 2: Full pipeline with LLM\")\n",
    "result2 = production_retrieve_and_rerank(query, use_llm_rerank=True)\n",
    "print(f\"Stages used: {result2['stages_used']}\")\n",
    "print(f\"Timing: {result2['timing']}\")\n",
    "\n",
    "print(\"\\nTop result (LLM-reranked):\")\n",
    "print(result2['documents'][0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_takeaways",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "**When to Use Each Approach**:\n",
    "\n",
    "1. **Hybrid Only**: \n",
    "   - High-volume applications (>1000 QPS)\n",
    "   - Latency critical (<50ms requirement)\n",
    "   - Good enough quality for most queries\n",
    "\n",
    "2. **Hybrid + Cross-Encoder**:\n",
    "   - Production default for most applications\n",
    "   - Moderate latency acceptable (100-200ms)\n",
    "   - Significant quality improvement for minimal cost\n",
    "   - Best bang for buck\n",
    "\n",
    "3. **Hybrid + Cross-Encoder + LLM**:\n",
    "   - Complex queries requiring reasoning\n",
    "   - Low volume, high value use cases\n",
    "   - User-facing features where quality matters most\n",
    "   - Budget for LLM API costs\n",
    "\n",
    "**Cost Comparison** (rough estimates per 1000 queries):\n",
    "- Hybrid only: ~$0\n",
    "- + Cross-encoder: ~$0 (self-hosted) or ~$0.10 (API)\n",
    "- + LLM rerank: ~$5-50 depending on model\n",
    "\n",
    "**Quality Improvements** (measured by NDCG@5):\n",
    "- Hybrid only: Baseline\n",
    "- + Cross-encoder: +10-20% improvement\n",
    "- + LLM rerank: +5-15% additional improvement\n",
    "\n",
    "**Production Recommendation**: \n",
    "Start with Hybrid + Cross-encoder. Add LLM reranking selectively based on query complexity signals (question length, ambiguity, user tier)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
