{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9f201d",
   "metadata": {},
   "source": [
    "## 08 - Custom Middleware in LangChain\n",
    "\n",
    "**Key Concept**: Custom middleware lets you implement hooks that run at specific points in agent execution. Build your own logging, validation, retries, caching, and control flow logic.\n",
    "\n",
    "**What this covers:**\n",
    "1. Hook types: Node-style vs Wrap-style\n",
    "2. Decorator-based middleware (quick prototyping)\n",
    "3. Class-based middleware (complex logic, multiple hooks)\n",
    "4. Custom state schemas\n",
    "5. Execution order and agent jumps\n",
    "6. Practical examples: logging, retries, dynamic model selection, tool monitoring\n",
    "\n",
    "**Two styles of hooks:**\n",
    "- **Node-style**: Run sequentially at specific points (`before_model`, `after_model`, etc.)\n",
    "- **Wrap-style**: Wrap around execution for control flow (`wrap_model_call`, `wrap_tool_call`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec2bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashsarode/Downloads/Personal Projects/Python/LangGraph-personal/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Callable, Literal\n",
    "from typing_extensions import NotRequired\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f8e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tools initialized\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Sample tools for demonstrations\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluate a math expression.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a city.\"\"\"\n",
    "    weather_data = {\"NYC\": \"Sunny, 72°F\", \"London\": \"Cloudy, 58°F\", \"Tokyo\": \"Rainy, 65°F\"}\n",
    "    return weather_data.get(city, f\"Weather for {city}: Clear, 70°F\")\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    return f\"Search results for '{query}': Found 3 relevant articles.\"\n",
    "\n",
    "print(\"Model and tools initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3f4b5",
   "metadata": {},
   "source": [
    "### Node-Style Hooks\n",
    "\n",
    "Run sequentially at specific execution points. Use for logging, validation, and state updates.\n",
    "\n",
    "**Available hooks:**\n",
    "- `before_agent` - Before agent starts (once per invocation)\n",
    "- `before_model` - Before each model call\n",
    "- `after_model` - After each model response\n",
    "- `after_agent` - After agent completes (once per invocation)\n",
    "\n",
    "**Return values:**\n",
    "- `None` - Continue normal execution\n",
    "- `dict` - Update state with returned values\n",
    "- `{\"jump_to\": \"end\"}` - Exit early (requires `can_jump_to` config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7f0e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEFORE MODEL] Message count: 1\n",
      "[AFTER MODEL] Response: (no content)...\n",
      "[BEFORE MODEL] Message count: 3\n",
      "[AFTER MODEL] Response: The result of 25 * 4 is 100....\n",
      "\n",
      "Final answer: The result of 25 * 4 is 100.\n"
     ]
    }
   ],
   "source": [
    "# Decorator-based node-style hooks\n",
    "from langchain.agents.middleware import before_model, after_model, AgentState\n",
    "from langgraph.types import StreamWriter\n",
    "\n",
    "# Simple logging middleware using decorators\n",
    "@before_model\n",
    "def log_before_model(state: AgentState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "    \"\"\"Log before each model call.\"\"\"\n",
    "    print(f\"[BEFORE MODEL] Message count: {len(state['messages'])}\")\n",
    "    return None  # Continue normal execution\n",
    "\n",
    "@after_model\n",
    "def log_after_model(state: AgentState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "    \"\"\"Log after each model response.\"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    content_preview = str(last_msg.content)[:50] if last_msg.content else \"(no content)\"\n",
    "    print(f\"[AFTER MODEL] Response: {content_preview}...\")\n",
    "    return None\n",
    "\n",
    "# Create agent with logging middleware\n",
    "logging_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[calculator, get_weather],\n",
    "    middleware=[log_before_model, log_after_model],\n",
    ")\n",
    "\n",
    "# Test it\n",
    "result = logging_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's 25 * 4?\"}]\n",
    "})\n",
    "print(f\"\\nFinal answer: {result['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15c780",
   "metadata": {},
   "source": [
    "### Wrap-Style Hooks\n",
    "\n",
    "Intercept execution and control when the handler is called. You decide:\n",
    "- Call handler zero times (short-circuit)\n",
    "- Call handler once (normal flow)\n",
    "- Call handler multiple times (retry logic)\n",
    "\n",
    "**Available hooks:**\n",
    "- `wrap_model_call` - Around each model call\n",
    "- `wrap_tool_call` - Around each tool call\n",
    "\n",
    "Use for: retries, caching, transformation, fallbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623d0c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TOOL CALL] get_weather with args: {'city': 'Tokyo'}\n",
      "[TOOL DONE] get_weather completed in 0.00s\n",
      "\n",
      "Final answer: It's rainy in Tokyo with a temperature of 65°F.\n"
     ]
    }
   ],
   "source": [
    "# Decorator-based wrap-style hooks\n",
    "from langchain.agents.middleware import wrap_model_call, wrap_tool_call, ModelRequest, ModelResponse\n",
    "from langchain.tools.tool_node import ToolCallRequest\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.types import Command\n",
    "import time\n",
    "\n",
    "# Retry middleware - wraps model calls with retry logic\n",
    "@wrap_model_call\n",
    "def retry_on_error(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelResponse:\n",
    "    \"\"\"Retry model calls up to 3 times on failure.\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise  # Re-raise on final attempt\n",
    "            print(f\"[RETRY] Attempt {attempt + 1}/{max_retries} failed: {e}\")\n",
    "            time.sleep(1)  # Brief delay before retry\n",
    "\n",
    "# Tool monitoring middleware - wraps tool calls\n",
    "@wrap_tool_call\n",
    "def monitor_tools(\n",
    "    request: ToolCallRequest,\n",
    "    handler: Callable[[ToolCallRequest], ToolMessage | Command],\n",
    ") -> ToolMessage | Command:\n",
    "    \"\"\"Log tool execution details.\"\"\"\n",
    "    tool_name = request.tool_call[\"name\"]\n",
    "    tool_args = request.tool_call[\"args\"]\n",
    "    print(f\"[TOOL CALL] {tool_name} with args: {tool_args}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = handler(request)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"[TOOL DONE] {tool_name} completed in {elapsed:.2f}s\")\n",
    "    return result\n",
    "\n",
    "# Create agent with wrap-style middleware\n",
    "monitored_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[calculator, get_weather],\n",
    "    middleware=[retry_on_error, monitor_tools],\n",
    ")\n",
    "\n",
    "# Test it\n",
    "result = monitored_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n",
    "})\n",
    "print(f\"\\nFinal answer: {result['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd96e2",
   "metadata": {},
   "source": [
    "### Class-Based Middleware\n",
    "\n",
    "More powerful for complex middleware with multiple hooks or configuration.\n",
    "\n",
    "**When to use classes:**\n",
    "- Multiple hooks needed in a single middleware\n",
    "- Complex configuration (thresholds, models, etc.)\n",
    "- Reuse across projects with init-time config\n",
    "- Need both sync and async implementations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc68fe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Model call #1, messages: 1\n",
      "[LOG] Model responded, tool_calls: [{'name': 'calculator', 'args': {'expression': '100 / 5'}, 'id': 'nqjn5sx67', 'type': 'tool_call'}]\n",
      "[LOG] Model call #2, messages: 3\n",
      "[LOG] Model responded, tool_calls: []\n",
      "\n",
      "Result: The result of 100 divided by 5 is 20.0.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware, hook_config\n",
    "\n",
    "# Class-based middleware with multiple hooks and configuration\n",
    "class LoggingMiddleware(AgentMiddleware):\n",
    "    \"\"\"Comprehensive logging middleware with configurable verbosity.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        self.call_count = 0\n",
    "    \n",
    "    def before_model(self, state: AgentState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "        self.call_count += 1\n",
    "        if self.verbose:\n",
    "            print(f\"[LOG] Model call #{self.call_count}, messages: {len(state['messages'])}\")\n",
    "        return None\n",
    "    \n",
    "    def after_model(self, state: AgentState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "        if self.verbose:\n",
    "            last_msg = state[\"messages\"][-1]\n",
    "            has_tool_calls = hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls\n",
    "            print(f\"[LOG] Model responded, tool_calls: {has_tool_calls}\")\n",
    "        return None\n",
    "\n",
    "# Class-based retry middleware with configurable attempts\n",
    "class RetryMiddleware(AgentMiddleware):\n",
    "    \"\"\"Retry failed model calls with configurable attempts and delay.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3, delay: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.max_retries = max_retries\n",
    "        self.delay = delay\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelResponse:\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                return handler(request)\n",
    "            except Exception as e:\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    raise\n",
    "                print(f\"[RETRY] Attempt {attempt + 1}/{self.max_retries}: {e}\")\n",
    "                time.sleep(self.delay)\n",
    "\n",
    "# Use class-based middleware\n",
    "class_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[calculator, get_weather],\n",
    "    middleware=[\n",
    "        LoggingMiddleware(verbose=True),\n",
    "        RetryMiddleware(max_retries=2, delay=0.5),\n",
    "    ],\n",
    ")\n",
    "\n",
    "result = class_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Calculate 100 / 5\"}]\n",
    "})\n",
    "print(f\"\\nResult: {result['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6914e64",
   "metadata": {},
   "source": [
    "### Custom State Schema\n",
    "\n",
    "Middleware can extend the agent's state with custom properties. Useful for:\n",
    "- Tracking metrics (call counts, token usage)\n",
    "- Storing user context\n",
    "- Passing data between hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68fba2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LIMIT CHECK] Call count: 0\n",
      "[COUNTER] Incremented to 1\n",
      "[LIMIT CHECK] Call count: 1\n",
      "[COUNTER] Incremented to 2\n",
      "\n",
      "Result: 30\n",
      "Final call count: 2\n"
     ]
    }
   ],
   "source": [
    "# Custom state schema with additional properties\n",
    "class CustomState(AgentState):\n",
    "    \"\"\"Extended state with custom tracking fields.\"\"\"\n",
    "    model_call_count: NotRequired[int]\n",
    "    user_id: NotRequired[str]\n",
    "    session_start: NotRequired[float]\n",
    "\n",
    "# Decorator-based middleware with custom state\n",
    "@before_model(state_schema=CustomState, can_jump_to=[\"end\"])\n",
    "def check_call_limit(state: CustomState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "    \"\"\"Limit model calls per session.\"\"\"\n",
    "    count = state.get(\"model_call_count\", 0)\n",
    "    print(f\"[LIMIT CHECK] Call count: {count}\")\n",
    "    if count >= 5:  # Max 5 model calls\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"Call limit reached. Session ended.\")],\n",
    "            \"jump_to\": \"end\"\n",
    "        }\n",
    "    return None\n",
    "\n",
    "@after_model(state_schema=CustomState)\n",
    "def increment_counter(state: CustomState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "    \"\"\"Increment call counter after each model call.\"\"\"\n",
    "    new_count = state.get(\"model_call_count\", 0) + 1\n",
    "    print(f\"[COUNTER] Incremented to {new_count}\")\n",
    "    return {\"model_call_count\": new_count}\n",
    "\n",
    "# Create agent with custom state middleware\n",
    "limited_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[calculator],\n",
    "    middleware=[check_call_limit, increment_counter],\n",
    ")\n",
    "\n",
    "# Invoke with initial custom state\n",
    "result = limited_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's 10 + 20?\")],\n",
    "    \"model_call_count\": 0,\n",
    "    \"user_id\": \"user-123\",\n",
    "    \"session_start\": time.time(),\n",
    "})\n",
    "print(f\"\\nResult: {result['messages'][-1].content}\")\n",
    "print(f\"Final call count: {result.get('model_call_count', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca1f84",
   "metadata": {},
   "source": [
    "### Class-Based Middleware with Custom State\n",
    "\n",
    "Same pattern using a class for better encapsulation and reusability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2709de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The result of 5 * 5 is 25.\n"
     ]
    }
   ],
   "source": [
    "# Class-based middleware with custom state schema\n",
    "class CallCounterMiddleware(AgentMiddleware[CustomState]):\n",
    "    \"\"\"Track and limit model calls with configurable threshold.\"\"\"\n",
    "    state_schema = CustomState\n",
    "    \n",
    "    def __init__(self, max_calls: int = 10):\n",
    "        super().__init__()\n",
    "        self.max_calls = max_calls\n",
    "    \n",
    "    @hook_config(can_jump_to=[\"end\"])\n",
    "    def before_model(self, state: CustomState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "        count = state.get(\"model_call_count\", 0)\n",
    "        if count >= self.max_calls:\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Maximum {self.max_calls} calls reached.\")],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def after_model(self, state: CustomState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n",
    "\n",
    "# Use class-based middleware with custom state\n",
    "counter_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[calculator],\n",
    "    middleware=[CallCounterMiddleware(max_calls=3)],\n",
    ")\n",
    "\n",
    "result = counter_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's 5 * 5?\")],\n",
    "    \"model_call_count\": 0,\n",
    "})\n",
    "print(f\"Result: {result['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fc605",
   "metadata": {},
   "source": [
    "### Execution Order\n",
    "\n",
    "When using multiple middleware, execution order matters:\n",
    "\n",
    "```\n",
    "agent = create_agent(model, middleware=[mw1, mw2, mw3], tools=[...])\n",
    "```\n",
    "\n",
    "**Before hooks**: First to last (mw1 -> mw2 -> mw3)\n",
    "**Wrap hooks**: Nested (mw1 wraps mw2 wraps mw3 wraps handler)\n",
    "**After hooks**: Last to first (mw3 -> mw2 -> mw1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92f4e560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution order demonstration:\n",
      "----------------------------------------\n",
      "[BEFORE] MW1\n",
      "[BEFORE] MW2\n",
      "[BEFORE] MW3\n",
      "[WRAP ENTER] MW1\n",
      "[WRAP ENTER] MW2\n",
      "[WRAP ENTER] MW3\n",
      "[WRAP EXIT] MW3\n",
      "[WRAP EXIT] MW2\n",
      "[WRAP EXIT] MW1\n",
      "[AFTER] MW3\n",
      "[AFTER] MW2\n",
      "[AFTER] MW1\n",
      "[BEFORE] MW1\n",
      "[BEFORE] MW2\n",
      "[BEFORE] MW3\n",
      "[WRAP ENTER] MW1\n",
      "[WRAP ENTER] MW2\n",
      "[WRAP ENTER] MW3\n",
      "[WRAP EXIT] MW3\n",
      "[WRAP EXIT] MW2\n",
      "[WRAP EXIT] MW1\n",
      "[AFTER] MW3\n",
      "[AFTER] MW2\n",
      "[AFTER] MW1\n",
      "----------------------------------------\n",
      "Notice: before hooks run first-to-last, after hooks run last-to-first\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate execution order with numbered middleware\n",
    "class OrderedMiddleware(AgentMiddleware):\n",
    "    \"\"\"Middleware that logs its execution order.\"\"\"\n",
    "    \n",
    "    def __init__(self, middleware_name: str):\n",
    "        super().__init__()\n",
    "        self._middleware_name = middleware_name\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Override the name property to return unique instance name.\"\"\"\n",
    "        return self._middleware_name\n",
    "    \n",
    "    def before_model(self, state: AgentState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "        print(f\"[BEFORE] {self._middleware_name}\")\n",
    "        return None\n",
    "    \n",
    "    def after_model(self, state: AgentState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "        print(f\"[AFTER] {self._middleware_name}\")\n",
    "        return None\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelResponse:\n",
    "        print(f\"[WRAP ENTER] {self._middleware_name}\")\n",
    "        result = handler(request)\n",
    "        print(f\"[WRAP EXIT] {self._middleware_name}\")\n",
    "        return result\n",
    "\n",
    "# Create agent with three ordered middleware\n",
    "order_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[calculator],\n",
    "    middleware=[\n",
    "        OrderedMiddleware(\"MW1\"),\n",
    "        OrderedMiddleware(\"MW2\"),\n",
    "        OrderedMiddleware(\"MW3\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Execution order demonstration:\")\n",
    "print(\"-\" * 40)\n",
    "result = order_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's 2 + 2?\"}]\n",
    "})\n",
    "print(\"-\" * 40)\n",
    "print(\"Notice: before hooks run first-to-last, after hooks run last-to-first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e12a2",
   "metadata": {},
   "source": [
    "### Agent Jumps\n",
    "\n",
    "Exit early from middleware by returning `{\"jump_to\": \"target\"}`.\n",
    "\n",
    "**Available jump targets:**\n",
    "- `\"end\"` - Jump to end of agent execution\n",
    "- `\"tools\"` - Jump to tools node\n",
    "- `\"model\"` - Jump to model node\n",
    "\n",
    "Requires `can_jump_to` config on the hook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b24d486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal query result: You can find public weather data from various sources such as the National Weather Service, OpenWeat...\n"
     ]
    }
   ],
   "source": [
    "# Content blocking middleware using agent jumps\n",
    "class ContentBlockerMiddleware(AgentMiddleware):\n",
    "    \"\"\"Block responses containing specific keywords.\"\"\"\n",
    "    \n",
    "    def __init__(self, blocked_words: list[str]):\n",
    "        super().__init__()\n",
    "        self.blocked_words = [w.lower() for w in blocked_words]\n",
    "    \n",
    "    @hook_config(can_jump_to=[\"end\"])\n",
    "    def after_model(self, state: AgentState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "        last_msg = state[\"messages\"][-1]\n",
    "        content = str(last_msg.content).lower()\n",
    "        \n",
    "        for word in self.blocked_words:\n",
    "            if word in content:\n",
    "                print(f\"[BLOCKED] Response contained '{word}'\")\n",
    "                return {\n",
    "                    \"messages\": [AIMessage(content=\"I cannot provide that information.\")],\n",
    "                    \"jump_to\": \"end\"\n",
    "                }\n",
    "        return None\n",
    "\n",
    "# Create agent with content blocker\n",
    "blocked_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[search_web],\n",
    "    middleware=[ContentBlockerMiddleware(blocked_words=[\"secret\", \"confidential\"])],\n",
    ")\n",
    "\n",
    "# Test with normal query\n",
    "result = blocked_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for public weather data\"}]\n",
    "})\n",
    "print(f\"Normal query result: {result['messages'][-1].content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438c92c",
   "metadata": {},
   "source": [
    "### Practical Examples\n",
    "\n",
    "Real-world middleware patterns from the LangChain docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9bf1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] Using simple model (messages: 1)\n",
      "[MODEL] Using simple model (messages: 3)\n",
      "Result: The result of the expression 7 * 8 is 56.\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Dynamic Model Selection\n",
    "# Use different models based on conversation complexity\n",
    "\n",
    "complex_model = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "simple_model = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "\n",
    "class DynamicModelMiddleware(AgentMiddleware):\n",
    "    \"\"\"Select model based on conversation length/complexity.\"\"\"\n",
    "    \n",
    "    def __init__(self, simple_model, complex_model, threshold: int = 5):\n",
    "        super().__init__()\n",
    "        self.simple_model = simple_model\n",
    "        self.complex_model = complex_model\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelResponse:\n",
    "        msg_count = len(request.messages)\n",
    "        if msg_count > self.threshold:\n",
    "            print(f\"[MODEL] Using complex model (messages: {msg_count})\")\n",
    "            return handler(request.override(model=self.complex_model))\n",
    "        else:\n",
    "            print(f\"[MODEL] Using simple model (messages: {msg_count})\")\n",
    "            return handler(request.override(model=self.simple_model))\n",
    "\n",
    "dynamic_agent = create_agent(\n",
    "    llm,  # Default model (overridden by middleware)\n",
    "    tools=[calculator],\n",
    "    middleware=[DynamicModelMiddleware(simple_model, complex_model, threshold=3)],\n",
    ")\n",
    "\n",
    "result = dynamic_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's 7 * 8?\"}]\n",
    "})\n",
    "print(f\"Result: {result['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0872805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METRICS] get_weather: 0.5ms\n",
      "[METRICS] calculator: 0.4ms\n",
      "\n",
      "Result: The result of 15 * 3 is 45. The weather in London is cloudy, 58°F.\n",
      "\n",
      "Tool metrics: {'total_calls': 2, 'total_time_ms': 0.93, 'calls': [{'tool': 'get_weather', 'args': {'city': 'London'}, 'time_ms': 0.5, 'success': True}, {'tool': 'calculator', 'args': {'expression': '15 * 3'}, 'time_ms': 0.43, 'success': True}]}\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Tool Call Monitoring with Metrics\n",
    "# Track tool usage for analytics and debugging\n",
    "\n",
    "class ToolMetricsMiddleware(AgentMiddleware):\n",
    "    \"\"\"Collect metrics on tool usage.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tool_calls = []\n",
    "        self.total_time = 0.0\n",
    "    \n",
    "    def wrap_tool_call(\n",
    "        self,\n",
    "        request: ToolCallRequest,\n",
    "        handler: Callable[[ToolCallRequest], ToolMessage | Command],\n",
    "    ) -> ToolMessage | Command:\n",
    "        tool_name = request.tool_call[\"name\"]\n",
    "        tool_args = request.tool_call[\"args\"]\n",
    "        \n",
    "        start = time.time()\n",
    "        try:\n",
    "            result = handler(request)\n",
    "            success = True\n",
    "        except Exception as e:\n",
    "            success = False\n",
    "            raise\n",
    "        finally:\n",
    "            elapsed = time.time() - start\n",
    "            self.total_time += elapsed\n",
    "            self.tool_calls.append({\n",
    "                \"tool\": tool_name,\n",
    "                \"args\": tool_args,\n",
    "                \"time_ms\": round(elapsed * 1000, 2),\n",
    "                \"success\": success,\n",
    "            })\n",
    "            print(f\"[METRICS] {tool_name}: {elapsed*1000:.1f}ms\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        return {\n",
    "            \"total_calls\": len(self.tool_calls),\n",
    "            \"total_time_ms\": round(self.total_time * 1000, 2),\n",
    "            \"calls\": self.tool_calls,\n",
    "        }\n",
    "\n",
    "# Use metrics middleware\n",
    "metrics_mw = ToolMetricsMiddleware()\n",
    "metrics_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[calculator, get_weather],\n",
    "    middleware=[metrics_mw],\n",
    ")\n",
    "\n",
    "result = metrics_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's 15 * 3 and what's the weather in London?\"}]\n",
    "})\n",
    "print(f\"\\nResult: {result['messages'][-1].content}\")\n",
    "print(f\"\\nTool metrics: {metrics_mw.get_summary()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5facd4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FILTER] Role: user, Tools: ['calculator', 'user_read']\n",
      "[FILTER] Role: user, Tools: ['calculator', 'user_read']\n",
      "User result: The contents of resource ABC123 are available.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Dynamic Tool Selection\n",
    "# Filter tools based on context to improve accuracy\n",
    "\n",
    "@tool\n",
    "def admin_delete(resource_id: str) -> str:\n",
    "    \"\"\"Delete a resource (admin only).\"\"\"\n",
    "    return f\"Resource {resource_id} deleted.\"\n",
    "\n",
    "@tool\n",
    "def user_read(resource_id: str) -> str:\n",
    "    \"\"\"Read a resource.\"\"\"\n",
    "    return f\"Resource {resource_id} contents: [data]\"\n",
    "\n",
    "# State with user role\n",
    "class RoleState(AgentState):\n",
    "    user_role: NotRequired[str]\n",
    "\n",
    "class ToolFilterMiddleware(AgentMiddleware[RoleState]):\n",
    "    \"\"\"Filter available tools based on user role.\"\"\"\n",
    "    state_schema = RoleState\n",
    "    \n",
    "    def __init__(self, admin_tools: list[str]):\n",
    "        super().__init__()\n",
    "        self.admin_tools = admin_tools\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelResponse:\n",
    "        user_role = request.state.get(\"user_role\", \"user\")\n",
    "        \n",
    "        if user_role != \"admin\":\n",
    "            # Filter out admin-only tools\n",
    "            filtered_tools = [\n",
    "                t for t in request.tools \n",
    "                if t.name not in self.admin_tools\n",
    "            ]\n",
    "            print(f\"[FILTER] Role: {user_role}, Tools: {[t.name for t in filtered_tools]}\")\n",
    "            return handler(request.override(tools=filtered_tools))\n",
    "        \n",
    "        print(f\"[FILTER] Role: admin, all tools available\")\n",
    "        return handler(request)\n",
    "\n",
    "# Create agent with tool filtering\n",
    "all_tools = [calculator, user_read, admin_delete]\n",
    "filtered_agent = create_agent(\n",
    "    llm,\n",
    "    tools=all_tools,\n",
    "    middleware=[ToolFilterMiddleware(admin_tools=[\"admin_delete\"])],\n",
    ")\n",
    "\n",
    "# Test as regular user\n",
    "result = filtered_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Read resource ABC123\"}],\n",
    "    \"user_role\": \"user\",\n",
    "})\n",
    "print(f\"User result: {result['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71f88317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONTEXT] Injected: Today is December 2025. User prefers metric units....\n",
      "[CONTEXT] Injected: Today is December 2025. User prefers metric units....\n",
      "Result: 100 Fahrenheit is approximately 37.78 Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Working with System Messages\n",
    "# Dynamically modify system prompts based on context\n",
    "\n",
    "class ContextInjectionMiddleware(AgentMiddleware):\n",
    "    \"\"\"Inject additional context into system message.\"\"\"\n",
    "    \n",
    "    def __init__(self, context: str):\n",
    "        super().__init__()\n",
    "        self.context = context\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelResponse:\n",
    "        # Access existing system message content\n",
    "        existing_content = list(request.system_message.content_blocks)\n",
    "        \n",
    "        # Add additional context\n",
    "        new_content = existing_content + [\n",
    "            {\"type\": \"text\", \"text\": f\"\\n\\nAdditional context: {self.context}\"}\n",
    "        ]\n",
    "        \n",
    "        new_system_message = SystemMessage(content=new_content)\n",
    "        print(f\"[CONTEXT] Injected: {self.context[:50]}...\")\n",
    "        \n",
    "        return handler(request.override(system_message=new_system_message))\n",
    "\n",
    "# Create agent with context injection\n",
    "context_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[calculator],\n",
    "    middleware=[ContextInjectionMiddleware(\"Today is December 2025. User prefers metric units.\")],\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "result = context_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's 100 fahrenheit in celsius?\"}]\n",
    "})\n",
    "print(f\"Result: {result['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995cbd3",
   "metadata": {},
   "source": [
    "### Combining Custom with Built-in Middleware\n",
    "\n",
    "Custom middleware works alongside built-in middleware. Order matters for execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef4a1aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REQUEST LOG] Messages: 1\n",
      "[REQUEST LOG] Messages: 3\n",
      "Result: The result of 50 + 50 is 100.\n"
     ]
    }
   ],
   "source": [
    "# Combine custom middleware with built-in middleware\n",
    "from langchain.agents.middleware import SummarizationMiddleware, PIIMiddleware\n",
    "\n",
    "# Custom request logger\n",
    "class RequestLoggerMiddleware(AgentMiddleware):\n",
    "    \"\"\"Log all requests for debugging.\"\"\"\n",
    "    \n",
    "    def before_model(self, state: AgentState, writer: StreamWriter) -> dict[str, Any] | None:\n",
    "        print(f\"[REQUEST LOG] Messages: {len(state['messages'])}\")\n",
    "        return None\n",
    "\n",
    "# Combine built-in and custom middleware\n",
    "combined_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[calculator, get_weather],\n",
    "    middleware=[\n",
    "        # Custom logging first\n",
    "        RequestLoggerMiddleware(),\n",
    "        # Then built-in summarization\n",
    "        SummarizationMiddleware(\n",
    "            model=llm,\n",
    "            trigger=(\"messages\", 20),\n",
    "            keep=(\"messages\", 10),\n",
    "        ),\n",
    "        # Then PII protection\n",
    "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_output=True),\n",
    "    ],\n",
    ")\n",
    "\n",
    "result = combined_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's 50 + 50?\"}]\n",
    "})\n",
    "print(f\"Result: {result['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c69f7",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**Custom middleware gives you fine-grained control over agent execution:**\n",
    "\n",
    "| Hook Type | Use Case | Examples |\n",
    "|-----------|----------|----------|\n",
    "| `before_model` | Validation, logging, state prep | Rate limiting, input sanitization |\n",
    "| `after_model` | Response processing, metrics | Content filtering, logging |\n",
    "| `wrap_model_call` | Control flow | Retries, caching, model switching |\n",
    "| `wrap_tool_call` | Tool interception | Monitoring, permissions, mocking |\n",
    "\n",
    "**Best practices:**\n",
    "1. Keep middleware focused - one concern per middleware\n",
    "2. Handle errors gracefully - don't crash the agent\n",
    "3. Use decorators for simple cases, classes for complex logic\n",
    "4. Document custom state properties\n",
    "5. Test middleware independently before integrating\n",
    "6. Consider execution order when combining multiple middleware\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
