{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "### Query Expansion for Better Retrieval\n",
    "\n",
    "**The Problem**: User queries are often too short or use different terminology than documents. A query like \"transformers\" might miss documents about \"attention mechanisms\" or \"self-attention layers\".\n",
    "\n",
    "**The Solution**: Query expansion generates multiple variations of the original query to cast a wider net.\n",
    "\n",
    "**Why It Matters**:\n",
    "- Better recall: Catch documents using synonyms or related terms\n",
    "- Overcome vocabulary mismatch: User language vs document language\n",
    "- More robust retrieval: Single query might miss relevant docs\n",
    "\n",
    "**Techniques Covered**:\n",
    "1. Multi-query expansion: Generate 3-5 alternative phrasings\n",
    "2. Query rewriting: Add technical terms and context\n",
    "3. Step-back prompting: Ask broader questions for context\n",
    "4. HyDE: Generate hypothetical answers and search with them\n",
    "5. Query decomposition: Break complex queries into sub-queries\n",
    "6. Agent integration: Use all techniques with LangChain agents\n",
    "\n",
    "**The Trade-off**: More queries = better results but higher latency. Choose based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashsarode/Downloads/Personal Projects/Python/LangGraph-personal/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "### Load Wikipedia Data\n",
    "\n",
    "Using Wikipedia for this demo because:\n",
    "- Rich, technical content on transformers and deep learning\n",
    "- Good test case for terminology mismatch (formal vs informal language)\n",
    "- Real-world complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_docs",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashsarode/Downloads/Personal Projects/Python/LangGraph-personal/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/yashsarode/Downloads/Personal Projects/Python/LangGraph-personal/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 Wikipedia pages\n",
      "\n",
      "First document preview:\n",
      "In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and e...\n",
      "\n",
      "Article: Transformer (deep learning)\n"
     ]
    }
   ],
   "source": [
    "# Load Wikipedia articles on transformers\n",
    "loader = WikipediaLoader(\n",
    "    query=\"Transformer (deep learning)\",\n",
    "    load_max_docs=10\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} Wikipedia pages\")\n",
    "print(f\"\\nFirst document preview:\")\n",
    "print(documents[0].page_content[:200] + \"...\")\n",
    "print(f\"\\nArticle: {documents[0].metadata.get('title', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chunk_docs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 104 chunks from 9 articles\n",
      "\n",
      "Sample chunk:\n",
      ". A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be...\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} articles\")\n",
    "print(f\"\\nSample chunk:\\n{chunks[10].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "build_vectorstore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store ready\n",
      "Retriever: MMR with k=4 (diverse results)\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Base retriever (no expansion yet)\n",
    "base_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 4, \"fetch_k\": 20}\n",
    ")\n",
    "\n",
    "print(\"Vector store ready\")\n",
    "print(\"Retriever: MMR with k=4 (diverse results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "setup_llm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized for query expansion\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM for query expansion\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0.3  # Some creativity for variations\n",
    ")\n",
    "\n",
    "print(\"LLM initialized for query expansion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_test",
   "metadata": {},
   "source": [
    "### Baseline: No Query Expansion\n",
    "\n",
    "First, test retrieval without expansion to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "test_baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do attention mechanisms work in transformers?\n",
      "\n",
      "======================================================================\n",
      "BASELINE RETRIEVAL (No Expansion)\n",
      "======================================================================\n",
      "\n",
      "Retrieved 4 documents:\n",
      "\n",
      "[1] The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Googl...\n",
      "\n",
      "[2] == Variants ==\n",
      "\n",
      "Many variants of attention implement soft weights, such as...\n",
      "\n",
      "[3] Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using in...\n",
      "\n",
      "[4] Mamba-2 serves as a successor to Mamba by introducing a new theoretical and computational framework called Structured St...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_query = \"How do attention mechanisms work in transformers?\"\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE RETRIEVAL (No Expansion)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_results = base_retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\nRetrieved {len(baseline_results)} documents:\\n\")\n",
    "for i, doc in enumerate(baseline_results, 1):\n",
    "    print(f\"[{i}] {doc.page_content[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiquery_technique",
   "metadata": {},
   "source": [
    "### Technique 1: Multi-Query Expansion\n",
    "\n",
    "**Concept**: Generate 3-5 alternative phrasings, retrieve with all, merge results.\n",
    "\n",
    "**Why It Works**: Different phrasings match different documents, improving recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "multiquery_prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-query expansion chain ready\n"
     ]
    }
   ],
   "source": [
    "multiquery_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Generate 3 different versions of this question to retrieve relevant documents.\n",
    "Use different technical terminology and synonyms while keeping the core intent.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Provide exactly 3 alternative versions, one per line:\n",
    "\"\"\")\n",
    "\n",
    "multiquery_chain = multiquery_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Multi-query expansion chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "multiquery_retrieval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MULTI-QUERY EXPANSION\n",
      "======================================================================\n",
      "Searching with 4 queries:\n",
      "  1. How do attention mechanisms work in transformers?\n",
      "  2. What is the underlying process of attention mechanisms in transformer architectures?\n",
      "  3. Can you explain how self‑attention operates within a transformer model?\n",
      "  4. Describe the functioning of attention modules in transformer‑based networks.\n",
      "\n",
      "Retrieved 10 unique documents\n",
      "\n",
      "\n",
      "[1] The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Googl...\n",
      "\n",
      "[2] == Variants ==\n",
      "\n",
      "Many variants of attention implement soft weights, such as...\n",
      "\n",
      "[3] Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using in...\n",
      "\n",
      "[4] Mamba-2 serves as a successor to Mamba by introducing a new theoretical and computational framework called Structured St...\n"
     ]
    }
   ],
   "source": [
    "def multiquery_retrieval(query: str, retriever, top_k: int = 4):\n",
    "    \"\"\"Retrieve using multiple query variations.\"\"\"\n",
    "    variations_text = multiquery_chain.invoke({\"question\": query})\n",
    "    variations = [\n",
    "        line.strip().lstrip('0123456789.-) ')\n",
    "        for line in variations_text.strip().split(\"\\n\")\n",
    "        if line.strip()\n",
    "    ]\n",
    "    \n",
    "    all_queries = [query] + variations\n",
    "    \n",
    "    print(f\"Searching with {len(all_queries)} queries:\")\n",
    "    for i, q in enumerate(all_queries, 1):\n",
    "        print(f\"  {i}. {q}\")\n",
    "    print()\n",
    "    \n",
    "    all_docs = []\n",
    "    seen_content = set()\n",
    "    \n",
    "    for q in all_queries:\n",
    "        docs = retriever.invoke(q)\n",
    "        for doc in docs:\n",
    "            content_hash = hash(doc.page_content)\n",
    "            if content_hash not in seen_content:\n",
    "                seen_content.add(content_hash)\n",
    "                all_docs.append(doc)\n",
    "    \n",
    "    print(f\"Retrieved {len(all_docs)} unique documents\\n\")\n",
    "    return all_docs[:top_k]\n",
    "\n",
    "# Test\n",
    "print(\"=\" * 70)\n",
    "print(\"MULTI-QUERY EXPANSION\")\n",
    "print(\"=\" * 70)\n",
    "multiquery_results = multiquery_retrieval(test_query, base_retriever, top_k=4)\n",
    "for i, doc in enumerate(multiquery_results, 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rewriting_technique",
   "metadata": {},
   "source": [
    "### Technique 2: Query Rewriting\n",
    "\n",
    "**Concept**: Rewrite query to be more specific and technical (1 improved version → 1 search).\n",
    "\n",
    "**When to Use**: Vague or informal queries need enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rewriting_prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Rewriting Examples\n",
      "======================================================================\n",
      "\n",
      "Original:  how transformers work\n",
      "Rewritten: **Rewritten query:**  \n",
      "How do transformer neural network architectures operate—specifically the self‑attention mechanism, multi‑head attention, positional encoding, layer normalization, and feed‑forward sub‑layers—and what are the underlying principles (e.g., attention‑based deep learning, encoder‑decoder structure, transformer models, and attention mechanisms) that enable their performance?\n",
      "\n",
      "Original:  what is attention\n",
      "Rewritten: **Rewritten query:**  \n",
      "\n",
      "*What is the attention mechanism in deep learning—specifically the scaled‑dot‑product, self‑attention, and multi‑head attention used in Transformer architectures—and how do related concepts such as context weighting, query‑key‑value interactions, alignment scores, and focus/weighting of input representations function and differ?*\n"
     ]
    }
   ],
   "source": [
    "rewriting_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Rewrite this question to be more specific and include relevant technical terms.\n",
    "Add key concepts and synonyms for better retrieval.\n",
    "\n",
    "Original query: {query}\n",
    "\n",
    "Rewritten query:\n",
    "\"\"\")\n",
    "\n",
    "rewriting_chain = rewriting_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test\n",
    "test_queries = [\"how transformers work\", \"what is attention\"]\n",
    "print(\"Query Rewriting Examples\")\n",
    "print(\"=\" * 70)\n",
    "for query in test_queries:\n",
    "    rewritten = rewriting_chain.invoke({\"query\": query})\n",
    "    print(f\"\\nOriginal:  {query}\")\n",
    "    print(f\"Rewritten: {rewritten}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stepback_technique",
   "metadata": {},
   "source": [
    "### Technique 3: Step-Back Prompting\n",
    "\n",
    "**Concept**: Generate a broader, conceptual version of the query to find background documents.\n",
    "\n",
    "**Example**:\n",
    "- Specific: \"How do I configure FAISS indexing?\"\n",
    "- Step-back: \"What are vector databases and how do they work?\"\n",
    "\n",
    "**Why It Works**: Specific queries miss foundational documents. Broader queries provide context.\n",
    "\n",
    "**Pattern**: Search with BOTH step-back and original, merge results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stepback_prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-back prompting chain ready\n"
     ]
    }
   ],
   "source": [
    "stepback_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Given a specific question, generate a broader \"step-back\" question that would help \n",
    "understand the general context and foundational concepts.\n",
    "\n",
    "Examples:\n",
    "- Specific: \"How does scaled dot-product attention work?\"\n",
    "  Step-back: \"What are the different types of attention mechanisms?\"\n",
    "\n",
    "- Specific: \"What is the role of positional encoding?\"\n",
    "  Step-back: \"How do transformers handle sequence information?\"\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Generate a broader step-back question:\n",
    "\"\"\")\n",
    "\n",
    "stepback_chain = stepback_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Step-back prompting chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "test_stepback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP-BACK PROMPTING\n",
      "======================================================================\n",
      "Original query: What is the purpose of multi-head attention?\n",
      "Step-back query: **Step‑back question:**  \n",
      "*How do attention mechanisms, including multi‑head attention, enable transformers to process and represent sequence information?*\n",
      "\n",
      "\n",
      "Final results (interleaved):\n",
      "\n",
      "[1] . At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens v...\n",
      "\n",
      "[2] Additional surveys of the attention mechanism in deep learning are provided by Niu et al. and Soydaner.\n",
      "The major breakt...\n",
      "\n",
      "[3] == Career ==\n",
      "Noam Shazeer joined Google in 2000. One of his first major achievements was improving the spelling correcto...\n",
      "\n",
      "[4] == Variants ==\n",
      "\n",
      "Many variants of attention implement soft weights, such as...\n"
     ]
    }
   ],
   "source": [
    "def stepback_retrieval(query: str, retriever, top_k: int = 4):\n",
    "    \"\"\"Retrieve using both original and step-back queries.\"\"\"\n",
    "    stepback_query = stepback_chain.invoke({\"question\": query}).strip()\n",
    "    \n",
    "    print(f\"Original query: {query}\")\n",
    "    print(f\"Step-back query: {stepback_query}\")\n",
    "    print()\n",
    "    \n",
    "    # Retrieve from both\n",
    "    original_docs = retriever.invoke(query)\n",
    "    stepback_docs = retriever.invoke(stepback_query)\n",
    "    \n",
    "    # Interleave results\n",
    "    combined = []\n",
    "    seen = set()\n",
    "    \n",
    "    for i in range(max(len(original_docs), len(stepback_docs))):\n",
    "        if i < len(original_docs):\n",
    "            doc = original_docs[i]\n",
    "            if doc.page_content not in seen:\n",
    "                combined.append(doc)\n",
    "                seen.add(doc.page_content)\n",
    "        \n",
    "        if i < len(stepback_docs):\n",
    "            doc = stepback_docs[i]\n",
    "            if doc.page_content not in seen:\n",
    "                combined.append(doc)\n",
    "                seen.add(doc.page_content)\n",
    "        \n",
    "        if len(combined) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return combined[:top_k]\n",
    "\n",
    "# Test\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP-BACK PROMPTING\")\n",
    "print(\"=\" * 70)\n",
    "stepback_results = stepback_retrieval(\n",
    "    \"What is the purpose of multi-head attention?\",\n",
    "    base_retriever,\n",
    "    top_k=4\n",
    ")\n",
    "print(\"\\nFinal results (interleaved):\")\n",
    "for i, doc in enumerate(stepback_results, 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyde_technique",
   "metadata": {},
   "source": [
    "### Technique 4: HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "**Concept**: Generate a hypothetical answer, then search with that instead of the query.\n",
    "\n",
    "**The Insight**: \n",
    "- Queries and documents live in different embedding spaces\n",
    "- \"How do transformers work?\" (query) is far from \"Transformers use self-attention...\" (document)\n",
    "- Hypothetical answers are closer to real documents in embedding space\n",
    "\n",
    "**Process**:\n",
    "1. Query: \"How do transformers process sequences?\"\n",
    "2. Generate hypothetical answer: \"Transformers process sequences using self-attention mechanisms that...\"\n",
    "3. Embed the hypothetical answer (not the query)\n",
    "4. Search for documents similar to hypothetical answer\n",
    "\n",
    "**When to Use**: Complex questions where you want documents containing answers, not documents repeating questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hyde_prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyDE chain ready\n"
     ]
    }
   ],
   "source": [
    "hyde_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Generate a concise, factual answer to the following question as if you were writing \n",
    "technical documentation. The answer should be 2-3 sentences.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Hypothetical answer:\n",
    "\"\"\")\n",
    "\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"HyDE chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "test_hyde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HyDE (HYPOTHETICAL DOCUMENT EMBEDDINGS)\n",
      "======================================================================\n",
      "Original query: How do transformers handle long sequences?\n",
      "\n",
      "Hypothetical answer:\n",
      "Transformers process long sequences by applying self‑attention, which compares every token to every other token, giving a quadratic O(N²) time and memory cost in the sequence length N. To mitigate this, modern architectures replace full attention with efficient variants—such as sliding‑window, dilated, or sparse attention, linear‑complexity kernels, and memory‑augmented or recurrent mechanisms—that limit the number of token‑pair interactions while preserving contextual information. These approaches enable handling of sequences that are orders of magnitude longer than the original transformer design.\n",
      "\n",
      "Retrieved documents:\n",
      "\n",
      "[1] Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurr...\n",
      "\n",
      "[2] Additional surveys of the attention mechanism in deep learning are provided by Niu et al. and Soydaner.\n",
      "The major breakt...\n",
      "\n",
      "[3] == Notable works ==\n",
      "Vaswani's most notable paper, \"Attention Is All You Need\", was published in 2017. The paper introduc...\n",
      "\n",
      "[4] . LSTM became the standard architecture for long sequence modelling until the 2017 publication of transformers. However,...\n"
     ]
    }
   ],
   "source": [
    "def hyde_retrieval(query: str, vectorstore, top_k: int = 4):\n",
    "    \"\"\"Retrieve using HyDE - search with hypothetical answer.\"\"\"\n",
    "    hypothetical_answer = hyde_chain.invoke({\"question\": query}).strip()\n",
    "    \n",
    "    print(f\"Original query: {query}\")\n",
    "    print(f\"\\nHypothetical answer:\\n{hypothetical_answer}\")\n",
    "    print()\n",
    "    \n",
    "    # Search using the hypothetical answer\n",
    "    docs = vectorstore.similarity_search(hypothetical_answer, k=top_k)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Test\n",
    "print(\"=\" * 70)\n",
    "print(\"HyDE (HYPOTHETICAL DOCUMENT EMBEDDINGS)\")\n",
    "print(\"=\" * 70)\n",
    "hyde_results = hyde_retrieval(\n",
    "    \"How do transformers handle long sequences?\",\n",
    "    vectorstore,\n",
    "    top_k=4\n",
    ")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(hyde_results, 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decomposition_technique",
   "metadata": {},
   "source": [
    "### Technique 5: Query Decomposition\n",
    "\n",
    "**Concept**: Break complex queries into simpler sub-queries, retrieve for each, then combine.\n",
    "\n",
    "**When to Use**: \n",
    "- Multi-part questions: \"Explain transformers and how they differ from RNNs\"\n",
    "- Questions requiring multiple pieces of information\n",
    "- Comparison questions\n",
    "\n",
    "**Process**:\n",
    "1. Decompose: \"How do transformers work?\" + \"How do RNNs work?\" + \"What's the difference?\"\n",
    "2. Retrieve independently for each sub-query\n",
    "3. Combine results\n",
    "\n",
    "**Benefit**: Each sub-query focuses on one aspect, improving precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "decomposition_prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query decomposition chain ready\n"
     ]
    }
   ],
   "source": [
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Break down this complex question into 2-4 simpler sub-questions that, \n",
    "when answered together, would fully address the original question.\n",
    "\n",
    "Example:\n",
    "Complex: \"Compare attention mechanisms in transformers and RNNs\"\n",
    "Sub-questions:\n",
    "1. How do attention mechanisms work in transformers?\n",
    "2. How do attention mechanisms work in RNNs?\n",
    "3. What are the key differences?\n",
    "\n",
    "Complex question: {question}\n",
    "\n",
    "List sub-questions (one per line):\n",
    "\"\"\")\n",
    "\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Query decomposition chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "test_decomposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY DECOMPOSITION\n",
      "======================================================================\n",
      "Original query: Explain the transformer architecture and its advantages over RNNs\n",
      "\n",
      "Decomposed into 4 sub-queries:\n",
      "  1. What are the core components and workflow of the transformer architecture (e.g., encoder‑decoder layers, multi‑head self‑attention, positional encoding, feed‑forward networks)?\n",
      "  2. How does self‑attention operate within a transformer and how does it differ from the sequential processing used in RNNs?\n",
      "  3. What specific advantages do transformers have over RNNs regarding parallelism, handling long‑range dependencies, training efficiency, and scalability?\n",
      "  4. In which practical tasks or scenarios do transformers outperform RNNs, and what trade‑offs (e.g., computational cost, data requirements) should be considered?\n",
      "\n",
      "Retrieved 6 unique documents\n",
      "\n",
      "Retrieved documents:\n",
      "\n",
      "[1] In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechani...\n",
      "\n",
      "[2] . The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may ...\n",
      "\n",
      "[3] Additional surveys of the attention mechanism in deep learning are provided by Niu et al. and Soydaner.\n",
      "The major breakt...\n",
      "\n",
      "[4] . LSTM became the standard architecture for long sequence modelling until the 2017 publication of transformers. However,...\n"
     ]
    }
   ],
   "source": [
    "def decomposed_retrieval(query: str, retriever, k_per_query: int = 2):\n",
    "    \"\"\"Decompose query and retrieve for each sub-query.\"\"\"\n",
    "    decomposition_text = decomposition_chain.invoke({\"question\": query})\n",
    "    sub_queries = [\n",
    "        q.strip().lstrip('0123456789.-) ') \n",
    "        for q in decomposition_text.strip().split(\"\\n\") \n",
    "        if q.strip()\n",
    "    ]\n",
    "    \n",
    "    print(f\"Original query: {query}\")\n",
    "    print(f\"\\nDecomposed into {len(sub_queries)} sub-queries:\")\n",
    "    for i, sq in enumerate(sub_queries, 1):\n",
    "        print(f\"  {i}. {sq}\")\n",
    "    print()\n",
    "    \n",
    "    all_docs = []\n",
    "    seen = set()\n",
    "    \n",
    "    for sq in sub_queries:\n",
    "        docs = retriever.invoke(sq)\n",
    "        for doc in docs[:k_per_query]:\n",
    "            if doc.page_content not in seen:\n",
    "                all_docs.append(doc)\n",
    "                seen.add(doc.page_content)\n",
    "    \n",
    "    print(f\"Retrieved {len(all_docs)} unique documents\\n\")\n",
    "    return all_docs\n",
    "\n",
    "# Test\n",
    "print(\"=\" * 70)\n",
    "print(\"QUERY DECOMPOSITION\")\n",
    "print(\"=\" * 70)\n",
    "complex_query = \"Explain the transformer architecture and its advantages over RNNs\"\n",
    "decomposed_results = decomposed_retrieval(\n",
    "    complex_query,\n",
    "    base_retriever,\n",
    "    k_per_query=2\n",
    ")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(decomposed_results[:4], 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "### Comparing All Techniques\n",
    "\n",
    "Let's compare all approaches on the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "full_comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON: Query Expansion Techniques\n",
      "================================================================================\n",
      "\n",
      "Query: How does self-attention work in transformers?\n",
      "\n",
      "[1] BASELINE\n",
      "Time: 0.348s | Docs: 4\n",
      "\n",
      "[2] MULTI-QUERY\n",
      "Searching with 4 queries:\n",
      "  1. How does self-attention work in transformers?\n",
      "  2. What is the mechanism behind self‑attention in transformer architectures?\n",
      "  3. How does the self‑attention module operate within a transformer model?\n",
      "  4. Can you explain the inner workings of self‑attention in transformer networks?\n",
      "\n",
      "Retrieved 7 unique documents\n",
      "\n",
      "Time: 0.624s (1.8x)\n",
      "\n",
      "[3] STEP-BACK\n",
      "Original query: How does self-attention work in transformers?\n",
      "Step-back query: **Step‑back question:**  \n",
      "*What are the core principles and components of attention mechanisms that enable transformers to model sequential data?*\n",
      "\n",
      "Time: 0.457s (1.3x)\n",
      "\n",
      "[4] HyDE\n",
      "Original query: How does self-attention work in transformers?\n",
      "\n",
      "Hypothetical answer:\n",
      "Self‑attention computes a weighted sum of token representations, where the weights are derived from pairwise similarity scores between all tokens in the same sequence. For each token, three linear projections produce a **query**, **key**, and **value** vector; the attention scores are obtained by taking the dot product of the query with all keys, scaling, and applying a softmax, then each token’s output is the sum of the values weighted by these scores. This operation is performed in parallel for all tokens and across multiple heads to capture diverse relational patterns.\n",
      "\n",
      "Time: 0.520s (1.5x)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "comparison_query = \"How does self-attention work in transformers?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: Query Expansion Techniques\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nQuery: {comparison_query}\\n\")\n",
    "\n",
    "# Baseline\n",
    "print(\"[1] BASELINE\")\n",
    "start = time.time()\n",
    "baseline = base_retriever.invoke(comparison_query)\n",
    "time_baseline = time.time() - start\n",
    "print(f\"Time: {time_baseline:.3f}s | Docs: {len(baseline)}\")\n",
    "\n",
    "# Multi-query\n",
    "print(\"\\n[2] MULTI-QUERY\")\n",
    "start = time.time()\n",
    "multiquery = multiquery_retrieval(comparison_query, base_retriever, top_k=4)\n",
    "time_multiquery = time.time() - start\n",
    "print(f\"Time: {time_multiquery:.3f}s ({time_multiquery/time_baseline:.1f}x)\")\n",
    "\n",
    "# Step-back\n",
    "print(\"\\n[3] STEP-BACK\")\n",
    "start = time.time()\n",
    "stepback = stepback_retrieval(comparison_query, base_retriever, top_k=4)\n",
    "time_stepback = time.time() - start\n",
    "print(f\"Time: {time_stepback:.3f}s ({time_stepback/time_baseline:.1f}x)\")\n",
    "\n",
    "# HyDE\n",
    "print(\"\\n[4] HyDE\")\n",
    "start = time.time()\n",
    "hyde = hyde_retrieval(comparison_query, vectorstore, top_k=4)\n",
    "time_hyde = time.time() - start\n",
    "print(f\"Time: {time_hyde:.3f}s ({time_hyde/time_baseline:.1f}x)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent_integration",
   "metadata": {},
   "source": [
    "### Agent Integration: All Techniques with LangChain Agents\n",
    "\n",
    "Now we'll build a complete agent that can use all expansion techniques.\n",
    "\n",
    "**Why Use Agents**:\n",
    "- Agents decide which expansion strategy to use\n",
    "- Multi-step reasoning: expand → retrieve → analyze → answer\n",
    "- Can combine multiple techniques\n",
    "\n",
    "This shows how query expansion fits into production agent systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "define_tools",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 4 tools for the agent\n"
     ]
    }
   ],
   "source": [
    "# Define tools for the agent\n",
    "\n",
    "@tool\n",
    "def search_multiquery(query: str) -> str:\n",
    "    \"\"\"Search using multi-query expansion. Use for complex technical questions requiring comprehensive coverage.\"\"\"\n",
    "    docs = multiquery_retrieval(query, base_retriever, top_k=3)\n",
    "    results = [f\"[{i+1}] {doc.page_content[:250]}...\" for i, doc in enumerate(docs)]\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "@tool\n",
    "def search_stepback(query: str) -> str:\n",
    "    \"\"\"Search with step-back prompting. Use when user needs both specific details and broader context.\"\"\"\n",
    "    docs = stepback_retrieval(query, base_retriever, top_k=3)\n",
    "    results = [f\"[{i+1}] {doc.page_content[:250]}...\" for i, doc in enumerate(docs)]\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "@tool\n",
    "def search_hyde(query: str) -> str:\n",
    "    \"\"\"Search using HyDE. Use for questions where you want documents containing detailed explanations.\"\"\"\n",
    "    docs = hyde_retrieval(query, vectorstore, top_k=3)\n",
    "    results = [f\"[{i+1}] {doc.page_content[:250]}...\" for i, doc in enumerate(docs)]\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "@tool\n",
    "def search_decomposed(query: str) -> str:\n",
    "    \"\"\"Search with query decomposition. Use for complex multi-part or comparison questions.\"\"\"\n",
    "    docs = decomposed_retrieval(query, base_retriever, k_per_query=2)\n",
    "    results = [f\"[{i+1}] {doc.page_content[:250]}...\" for i, doc in enumerate(docs[:3])]\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "tools = [search_multiquery, search_stepback, search_hyde, search_decomposed]\n",
    "\n",
    "print(f\"Defined {len(tools)} tools for the agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_agent_system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created successfully\n",
      "Available tools: ['search_multiquery', 'search_stepback', 'search_hyde', 'search_decomposed']\n"
     ]
    }
   ],
   "source": [
    "# Create agent using LangChain v1.x create_agent\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"\"\"\n",
    "You are a helpful AI assistant specializing in deep learning and transformers.\n",
    "\n",
    "You have access to different search strategies:\n",
    "- search_multiquery: For technical questions needing comprehensive coverage\n",
    "- search_stepback: For questions needing both details and broader context  \n",
    "- search_hyde: For questions wanting detailed explanations\n",
    "- search_decomposed: For complex multi-part or comparison questions\n",
    "\n",
    "Choose the appropriate strategy based on the query type.\n",
    "Provide clear, accurate answers based on the retrieved information.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Agent created successfully\")\n",
    "print(f\"Available tools: {[tool.name for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUERY 1: Explain self-attention in transformers\n",
      "================================================================================\n",
      "Original query: self-attention in transformers\n",
      "\n",
      "Hypothetical answer:\n",
      "Self‑attention is the core mechanism in transformer models that allows each token in a sequence to weigh and aggregate information from every other token. For each token, three learned projections produce a query vector **q**, a set of key vectors **k**, and value vectors **v**; attention scores are computed as the scaled dot‑product \\( \\text{softmax}\\big(\\frac{q\\,k^{\\top}}{\\sqrt{d_k}}\\big) \\) and used to form a weighted sum of the values, yielding a context‑aware representation. This operation is performed in parallel for all tokens and across multiple heads to capture diverse relational patterns.\n",
      "\n",
      "\n",
      "Agent Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Self-attention is a key component of the transformer architecture in deep learning. It allows the model to capture global dependencies and relationships between different elements in the input sequence by enabling each element to attend to all others. This is achieved through a multi-head attention mechanism, where text is converted into numerical representations called tokens, and each token is converted into a vector. The self-attention mechanism enables the model to process entire sequences of text at once, facilitating the training of larger and more sophisticated models.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUERY 2: Compare transformers and RNNs for sequence modeling\n",
      "================================================================================\n",
      "Original query: transformers vs RNNs for sequence modeling\n",
      "\n",
      "Decomposed into 4 sub-queries:\n",
      "  1. How do transformers model sequential data, and what are their core mechanisms (e.g., self‑attention, positional encoding)?\n",
      "  2. How do recurrent neural networks (RNNs) model sequential data, and what are their core mechanisms (e.g., hidden state recurrence, gating in LSTM/GRU)?\n",
      "  3. What are the main advantages and disadvantages of transformers versus RNNs for sequence‑modeling tasks (e.g., parallelism, long‑range dependencies, training efficiency, data requirements)?\n",
      "  4. In which types of sequence‑modeling applications or data regimes does each architecture tend to outperform the other?\n",
      "\n",
      "Retrieved 7 unique documents\n",
      "\n",
      "\n",
      "Agent Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Transformers and RNNs are both used for sequence modeling, but they differ in their architecture and functionality. RNNs process sequences one token at a time, from first to last, using sequential processing. In contrast, transformers use self-attention mechanisms, allowing each element in the input sequence to attend to all others, enabling the model to capture global dependencies.\n",
      "\n",
      "Transformers have become the standard architecture for many natural language processing tasks, surpassing RNNs like LSTMs, which were previously the standard for long sequence modeling. The key advantage of transformers is their ability to handle long-range dependencies and parallelize processing, making them more efficient than RNNs for many tasks.\n",
      "\n",
      "In summary, while both transformers and RNNs can be used for sequence modeling, transformers offer significant advantages in terms of their ability to capture global dependencies and parallelize processing, making them a popular choice for many natural language processing tasks.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUERY 3: What are the advantages of transformer architecture?\n",
      "================================================================================\n",
      "Original query: advantages of transformer architecture\n",
      "\n",
      "Hypothetical answer:\n",
      "Transformer architectures offer parallelizable computation by processing all tokens simultaneously, which dramatically reduces training time compared to recurrent models. Their self‑attention mechanism captures long‑range dependencies without a fixed context window, enabling superior performance on a wide range of sequence‑to‑sequence tasks. Additionally, the modular, stackable design scales efficiently across model sizes and hardware accelerators.\n",
      "\n",
      "\n",
      "Agent Response:\n",
      "--------------------------------------------------------------------------------\n",
      "The transformer architecture has several advantages, including the ability to capture global dependencies between input and output sequences using self-attention mechanisms, and requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM) due to the absence of recurrent units.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test agent with different query types\n",
    "test_questions = [\n",
    "    \"Explain self-attention in transformers\",\n",
    "    \"Compare transformers and RNNs for sequence modeling\",\n",
    "    \"What are the advantages of transformer architecture?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"QUERY {i}: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nAgent Response:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result[\"messages\"][-1].content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recommendations",
   "metadata": {},
   "source": [
    "### Production Recommendations\n",
    "\n",
    "**When to Use Each Technique**:\n",
    "\n",
    "1. **Multi-Query Expansion**\n",
    "   - Best for: Terminology mismatch, synonym problems\n",
    "   - Cost: High (3-5x retrieval)\n",
    "   - Use when: Quality >> speed\n",
    "\n",
    "2. **Query Rewriting**\n",
    "   - Best for: Vague or informal queries\n",
    "   - Cost: Low (1 LLM + 1 retrieval)\n",
    "   - Use when: Balanced needs\n",
    "\n",
    "3. **Step-Back Prompting**\n",
    "   - Best for: Specific questions needing context\n",
    "   - Cost: Low (2x retrieval)\n",
    "   - Use when: Users need background + details\n",
    "\n",
    "4. **HyDE**\n",
    "   - Best for: Technical documentation search\n",
    "   - Cost: Medium (LLM + retrieval)\n",
    "   - Use when: Documents are very technical\n",
    "\n",
    "5. **Query Decomposition**\n",
    "   - Best for: Complex multi-part questions\n",
    "   - Cost: High (N sub-queries)\n",
    "   - Use when: Comparison or analysis queries\n",
    "\n",
    "**Implementation Strategy**:\n",
    "\n",
    "Start with: Query rewriting (low cost, good results)\n",
    "Add selectively: Multi-query for critical queries\n",
    "Use agents: Let them choose strategy based on query type\n",
    "\n",
    "**Optimization Tips**:\n",
    "- Cache expanded queries\n",
    "- Use async for parallel retrieval\n",
    "- Set confidence thresholds\n",
    "- Monitor: precision@k, answer quality, latency\n",
    "\n",
    "**Cost per 1000 queries (estimates)**:\n",
    "- Baseline: ~$0.01\n",
    "- Query rewriting: +$0.10-0.50\n",
    "- Multi-query: +$0.30-1.50\n",
    "- Step-back: +$0.10-0.50\n",
    "- HyDE: +$0.20-0.80\n",
    "- Decomposition: +$0.40-2.00\n",
    "- Agent: +$0.50-2.00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
