{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-hybrid",
   "metadata": {},
   "source": [
    "### Hybrid Retrieval (Dense + Sparse) with LangChain v1.1\n",
    "- Blend semantic vectors and keyword scoring to boost recall and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-notes",
   "metadata": {},
   "source": [
    "**What we will build**\n",
    "- Dense retriever with HuggingFace embeddings (semantic match)\n",
    "- Sparse retriever with BM25 (keyword match)\n",
    "- Ensemble retriever to merge both\n",
    "- Agent-driven RAG flow using LangChain v1.1 `create_agent`\n",
    "\n",
    "**Prereqs**\n",
    "- `GROQ_API_KEY` in environment\n",
    "- `langchain`, `langchain-community`, `langchain-huggingface`, `langchain-groq`, `faiss-cpu` installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "if not os.environ[\"GROQ_API_KEY\"]:\n",
    "    raise ValueError(\"Set GROQ_API_KEY in your environment before running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 articles\n",
      "First doc preview:\n",
      " In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashsarode/Downloads/Personal Projects/Python/LangGraph-personal/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/yashsarode/Downloads/Personal Projects/Python/LangGraph-personal/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "# Load a small corpus from Wikipedia to keep the demo self contained\n",
    "loader = WikipediaLoader(query=\"Transformer (deep learning)\", load_max_docs=8)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} articles\")\n",
    "print(\"First doc preview:\\n\", documents[0].page_content[:240], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "splitter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 40 chunks\n",
      "Chunk preview:\n",
      " In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and e ...\n"
     ]
    }
   ],
   "source": [
    "# Chunk the corpus so retrieval is focused and fits the LLM context\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=150)\n",
    "docs = splitter.split_documents(documents)\n",
    "print(f\"Split into {len(docs)} chunks\")\n",
    "print(\"Chunk preview:\\n\", docs[0].page_content[:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace embeddings for dense retrieval (runs locally, no extra API cost)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "test_vector = embeddings.embed_query(docs[0].page_content)\n",
    "print(\"Vector dimension:\", len(test_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dense-retriever",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense retriever ready (semantic search)\n"
     ]
    }
   ],
   "source": [
    "# Dense retriever: FAISS vector store + embedding model\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "dense_store = vector_store.from_documents(docs, embeddings)\n",
    "dense_retriever = dense_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"Dense retriever ready (semantic search)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sparse-retriever",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse retriever ready (keyword search)\n"
     ]
    }
   ],
   "source": [
    "# Sparse retriever: BM25 ranks by keyword overlap\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k = 4\n",
    "print(\"Sparse retriever ready (keyword search)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-vs-sparse",
   "metadata": {},
   "source": [
    "#### Why hybrid?\n",
    "- Dense: great for meaning and paraphrases, can miss exact phrasing\n",
    "- Sparse: great for precise terms, can miss synonyms\n",
    "- Ensemble: weighted blend to keep both semantic coverage and lexical precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ensemble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid retriever assembled (dense + sparse)\n"
     ]
    }
   ],
   "source": [
    "# The Hybrid Approach: Combining Both Worlds\n",
    "# Mathematical Formulation\n",
    "# For each document, you compute two separate scores and combine them:\n",
    "# final_score = (alpha × sparse_score) + ((1 - alpha) × dense_score)\n",
    "# alpha is typically 0.5-0.7, tuned for your use case\n",
    "# Hybrid retriever combines both signals\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weights=[0.5, 0.5],  # tune these for your corpus\n",
    ")\n",
    "print(\"Hybrid retriever assembled (dense + sparse)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "probe-hybrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do transformers differ from RNNs?\n",
      "1. The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. The predecessors of transformers were...\n",
      "2. Modern transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scal...\n",
      "3. == Views ==\n",
      "Shazeer said about artificial general intelligence that he doesn't \"particularly care about AGI in the sense of wanting something that can do absolu...\n",
      "4. In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numeri...\n",
      "5. === Key components ===\n",
      "Selective state spaces (SSM): The core of Mamba, SSMs are recurrent models that selectively process information based on the current inpu...\n",
      "6. A key breakthrough was LSTM (1995), an RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequen...\n"
     ]
    }
   ],
   "source": [
    "# Quick probe to see what hybrid returns\n",
    "query = \"How do transformers differ from RNNs?\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content[:160]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "compare-retrievers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: semantic vector search databases\n",
      "\n",
      "Dense:\n",
      " 1. ×\n",
      "            P\n",
      "            ×\n",
      "            C\n",
      "          \n",
      "        \n",
      "      \n",
      "    \n",
      "    {\\displaystyle \\mathbb {R} ^{P\\times P\\t...\n",
      " 2. Sometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le....\n",
      " 3. == History ==\n",
      "On June 11, 2018, OpenAI researchers and engineers published a paper called \"Improving Language Understand...\n",
      " 4. In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechani...\n",
      "\n",
      "Sparse:\n",
      " 1. ×\n",
      "            P\n",
      "            ×\n",
      "            C\n",
      "          \n",
      "        \n",
      "      \n",
      "    \n",
      "    {\\displaystyle \\mathbb {R} ^{P\\times P\\t...\n",
      " 2. == Career ==\n",
      "Noam Shazeer joined Google in 2000. One of his first major achievements was improving the spelling correcto...\n",
      " 3. Sometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le....\n",
      " 4. === Attention with seq2seq ===\n",
      "\n",
      "The idea of encoder–decoder sequence transduction had been developed in the early 2010s;...\n",
      "\n",
      "Hybrid:\n",
      " 1. ×\n",
      "            P\n",
      "            ×\n",
      "            C\n",
      "          \n",
      "        \n",
      "      \n",
      "    \n",
      "    {\\displaystyle \\mathbb {R} ^{P\\times P\\t...\n",
      " 2. Sometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le....\n",
      " 3. == Career ==\n",
      "Noam Shazeer joined Google in 2000. One of his first major achievements was improving the spelling correcto...\n",
      " 4. == History ==\n",
      "On June 11, 2018, OpenAI researchers and engineers published a paper called \"Improving Language Understand...\n",
      " 5. In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechani...\n",
      " 6. === Attention with seq2seq ===\n",
      "\n",
      "The idea of encoder–decoder sequence transduction had been developed in the early 2010s;...\n"
     ]
    }
   ],
   "source": [
    "# Compare dense vs sparse vs hybrid for the same query\n",
    "test_query = \"semantic vector search databases\"\n",
    "\n",
    "dense_only = dense_retriever.invoke(test_query)\n",
    "sparse_only = sparse_retriever.invoke(test_query)\n",
    "hybrid = hybrid_retriever.invoke(test_query)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Dense:\")\n",
    "for i, doc in enumerate(dense_only, 1):\n",
    "    print(f\" {i}. {doc.page_content[:120]}...\")\n",
    "\n",
    "print(\"\\nSparse:\")\n",
    "for i, doc in enumerate(sparse_only, 1):\n",
    "    print(f\" {i}. {doc.page_content[:120]}...\")\n",
    "\n",
    "print(\"\\nHybrid:\")\n",
    "for i, doc in enumerate(hybrid, 1):\n",
    "    print(f\" {i}. {doc.page_content[:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-overview",
   "metadata": {},
   "source": [
    "### Build an agentic RAG flow (LangChain `create_agent`)\n",
    "- Tool: the hybrid retriever exposed via a `@tool` wrapper\n",
    "- Agent: `create_agent` orchestrates model + tool calls\n",
    "- Model: ChatGroq (LLM) drives tool selection and final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "llm-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq LLM (deterministic for repeatable demos)\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tool-build",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool registered: hybrid_search\n"
     ]
    }
   ],
   "source": [
    "# Turn the retriever into a tool that the agent can call\n",
    "@tool(\"hybrid_search\", description=\"Hybrid dense+sparse retrieval over the wiki corpus\")\n",
    "def hybrid_search(query: str):\n",
    "    \"\"\"Retrieve semantically and lexically relevant chunks for a user question.\"\"\"\n",
    "    return hybrid_retriever.invoke(query)\n",
    "\n",
    "tools = [hybrid_search]\n",
    "print(\"Tool registered:\", tools[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "agent-build",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent ready with tools: ['hybrid_search']\n"
     ]
    }
   ],
   "source": [
    "# System prompt keeps the agent grounded in retrieved context\n",
    "system_prompt = (\n",
    "    \"You are a concise technical assistant. Use the hybrid_search tool to fetch context \"\n",
    "    \"before answering. If nothing relevant is found, say you do not have context.\"\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "\n",
    "print(\"Agent ready with tools:\", [t.name for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "agent-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). They are also able to operate in parallel over all tokens in a sequence, whereas RNNs operate one token at a time from first to last. This allows transformers to be more efficient and scalable for large-scale natural language processing tasks.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Ask a question and let the agent decide how to use the hybrid retriever\n",
    "user_question = \"What advantage do transformers have over RNNs?\"\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=user_question)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(response[\"messages\"][-1].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "weight-sweep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights dense/sparse: 0.7/0.3\n",
      " 1. == History ==\n",
      "On June 11, 2018, OpenAI researchers and engineers published a paper called \"Improving Language Understand...\n",
      " 2. === Predecessors ===\n",
      "For many years, sequence modelling and generation was done by using plain recurrent neural networks...\n",
      "\n",
      "Weights dense/sparse: 0.5/0.5\n",
      " 1. == History ==\n",
      "On June 11, 2018, OpenAI researchers and engineers published a paper called \"Improving Language Understand...\n",
      " 2. SSD Layer: The main contribution of structured state space duality in Mamba-2 is through the SSD layer. In Mamba-1, the ...\n",
      "\n",
      "Weights dense/sparse: 0.3/0.7\n",
      " 1. SSD Layer: The main contribution of structured state space duality in Mamba-2 is through the SSD layer. In Mamba-1, the ...\n",
      " 2. === Mamba-2 ===\n",
      "Mamba-2 serves as a successor to Mamba by introducing a new theoretical and computational framework call...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick weight sweep to see how ranking changes\n",
    "def preview_weights(dense_weight: float, sparse_weight: float, user_query: str):\n",
    "    test_retriever = EnsembleRetriever(\n",
    "        retrievers=[dense_retriever, sparse_retriever],\n",
    "        weights=[dense_weight, sparse_weight],\n",
    "    )\n",
    "    docs_out = test_retriever.invoke(user_query)\n",
    "    print(f\"Weights dense/sparse: {dense_weight}/{sparse_weight}\")\n",
    "    for i, doc in enumerate(docs_out[:2], 1):\n",
    "        print(f\" {i}. {doc.page_content[:120]}...\")\n",
    "    print()\n",
    "\n",
    "\n",
    "sample_query = \"LangChain retrieval methods\"\n",
    "preview_weights(0.7, 0.3, sample_query)\n",
    "preview_weights(0.5, 0.5, sample_query)\n",
    "preview_weights(0.3, 0.7, sample_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices",
   "metadata": {},
   "source": [
    "### Notes and production tips\n",
    "- Tune `k` per retriever and weight blend based on evaluation, not guesses\n",
    "- Normalize embeddings for FAISS cosine similarity (done above)\n",
    "- For larger corpora, switch FAISS to IVF or move to managed vector DBs\n",
    "- Cache frequent queries and reuse embeddings to keep cost down\n",
    "- Keep prompts short; hybrid retrieval already improves recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
