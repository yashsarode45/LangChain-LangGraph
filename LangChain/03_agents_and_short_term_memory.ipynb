{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbcbc3ba",
   "metadata": {},
   "source": [
    "### 02 – Agents and Short-Term Memory\n",
    "\n",
    "In the previous module, we manually orchestrated tools: check if model wants a tool, execute it, send back result, repeat.\n",
    "\n",
    "**Agents automate this loop.** They decide:\n",
    "- Which tool to use (if any)\n",
    "- How many times to iterate\n",
    "- When they have enough information to answer\n",
    "\n",
    "This module covers:\n",
    "1. What agents are and why they matter\n",
    "2. Creating agents with LangChain v1 APIs\n",
    "3. Adding short-term memory for context across turns\n",
    "4. Structured outputs for reliable parsing\n",
    "5. Debugging agent behavior\n",
    "\n",
    "By the end, you'll understand the foundation for building complex, stateful AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4560542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashsarode/Downloads/Personal Projects/Python/LangGraph-personal/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core LangChain\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Agent creation\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Memory and state management\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# For structured output\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc656cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and 3 tools ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "@tool\n",
    "def calculator(\n",
    "    operation: Annotated[Literal[\"add\", \"subtract\", \"multiply\", \"divide\"], \"The math operation\"],\n",
    "    a: Annotated[float, \"First number\"],\n",
    "    b: Annotated[float, \"Second number\"]\n",
    ") -> str:\n",
    "    \"\"\"Perform basic arithmetic operations. Use for exact calculations.\"\"\"\n",
    "    ops = {\n",
    "        \"add\": a + b,\n",
    "        \"subtract\": a - b,\n",
    "        \"multiply\": a * b,\n",
    "        \"divide\": a / b if b != 0 else \"Error: Division by zero\"\n",
    "    }\n",
    "    result = ops.get(operation.lower(), \"Invalid operation\")\n",
    "    return f\"Result: {result}\"\n",
    "\n",
    "@tool\n",
    "def get_current_time(timezone: str = \"local\") -> str:\n",
    "    \"\"\"Get the current date and time.\"\"\"\n",
    "    current = datetime.now()\n",
    "    return f\"Current time: {current.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "@tool\n",
    "def search_database(query: Literal[\"laptop\", \"phone\", \"tablet\"]) -> str:\n",
    "    \"\"\"Search a simulated product database. Use when user asks about products or inventory.\"\"\"\n",
    "    # Simulated product database\n",
    "    products = {\n",
    "        \"laptop\": \"In stock: 15 units, Price: $999\",\n",
    "        \"phone\": \"In stock: 42 units, Price: $699\",\n",
    "        \"tablet\": \"Out of stock, Expected: Next week\",\n",
    "    }\n",
    "    result = products.get(query.lower(), f\"No product found matching '{query}'\")\n",
    "    return result\n",
    "\n",
    "tools = [calculator, get_current_time, search_database]\n",
    "\n",
    "print(f\"Model and {len(tools)} tools ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b4653",
   "metadata": {},
   "source": [
    "### What is an Agent?\n",
    "\n",
    "**Manual tool calling** (previous module):\n",
    "- We wrote the orchestration loop ourselves\n",
    "- We decided when to call tools and when to stop\n",
    "\n",
    "**Agent**:\n",
    "- Autonomous decision-maker\n",
    "- Decides which tool to call based on the query\n",
    "- Can call multiple tools in sequence\n",
    "- Knows when it has enough information to answer\n",
    "\n",
    "**The agent loop:**\n",
    "```\n",
    "Query → Agent → Call Tool? → Execute → Agent → Call Another Tool? → Final Answer\n",
    "```\n",
    "\n",
    "Agents use ReAct pattern (Reasoning + Acting): think about what to do, then do it, then think again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bca5b",
   "metadata": {},
   "source": [
    "### When to Use Agents vs Simple Chains\n",
    "\n",
    "**Use simple tool calling when:**\n",
    "- You know exactly which tool(s) to call\n",
    "- Single-step operations\n",
    "- Deterministic workflows\n",
    "\n",
    "**Use agents when:**\n",
    "- The tool choice depends on user input\n",
    "- Multiple tools might be needed\n",
    "- You need multi-step reasoning\n",
    "- Queries are open-ended\n",
    "\n",
    "Example: \"What's 25% of the laptop price?\" needs both search_database and calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6d17d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created\n",
      "Agent type: <class 'langgraph.graph.state.CompiledStateGraph'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant with access to tools.\n",
    "\n",
    "When a user asks a question:\n",
    "1. Think about which tool(s) you need\n",
    "2. Call the appropriate tools\n",
    "3. Provide a clear, concise answer\n",
    "\n",
    "Be direct and avoid unnecessary explanations of your process.\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "# In LangChain v1, create_agent is the standard way to build agents\n",
    "agent = create_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "\n",
    "print(\"Agent created\")\n",
    "print(f\"Agent type: {type(agent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1a92e",
   "metadata": {},
   "source": [
    "### Agent as a Runnable\n",
    "\n",
    "Agents implement the Runnable interface, which means:\n",
    "- `.invoke()` for single execution\n",
    "- `.batch()` for multiple queries\n",
    "- `.stream()` for streaming responses\n",
    "- Can be composed into larger pipelines\n",
    "\n",
    "This consistency is what makes LangChain powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac98d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is current date and time?\n",
      "\n",
      "Agent response:\n",
      "The current date and time is December 1, 2025, 21:57:02.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is current date and time?\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_current_time (fdm66vhh7)\n",
      " Call ID: fdm66vhh7\n",
      "  Args:\n",
      "    timezone: local\n",
      "None\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_current_time\n",
      "\n",
      "Current time: 2025-12-01 21:57:02\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current date and time is December 1, 2025, 21:57:02.\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single query - agent has no memory of previous interactions\n",
    "query = \"What is current date and time?\"\n",
    "\n",
    "response = agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nAgent response:\")\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "[print(msg.pretty_print()) for msg in response[\"messages\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0b08434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the laptop price in the database? After getting the price, calculate 15% of the price.\n",
      "\n",
      "Agent response:\n",
      "The laptop price is $999, and 15% of the price is $149.85.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the laptop price in the database? After getting the price, calculate 15% of the price.\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search_database (0m6d6c59h)\n",
      " Call ID: 0m6d6c59h\n",
      "  Args:\n",
      "    query: laptop\n",
      "None\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_database\n",
      "\n",
      "In stock: 15 units, Price: $999\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  calculator (6tt0ggvem)\n",
      " Call ID: 6tt0ggvem\n",
      "  Args:\n",
      "    a: 999\n",
      "    b: 0.15\n",
      "    operation: multiply\n",
      "None\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: calculator\n",
      "\n",
      "Result: 149.85\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The laptop price is $999, and 15% of the price is $149.85.\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query that requires multiple tools\n",
    "query = \"What is the laptop price in the database? After getting the price, calculate 15% of the price.\"\n",
    "\n",
    "response = agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nAgent response:\")\n",
    "print(response[\"messages\"][-1].content)\n",
    "[print(msg.pretty_print()) for msg in response[\"messages\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74f1fc",
   "metadata": {},
   "source": [
    "### Short-Term Memory in Agents\n",
    "\n",
    "Until now, our agent forgets everything after each call. Each invocation is independent.\n",
    "\n",
    "**The problem:**\n",
    "- User: \"Search for laptop\"\n",
    "- Agent: \"In stock: 15 units, Price: $999\"\n",
    "- User: \"What's 20% of that price?\"\n",
    "- Agent: \"What price are you referring to?\"\n",
    "\n",
    "**The solution:** Store conversation history and pass it with each request.\n",
    "\n",
    "In LangChain v1.x, we use the `state_schema` parameter in `create_agent` to define what state to track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent with memory created\n",
      "State schema: MessagesState (tracks conversation history)\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import MessagesState\n",
    "\n",
    "# Create a checkpointer for in-memory storage\n",
    "# This stores conversation state between agent calls\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Create agent with memory using state_schema\n",
    "agent_with_memory = create_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    system_prompt=system_prompt,\n",
    "    state_schema=MessagesState,  # Built-in state that tracks messages\n",
    "    checkpointer=memory,          # Persists state between calls\n",
    ")\n",
    "\n",
    "print(\"Agent with memory created\")\n",
    "print(\"State schema: MessagesState (tracks conversation history)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29458b39",
   "metadata": {},
   "source": [
    "### How Memory Works in LangChain v1.x\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "1. **state_schema**: Defines what data to track (messages, custom fields, etc.)\n",
    "2. **checkpointer**: Where to store state (memory, Redis, Postgres, etc.)\n",
    "3. **thread_id**: Session identifier - like a conversation ID\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "User message → Agent (loads history from thread_id) → Tool calls → Response → Save to thread_id\n",
    "```\n",
    "\n",
    "Each thread_id maintains its own isolated conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c842c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1:\n",
      "There are 15 laptops in stock, priced at $999.\n",
      "\n",
      "============================================================\n",
      "Turn 2 (agent remembers context):\n",
      "20% of $999 is $199.80.\n",
      "\n",
      "============================================================\n",
      "Turn 3:\n",
      "There are 15 laptops available.\n"
     ]
    }
   ],
   "source": [
    "def chat_with_memory(message: str, thread_id: str):\n",
    "    \"\"\"\n",
    "    Chat with memory-enabled agent.\n",
    "    thread_id acts as session identifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration with thread_id for memory lookup\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # Invoke agent - it automatically loads and saves history\n",
    "    response = agent_with_memory.invoke(\n",
    "        {\"messages\": [HumanMessage(content=message)]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Return the last message (agent's response)\n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "# Start a conversation\n",
    "thread = \"conversation_1\"\n",
    "\n",
    "print(\"Turn 1:\")\n",
    "response1 = chat_with_memory(\"Search for laptop\", thread_id=thread)\n",
    "print(response1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Turn 2 (agent remembers context):\")\n",
    "response2 = chat_with_memory(\"What's 20% of that price?\", thread_id=thread)\n",
    "print(response2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Turn 3:\")\n",
    "response3 = chat_with_memory(\"How many are available?\", thread_id=thread)\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25865406",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c0cac56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THREAD A - Alice:\n",
      "------------------------------------------------------------\n",
      "The phone costs $699.\n",
      "\n",
      "============================================================\n",
      "THREAD B - Bob:\n",
      "------------------------------------------------------------\n",
      "The tablet is expected to be available next week.\n",
      "\n",
      "============================================================\n",
      "Back to THREAD A - Alice:\n",
      "------------------------------------------------------------\n",
      "The 15% discount on the phone is $104.85, making the new price $594.15.\n"
     ]
    }
   ],
   "source": [
    "# Different threads maintain separate histories\n",
    "\n",
    "print(\"THREAD A - Alice:\")\n",
    "print(\"-\"*60)\n",
    "chat_with_memory(\"I want to buy a phone\", thread_id=\"alice\")\n",
    "response_a = chat_with_memory(\"How much does it cost?\", thread_id=\"alice\")\n",
    "print(response_a)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THREAD B - Bob:\")\n",
    "print(\"-\"*60)\n",
    "chat_with_memory(\"I want to buy a tablet\", thread_id=\"bob\")\n",
    "response_b = chat_with_memory(\"When will it be available?\", thread_id=\"bob\")\n",
    "print(response_b)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Back to THREAD A - Alice:\")\n",
    "print(\"-\"*60)\n",
    "# Alice's context is preserved\n",
    "response_a2 = chat_with_memory(\"Can you calculate 15% discount?\", thread_id=\"alice\")\n",
    "print(response_a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02903d66",
   "metadata": {},
   "source": [
    "### Structured Output from Agents\n",
    "\n",
    "Sometimes you need agents to return data in a specific format, not just text.\n",
    "\n",
    "**Use cases:**\n",
    "- Extract structured information (names, dates, amounts)\n",
    "- Form filling from conversation\n",
    "- API integration requiring specific JSON\n",
    "- Database operations\n",
    "\n",
    "We define a Pydantic schema and the agent returns validated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb1ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output agent created\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class ProductInquiry(BaseModel):\n",
    "    \"\"\"Structured information extracted from product queries\"\"\"\n",
    "    product_name: str = Field(description=\"Name of the product user asked about\")\n",
    "    price: Optional[float] = Field(default=None, description=\"Product price if mentioned\")\n",
    "    stock_quantity: Optional[int] = Field(default=None, description=\"Available quantity if mentioned\")\n",
    "    user_wants_calculation: bool = Field(description=\"Whether user requested a calculation\")\n",
    "    calculation_result: Optional[float] = Field(default=None, description=\"Result of calculation if performed\")\n",
    "\n",
    "# Agent that returns structured data\n",
    "structured_agent = create_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    system_prompt=\"\"\"You are an assistant that extracts structured information.\n",
    "\n",
    "After gathering information via tools, return your findings in the specified structured format.\n",
    "Be precise with the data extraction.\"\"\",\n",
    "    state_schema=MessagesState,\n",
    "    response_format=ProductInquiry,  # Enforce this output structure\n",
    ")\n",
    "# response_format = ToolStrategy(ProductInquiry) is the way to do in LangChain v1 as per docs, but some open source models don't support it\n",
    "print(\"Structured output agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c85480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output:\n",
      "============================================================\n",
      "Returning structured response: product_name='laptop' price=999.0 stock_quantity=15 user_wants_calculation=False calculation_result=None\n"
     ]
    }
   ],
   "source": [
    "# Query that should produce structured output\n",
    "query = \"What's the laptop price and how many do we have in stock?\"\n",
    "\n",
    "try:\n",
    "    response = structured_agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=query)]},\n",
    "        config={\"configurable\": {\"thread_id\": \"structured_test\"}}\n",
    "    )\n",
    "    \n",
    "    # The agent's response will be structured according to ProductInquiry\n",
    "    final_message = response[\"messages\"][-1]\n",
    "    \n",
    "    print(\"Structured output:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # If the model supports structured output, content will be JSON\n",
    "    if isinstance(final_message.content, str):\n",
    "        try:\n",
    "            parsed = json.loads(final_message.content)\n",
    "            print(json.dumps(parsed, indent=2))\n",
    "        except:\n",
    "            print(final_message.content)\n",
    "    else:\n",
    "        print(final_message.content)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Note: Structured output requires model support\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nYour model may not support response_format parameter\")\n",
    "    print(\"This is fine - structured output is an advanced feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22680e2",
   "metadata": {},
   "source": [
    "### Understanding response_format\n",
    "\n",
    "**How it works:**\n",
    "1. You provide a Pydantic model as `response_format`\n",
    "2. LangChain converts it to a JSON schema\n",
    "3. The model is instructed to return data matching that schema\n",
    "4. Output is automatically validated against your model\n",
    "\n",
    "**Requirements:**\n",
    "- Model must support structured output (OpenAI, Anthropic Claude, some others)\n",
    "- Not all models support this feature\n",
    "- Groq models may have limited support depending on the specific model\n",
    "\n",
    "**Alternative approach:** Parse agent text output yourself using another LLM call or traditional parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0638dcb",
   "metadata": {},
   "source": [
    "### State Schema: Beyond Messages\n",
    "\n",
    "So far we've used `MessagesState` - it only tracks conversation history.\n",
    "\n",
    "But you can define custom state to track additional information:\n",
    "```python\n",
    "from typing import TypedDict\n",
    "\n",
    "class CustomState(TypedDict):\n",
    "    messages: List[BaseMessage]  # Required\n",
    "    user_preferences: dict        # Custom field\n",
    "    current_cart: List[str]       # Custom field\n",
    "    total_cost: float             # Custom field\n",
    "```\n",
    "\n",
    "**Why custom state:**\n",
    "- Track domain-specific data\n",
    "- Maintain application state alongside conversation\n",
    "- Enable complex workflows\n",
    "\n",
    "In the LangGraph module where custom state truly shines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2d831",
   "metadata": {},
   "source": [
    "### Error Handling and Interrupts in Production\n",
    "\n",
    "LangChain v1.x provides several mechanisms for robust agent execution:\n",
    "\n",
    "**Tool-level error handling:**\n",
    "- `@wrap_tool_call` decorator: Customize how individual tool errors are handled\n",
    "- Prevents one failing tool from crashing the entire agent\n",
    "\n",
    "**Agent-level interrupts:**\n",
    "- `interrupt_before`: Pause execution before specific nodes (e.g., for user confirmation)\n",
    "- `interrupt_after`: Pause after nodes to validate output or add processing\n",
    "\n",
    "**Use cases:**\n",
    "- Retry failed tool calls with different strategies\n",
    "- Request user confirmation before critical actions\n",
    "- Validate outputs before proceeding\n",
    "- Graceful degradation when tools fail\n",
    "\n",
    "The agent will return a ToolMessage with the custom error message when a tool fails:\n",
    "```python\n",
    "[\n",
    "    ...\n",
    "    ToolMessage(\n",
    "        content=\"Tool error: Please check your input and try again. (division by zero)\",\n",
    "        tool_call_id=\"...\"\n",
    "    ),\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fdab507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What will be the total cost of entire laptop stock?', additional_kwargs={}, response_metadata={}, id='786fc002-6037-4372-b529-57c2fe604bbf'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'a78vten8e', 'function': {'arguments': '{\"query\":\"laptop\"}', 'name': 'search_database'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 848, 'total_tokens': 878, 'completion_time': 0.024905078, 'completion_tokens_details': None, 'prompt_time': 0.412374327, 'prompt_tokens_details': None, 'queue_time': 0.348187761, 'total_time': 0.437279405}, 'model_name': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': 'fp_9b0c2006ef', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--e1224185-54cb-481a-a099-972d8c3da219-0', tool_calls=[{'name': 'search_database', 'args': {'query': 'laptop'}, 'id': 'a78vten8e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 848, 'output_tokens': 30, 'total_tokens': 878}),\n",
       "  ToolMessage(content='In stock: 15 units, Price: $999', name='search_database', id='dc2d2774-c51d-48b5-93c4-e928b6155924', tool_call_id='a78vten8e'),\n",
       "  AIMessage(content=\"To find the total cost of the entire laptop stock, we need to multiply the number of units by the price per unit.\\n\\nLet's perform the calculation: 15 units * $999 = $14,985.\\n\\nNow, let's use the calculator function to verify this calculation.\\n\\n\", additional_kwargs={'tool_calls': [{'id': 'vk756sfdq', 'function': {'arguments': '{\"a\":15,\"b\":999,\"operation\":\"multiply\"}', 'name': 'calculator'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 910, 'total_tokens': 1006, 'completion_time': 1.57543375, 'completion_tokens_details': None, 'prompt_time': 0.01813751, 'prompt_tokens_details': None, 'queue_time': 0.743104278, 'total_time': 1.59357126}, 'model_name': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': 'fp_d2c1f7e199', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--88de59b2-9fd2-404f-958e-1bc787a87d37-0', tool_calls=[{'name': 'calculator', 'args': {'a': 15, 'b': 999, 'operation': 'multiply'}, 'id': 'vk756sfdq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 910, 'output_tokens': 96, 'total_tokens': 1006}),\n",
       "  ToolMessage(content='Result: 14985.0', name='calculator', id='952faef9-360e-4ee7-a040-94fa54023b06', tool_call_id='vk756sfdq'),\n",
       "  AIMessage(content='The total cost of the entire laptop stock is $14,985.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 980, 'total_tokens': 995, 'completion_time': 0.046726361, 'completion_tokens_details': None, 'prompt_time': 0.021061421, 'prompt_tokens_details': None, 'queue_time': 0.115380757, 'total_time': 0.067787782}, 'model_name': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': 'fp_d2c1f7e199', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--1a358731-ea15-4b94-a693-9e87b53bf16a-0', usage_metadata={'input_tokens': 980, 'output_tokens': 15, 'total_tokens': 995})]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "agent = create_agent(\n",
    "   llm,\n",
    "    tools,\n",
    "    middleware=[handle_tool_errors]\n",
    ")\n",
    "\n",
    "agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What will be the total cost of entire laptop stock?\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349bd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
