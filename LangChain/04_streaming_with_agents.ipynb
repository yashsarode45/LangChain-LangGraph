{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a55e27e",
   "metadata": {},
   "source": [
    "Streaming in LangChain v1.x \n",
    "------------------------------------------------------\n",
    "This notebook demonstrates all streaming patterns in modern LangChain:\n",
    "- Token-level streaming from LLMs\n",
    "- Agent step streaming with LangGraph\n",
    "- Custom streaming for tool execution\n",
    "- Multi-mode streaming for complex UIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f42f9",
   "metadata": {},
   "source": [
    "### Token Streaming Fundamentals\n",
    "\n",
    "When you call an LLM, it generates text sequentially. Streaming exposes\n",
    "this incremental generation to your application in real-time.\n",
    "\n",
    "Key concept: Each \"chunk\" contains a partial response. You accumulate\n",
    "these chunks to build the complete output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d308d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming tokens from LLM:\n",
      "\n",
      "Quantum computing! A fascinating and complex topic that has been gaining significant attention in recent years. I'll try to break it down in a way that's easy to understand.\n",
      "\n",
      "**What is Quantum Computing?**\n",
      "\n",
      "Quantum computing is a new paradigm for computing that uses the principles of quantum mechanics to perform calculations and operations on data. It's a fundamentally different approach from classical computing, which uses bits to represent information as 0s and 1s.\n",
      "\n",
      "**Classical Computing vs. Quantum Computing**\n",
      "\n",
      "Classical computers use \"bits\" to process information, which can only be in one of two states: 0 or 1. This limits their ability to process complex problems, as they can only perform one calculation at a time.\n",
      "\n",
      "Quantum computers, on the other hand, use \"qubits\" (quantum bits), which can exist in multiple states simultaneously, represented by a combination of 0 and 1. This property, known as superposition, allows qubits to process multiple possibilities simultaneously, making quantum computers potentially much faster than classical computers for certain types of calculations.\n",
      "\n",
      "**Key Principles of Quantum Computing**\n",
      "\n",
      "1. **Superposition**: Qubits can exist in multiple states (0, 1, or both) at the same time, allowing for parallel processing of multiple possibilities.\n",
      "2. **Entanglement**: Qubits can be connected in a way that their properties are correlated, enabling the manipulation of multiple qubits simultaneously.\n",
      "3. **Quantum measurement**: Measuring a qubit causes its state to collapse to one of the possible outcomes, which is known as wave function collapse.\n",
      "4. **Quantum gates**: Quantum algorithms are composed of a series of quantum gates, which are the quantum equivalent of logic gates in classical computing.\n",
      "\n",
      "**How Quantum Computing Works**\n",
      "\n",
      "1. **Qubits are prepared**: Qubits are initialized in a specific state, often a superposition of 0 and 1.\n",
      "2. **Quantum gates are applied**: Quantum gates are applied to the qubits to manipulate their states and perform calculations.\n",
      "3. **Qubits are measured**: The qubits are measured, causing their state to collapse to one of the possible outcomes.\n",
      "4. **Results are processed**: The measured outcomes are processed and analyzed to obtain the final result.\n",
      "\n",
      "**Applications of Quantum Computing**\n",
      "\n",
      "1. **Cryptography**: Quantum computers can potentially break certain classical encryption algorithms, but they can also be used to create new, quantum-resistant encryption methods.\n",
      "2. **Optimization**: Quantum computers can be used to optimize complex systems, such as logistics, finance, and energy management.\n",
      "3. **Simulation**: Quantum computers can simulate complex quantum systems, enabling breakthroughs in fields like chemistry and materials science.\n",
      "4. **Machine learning**: Quantum computers can be used to speed up certain machine learning algorithms, potentially leading to breakthroughs in areas like image recognition and natural language processing.\n",
      "\n",
      "**Challenges and Limitations**\n",
      "\n",
      "1. **Noise and error correction**: Quantum computers are prone to errors due to the noisy nature of quantum systems. Developing robust error correction methods is essential.\n",
      "2. **Scalability**: Currently, quantum computers are small-scale and need to be scaled up to perform complex tasks.\n",
      "3. **Quantum control**: Maintaining control over the quantum states of qubits is crucial, but challenging.\n",
      "\n",
      "**Current State and Future Prospects**\n",
      "\n",
      "Quantum computing is still an emerging field, with significant technical challenges to overcome. However, major companies like Google, IBM, and Microsoft, as well as startups and research institutions, are actively working on developing quantum computing technology.\n",
      "\n",
      "In the near future, we can expect to see:\n",
      "\n",
      "1. **Advances in quantum algorithms**: New quantum algorithms will be developed to take advantage of the unique capabilities of quantum computers.\n",
      "2. **Improved quantum hardware**: Quantum computing hardware will continue to improve, with more stable and scalable qubits.\n",
      "3. **Quantum-classical hybrids**: Hybrid systems that combine classical and quantum computing will become more prevalent, enabling the solution of complex problems.\n",
      "\n",
      "In summary, quantum computing is a revolutionary technology that has the potential to solve complex problems that are intractable or require an unfeasible amount of time on classical computers. While significant challenges remain, the progress being made in this field is exciting, and we can expect to see significant breakthroughs in the coming years.\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize model with streaming enabled (default in v1.x)\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    "    streaming=True  # Explicitly enabled, though it's default\n",
    ")\n",
    "\n",
    "# Simple streaming example\n",
    "print(\"Streaming tokens from LLM:\\n\")\n",
    "for chunk in llm.stream(\"Explain quantum computing\"):\n",
    "    # Each chunk contains a partial response\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59411ed",
   "metadata": {},
   "source": [
    "### Understanding Message Flow\n",
    "\n",
    "LangChain uses typed messages (AIMessage, HumanMessage, SystemMessage).\n",
    "When streaming, you receive AIMessageChunk objects that contain:\n",
    "- content: The actual text/tool calls\n",
    "- response_metadata: Token counts, model info\n",
    "- usage_metadata: Input/output token details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4391e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming with structured messages:\n",
      "\n",
      "Here is an example of a decorator in Python that can be used to time the execution of a function:\n",
      "\n",
      "```python\n",
      "import time\n",
      "from functools import wraps\n",
      "\n",
      "def timer_decorator(func):\n",
      "    \"\"\"\n",
      "    A decorator to time the execution of a function.\n",
      "    \n",
      "    Args:\n",
      "        func (function): The function to be timed.\n",
      "    \n",
      "    Returns:\n",
      "        function: The decorated function.\n",
      "    \"\"\"\n",
      "    @wraps(func)\n",
      "    def wrapper_timer(*args, **kwargs):\n",
      "        \"\"\"\n",
      "        The wrapper function that records the start and end time of the function execution.\n",
      "        \n",
      "        Args:\n",
      "            *args: The positional arguments to the function.\n",
      "            **kwargs: The keyword arguments to the function.\n",
      "        \"\"\"\n",
      "        start_time = time.perf_counter()  # Record the start time\n",
      "        value = func(*args, **kwargs)  # Execute the function\n",
      "        end_time = time.perf_counter()  # Record the end time\n",
      "        run_time = end_time - start_time  # Calculate the execution time\n",
      "        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n",
      "        return value\n",
      "    return wrapper_timer\n",
      "\n",
      "# Example usage\n",
      "@timer_decorator\n",
      "def example_function():\n",
      "    time.sleep(1)  # Simulate some work\n",
      "\n",
      "example_function()\n",
      "```\n",
      "\n",
      "This decorator will print the execution time of the `example_function` when it is called.\n",
      "\n",
      "However, if you want to be able to get the execution time in addition to seeing it printed, you can modify the decorator to return a tuple containing the result of the function and the execution time like so:\n",
      "\n",
      "```python\n",
      "import time\n",
      "from functools import wraps\n",
      "\n",
      "def timer_decorator(func):\n",
      "    \"\"\"\n",
      "    A decorator to time the execution of a function.\n",
      "    \n",
      "    Args:\n",
      "        func (function): The function to be timed.\n",
      "    \n",
      "    Returns:\n",
      "        function: The decorated function.\n",
      "    \"\"\"\n",
      "    @wraps(func)\n",
      "    def wrapper_timer(*args, **kwargs):\n",
      "        \"\"\"\n",
      "        The wrapper function that records the start and end time of the function execution.\n",
      "        \n",
      "        Args:\n",
      "            *args: The positional arguments to the function.\n",
      "            **kwargs: The keyword arguments to the function.\n",
      "        \n",
      "        Returns:\n",
      "            tuple: A tuple containing the result of the function and the execution time.\n",
      "        \"\"\"\n",
      "        start_time = time.perf_counter()  # Record the start time\n",
      "        value = func(*args, **kwargs)  # Execute the function\n",
      "        end_time = time.perf_counter()  # Record the end time\n",
      "        run_time = end_time - start_time  # Calculate the execution time\n",
      "        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n",
      "        return value, run_time\n",
      "    return wrapper_timer\n",
      "\n",
      "# Example usage\n",
      "@timer_decorator\n",
      "def example_function():\n",
      "    time.sleep(1)  # Simulate some work\n",
      "\n",
      "result, execution_time = example_function()\n",
      "print(f\"Result: {result}\")\n",
      "print(f\"Execution Time: {execution_time} secs\")\n",
      "```\n",
      "\n",
      "This way, you can get the execution time in addition to seeing it printed.\n",
      "\n",
      "Complete response length: 2881 characters\n",
      "Response metadata: N/A\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant specialized in Python.\"),\n",
    "    HumanMessage(content=\"Write a decorator for timing function execution\")\n",
    "]\n",
    "\n",
    "print(\"Streaming with structured messages:\\n\")\n",
    "full_response = \"\"\n",
    "\n",
    "for chunk in llm.stream(messages):\n",
    "    # Accumulate the full response\n",
    "    full_response += chunk.content\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(f\"\\n\\nComplete response length: {len(full_response)} characters\")\n",
    "print(f\"Response metadata: {chunk.response_metadata.get('model_name', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b59af",
   "metadata": {},
   "source": [
    "### Streaming Agent Steps\n",
    "\n",
    "stream_mode=\"updates\" shows you the agent's reasoning process:\n",
    "- When it decides to call a tool\n",
    "- When tools return results  \n",
    "- When it generates the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb675ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created successfully\n",
      "Available tools: ['get_weather', 'get_forecast']\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get current weather for a city. Use this when users ask about weather.\"\"\"\n",
    "    # Simulate API call\n",
    "    import random\n",
    "    conditions = [\"sunny\", \"cloudy\", \"rainy\", \"windy\"]\n",
    "    temp = random.randint(15, 30)\n",
    "    return f\"Weather in {city}: {random.choice(conditions)}, {temp}°C\"\n",
    "\n",
    "@tool\n",
    "def get_forecast(city: str, days: int = 3) -> str:\n",
    "    \"\"\"Get weather forecast for a city. Use for multi-day predictions.\"\"\"\n",
    "    return f\"{days}-day forecast for {city}: Mostly sunny with occasional clouds\"\n",
    "\n",
    "\n",
    "# Create agent with our tools\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_weather, get_forecast],\n",
    ")\n",
    "\n",
    "# Verify agent structure\n",
    "print(\"Agent created successfully\")\n",
    "print(f\"Available tools: {[tool.name for tool in [get_weather, get_forecast]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15da985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming Agent Progress ===\n",
      "\n",
      "\n",
      "--- Node: model ---\n",
      "Tool: get_weather\n",
      "Args: {'city': 'Tokyo'}\n",
      "\n",
      "--- Node: tools ---\n",
      "Content: Weather in Tokyo: sunny, 24°C\n",
      "\n",
      "--- Node: model ---\n",
      "Content: The weather in Tokyo is sunny, 24°C.\n",
      "\n",
      "=== Agent execution complete ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]}\n",
    "\n",
    "print(\"=== Streaming Agent Progress ===\\n\")\n",
    "\n",
    "for chunk in agent.stream(query, stream_mode=\"updates\"):\n",
    "    for node_name, node_data in chunk.items():\n",
    "        print(f\"\\n--- Node: {node_name} ---\")\n",
    "        \n",
    "        # Extract and display the latest message\n",
    "        if \"messages\" in node_data and node_data[\"messages\"]:\n",
    "            latest_msg = node_data[\"messages\"][-1]\n",
    "            \n",
    "            # Check if it's a tool call\n",
    "            if hasattr(latest_msg, \"tool_calls\") and latest_msg.tool_calls:\n",
    "                for tool_call in latest_msg.tool_calls:\n",
    "                    print(f\"Tool: {tool_call['name']}\")\n",
    "                    print(f\"Args: {tool_call['args']}\")\n",
    "            \n",
    "            # Check if it's a tool result\n",
    "            elif hasattr(latest_msg, \"content\"):\n",
    "                print(f\"Content: {latest_msg.content}\")\n",
    "\n",
    "print(\"\\n=== Agent execution complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f587e",
   "metadata": {},
   "source": [
    "### Streaming LLM Tokens from Agents\n",
    "\n",
    "stream_mode=\"messages\" gives you real-time token generation,\n",
    "but now within an agent context. This shows:\n",
    "- Tool call chunks being built incrementally\n",
    "- Final response tokens as they're generated\n",
    "\n",
    "Use this for ChatGPT-like UIs where users see text appear in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49bfafd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming Tokens from Agent ===\n",
      "\n",
      "node: model\n",
      "content: []\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'cj02wd90n'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: []\n",
      "\n",
      "\n",
      "node: model\n",
      "content: []\n",
      "\n",
      "\n",
      "node: tools\n",
      "content: [{'type': 'text', 'text': 'Weather in San Francisco: sunny, 28°C'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: []\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': 'The'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' weather'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' in'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' San'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' Francisco'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' is'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' sunny'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ','}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' '}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': '28'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': '°C'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': '.'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: []\n",
      "\n",
      "\n",
      "node: model\n",
      "content: []\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=== Streaming complete ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Streaming Tokens from Agent ===\\n\")\n",
    "for token, metadata in agent.stream(  \n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(f\"node: {metadata['langgraph_node']}\")\n",
    "    print(f\"content: {token.content_blocks}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"\\n\\n=== Streaming complete ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8176b09f",
   "metadata": {},
   "source": [
    "### Custom Streaming - Application-Specific Signals\n",
    "\n",
    "Use get_stream_writer() inside tools to emit custom progress updates.\n",
    "This is powerful for long-running operations like:\n",
    "- Database queries\n",
    "- File processing\n",
    "- External API calls\n",
    "\n",
    "Critical: This only works inside LangGraph execution context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231b9c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Custom Streaming Example ===\n",
      "\n",
      "Progress: Loading dataset: sales_2024\n",
      "Progress: Preprocessing sales_2024...\n",
      "Progress: Running statistical analysis on sales_2024...\n",
      "Progress: Analysis complete for sales_2024\n"
     ]
    }
   ],
   "source": [
    "from langgraph.config import get_stream_writer\n",
    "\n",
    "@tool\n",
    "def analyze_data(dataset: str) -> str:\n",
    "    \"\"\"Analyze a dataset and return summary statistics.\"\"\"\n",
    "    writer = get_stream_writer()\n",
    "    \n",
    "    # Simulate multi-step analysis with progress updates\n",
    "    import time\n",
    "    \n",
    "    writer(f\"Loading dataset: {dataset}\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    writer(f\"Preprocessing {dataset}...\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    writer(f\"Running statistical analysis on {dataset}...\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    writer(f\"Analysis complete for {dataset}\")\n",
    "    \n",
    "    return f\"Dataset {dataset}: 1000 rows, 15 columns, no missing values\"\n",
    "\n",
    "# Create new agent with analysis tool\n",
    "analysis_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[analyze_data]\n",
    ")\n",
    "\n",
    "# Stream custom updates\n",
    "query = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Analyze the sales_2024 dataset\"}]\n",
    "}\n",
    "\n",
    "print(\"=== Custom Streaming Example ===\\n\")\n",
    "\n",
    "for chunk in analysis_agent.stream(query, stream_mode=\"custom\"):\n",
    "    print(f\"Progress: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70c57e",
   "metadata": {},
   "source": [
    "### Combining Multiple Stream Modes\n",
    "\n",
    "Real applications often need multiple types of feedback:\n",
    "- Show tokens to users (messages)\n",
    "- Log agent steps for debugging (updates)\n",
    "- Display progress bars (custom)\n",
    "\n",
    "You can stream multiple modes simultaneously by passing a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c61a6742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Mode Streaming ===\n",
      "\n",
      "[UPDATES] Node 'model' executed\n",
      "[CUSTOM] Connecting to /api/users...\n",
      "[CUSTOM] Fetching data from /api/users...\n",
      "[CUSTOM] Processing response...\n",
      "[UPDATES] Node 'tools' executed\n",
      "[UPDATES] Node 'model' executed\n",
      "\n",
      "=== Multi-mode streaming complete ===\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def fetch_api_data(endpoint: str) -> str:\n",
    "    \"\"\"Fetch data from an API endpoint.\"\"\"\n",
    "    writer = get_stream_writer()\n",
    "    import time\n",
    "    \n",
    "    writer(f\"Connecting to {endpoint}...\")\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    writer(f\"Fetching data from {endpoint}...\")\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    writer(f\"Processing response...\")\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    return f\"Successfully retrieved data from {endpoint}\"\n",
    "\n",
    "# Create agent with enhanced tool\n",
    "multi_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[fetch_api_data]\n",
    ")\n",
    "\n",
    "query = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Fetch data from /api/users\"}]\n",
    "}\n",
    "\n",
    "print(\"=== Multi-Mode Streaming ===\\n\")\n",
    "\n",
    "for stream_mode, chunk in multi_agent.stream(\n",
    "    query,\n",
    "    stream_mode=[\"updates\", \"custom\"]  # Multiple modes\n",
    "):\n",
    "    print(f\"[{stream_mode.upper()}]\", end=\" \")\n",
    "    \n",
    "    if stream_mode == \"custom\":\n",
    "        print(chunk)\n",
    "    elif stream_mode == \"updates\":\n",
    "        for node_name in chunk.keys():\n",
    "            print(f\"Node '{node_name}' executed\")\n",
    "\n",
    "print(\"\\n=== Multi-mode streaming complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd28bc",
   "metadata": {},
   "source": [
    "### Production Streaming Guidelines\n",
    "\n",
    "\n",
    "DO:\n",
    "- Use async streaming (astream) in web servers\n",
    "- Implement timeout handling for streaming calls\n",
    "- Buffer tokens for better UI performance (e.g., word-by-word vs char-by-char)\n",
    "- Add custom streaming for operations >1 second\n",
    "- Use \"updates\" mode for debugging, \"messages\" for UIs\n",
    "\n",
    "DON'T:\n",
    "- Block the main thread with synchronous streaming\n",
    "- Stream sensitive data without proper filtering\n",
    "- Forget to handle stream interruptions/cancellations\n",
    "- Use custom streaming for fast operations (<100ms)\n",
    "\n",
    "Memory: stream_mode=\"messages\" is your UI mode\n",
    "       stream_mode=\"updates\" is your debugging mode\n",
    "       stream_mode=\"custom\" is your progress bar mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557d6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
